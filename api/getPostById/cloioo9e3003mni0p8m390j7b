{"type":"getPostById","data":{"title":"SpringCloud","date":"2023-09-13T15:43:17.000Z","description":"面试精选","categories":[{"name":"FaceToFace","_id":"cloioo9dy0028ni0p4jcr48xx"}],"tags":[{"name":"SpringCloud","_id":"cloioo9ep007yni0p27lj8my2"}],"content":"<meta name=\"referrer\" content=\"no-referrer\">\n<h2 id=\"Spring-Cloud\">Spring Cloud</h2>\n<h3 id=\"服务注册和发现\">服务注册和发现</h3>\n<h4 id=\"Eureka\">Eureka</h4>\n<p>1、<code>Eureka</code>  功能</p>\n<p>​\t1.1、<code>服务治理</code>： 管理服务与服务之间依赖关系，可以实现服务调用、负载均衡、容错等，实现服务发现与注册</p>\n<p>​\t1.2、<code>服务注册</code>：Eureka Server作为服务注册功能的服务器，它是服务注册中心。系统中的其他微服务使用<code>Eureka的客户端</code>连接到<code>Eureka Server</code>并维持心跳连接。当服务器启动的时候，会把当前自己服务器的信息（比如服务地址、通讯地址等）以别名方式注册到注册中心上。另一方（消费者、服务提供者）以该别名的方式去注册中心上获取到实际的服务通讯地址，然后再实现本地RPC调用。</p>\n<p>​\t1.3、<code>服务发现</code>：从注册中心上获取注册了的微服务的信息；实质是存储key 的服务命令，并由此取出 value 的调用地址.  <code>@EnableDiscoveryClient</code></p>\n<blockquote>\n<p>RPC 远程调用的核心思想：注册中心，使用注册中心管理每个服务与服务之间的一个依赖关系（服务治理概念）</p>\n<p>RPC 远程调用为了实现高可用，解决单点故障：搭建Eureka注册中心集群 ，实现<strong>负载均衡+故障容错</strong></p>\n</blockquote>\n<p>2、Eureka 的组成</p>\n<p>​\t2.1、<code>Eureka Server</code> ： 在服务启动之后对服务进行注册，<code>服务的注册表</code>就会存储所有的可用服务节点的信息；只能进行服务注册</p>\n<p>​\t2.2、 <code>Eureka Client </code>： 可以通过注册中心访问、监控是否有停机的现象发生。客户端具备一个内置的、使用轮询（round-robin）负载算法的负载均衡器。微服务应用启动后，将会向Eureka Server发送心跳（默认周期30秒）。如果在多个生命周期内没有监听到某个节点的心跳，那么就会从服务注册中心把这个服务节点<code>移除</code>（默认90秒）。   &lt;既可以是服务提供者，又可以是服务消费者，所以可以  服务注册、服务发现&gt;</p>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305121420401.png\" alt=\"image-20230512142013314\" style=\"zoom:93%;\">\n<p>3、eureka工作原理</p>\n<p>​\t1、先启动eureka注册中心</p>\n<p>​\t2、启动服务提供者，并在启动后会把自身信息（服务地址  以 别名方式注册进eureka)</p>\n<p>​\t3、服务消费者 在需要调用接口时，使用服务别名去注册中心获取实际的RPC远程调用地址</p>\n<p>​\t4、消费者获得调用地址后，底层实际是利用<code>Httpclient</code>技术实现远程调用</p>\n<p>​\t5、消费者获得服务地址后会缓存在本地 jvm 内存中，默认每间隔30秒更新一次服务调用地址</p>\n<p>4、eureka 的自我保护</p>\n<p>​\t1、自我保护：是一种在  <code>网络分区场景</code>下对 eureka 服务器 和客户端之间的保护。一旦进入保护模式，EurekaServer 将会<code>尝试保护其服务注册表</code>中的信息，<code>不再删除</code>服务注册表中的数据，也就是<code>不会注销任何微服务</code>。 就是说： 某时刻某一个微服务不可用了，Eureka不会立刻清理，依旧会对该微服务的信息进行保存。</p>\n<p>​\t2、是AP架构的</p>\n<p>​\t3、进行自我保护的<code>原因</code>：  EurekaClient可以正常运行，但是由于网络等问题导致客户端与EurekaServer网络不通,为了防止这个客户端实例在90s 内被移除，所以此时EurekaServer不会立刻将EurekaClient服务剔除。</p>\n<h4 id=\"Zookeeper\">Zookeeper</h4>\n<p>zookeeper是一个分布式协调工具，可以实现注册中心功能；</p>\n<p>1、作用</p>\n<p>​\t用于服务注册和发现</p>\n<p>2、工作机制</p>\n<p>​\t基于<code>观察者模式设计</code>的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应。</p>\n<p>3、特点：</p>\n<p>​\t1）Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。</p>\n<p>​\t2）服务器集群中只要有<code>半数以上</code>节点存活，Zookeeper集群就能正常服务。所以Zookeeper适合安装<code>奇数台</code>服务器。</p>\n<p>​\t3）<code>全局数据一致（CP）</code>：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。</p>\n<p>​\t4）更新请求  <code>顺序执行</code>，来自同一个Client的更新请求<code>按其发送顺序依次执行</code>。</p>\n<p>​\t5）<code>数据更新是原子性的</code>，一次数据更新要么成功，要么失败。</p>\n<p>​\t6）<code>实时性</code>，在一定时间范围内，Client能读到最新数据。</p>\n<p>4、数据结构</p>\n<p>​\t整体上可以看作是一棵树，每个节点(Server)称做一个 ZNode。每一个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过其路径唯一标识表示。（就是都可以通过唯一路径找到该节点）</p>\n<p>5、使用场景</p>\n<p>​\t1、统一命名服务: 对应用或者服务统一命名，便于识别</p>\n<p>​\t2、统一配置管理：将配置信息写到Znode中，各个<code>客户端服务器监听</code>这个Znode；一旦Znode中的数据被修改，ZooKeeper将通知各个客户端服务器。</p>\n<p>​\t3、统一集群管理：对节点的实时状态进行调整，将节点信息写到Znode中,监听其实时状态变化</p>\n<p>​\t4、<code>服务器节点动态上下线</code>： 实时洞察服务器上下线变化</p>\n<p>​\t5、<code>软负载均衡</code>： 在zookeeper 中会记录每台服务器的访问数，让访问最少的服务器处理最新的客户端请求</p>\n<p>6、选举机制</p>\n<p>​\t\t6.1、首次选举</p>\n<p>​\t\t\t当启动的服务器数量没有达到一半以上时，选票会<code>集中</code>投给myid大的服务器，直到达到一半以上时选出leader，一旦选出了leader，无论后面的服务器myid如何，均为follower。</p>\n<p>​\t\t6.2、非首次选举</p>\n<p>​\t\t\t选举<strong>Leader</strong>规则 ：    1、EPOCH大的直接胜出    2、EPOCH相同，事务id大的胜出   3、事务id相同，服务器id大的胜出</p>\n<p>​\t\t\t6.2.1、何时选举：服务器初始化启动  或者  服务器运行期间无法和Leader保持连接。</p>\n<p>​\t\t\t6.2.2、选举期间集群的状态：</p>\n<p>​\t\t\t6.2.2.1、对于已经存在Leader的情况，机器试图去选举Leader时，会被告知当前服务器的Leader信息，对于该机器来说，仅仅<code>需要和Leader机器建立连接</code>，并进行状态同步即可。</p>\n<p>​\t\t\t6.2.2.2、集群中确实不存在Leader,就会根据选举规则进行选举</p>\n<blockquote>\n<p>进行Leader选举时集群都是不可用(因为在正在选举)</p>\n</blockquote>\n<p>7、节点类型</p>\n<p>​\t1、持久：客户端和服务器端断开连接后，创建的节点（服务器）不删除</p>\n<p>​\t\t\t1.1、有序号：Zookeeper会给该节点名称进行顺序编号</p>\n<p>​\t\t\t1.2、 无序号</p>\n<p>​\t2、临时：客户端和服务器端断开连接后，创建的节点（服务器）自己删除</p>\n<p>​\t\t\t2.1、有序号：Zookeeper 会给该节点名称进行顺序编号</p>\n<p>​\t\t\t2.2、 无序号</p>\n<blockquote>\n<p>创建znode时设置<code>顺序标识</code>，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护;<code>顺序号</code>可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序</p>\n</blockquote>\n<p>​\t==注册到zookeeper的微服务是一个zNode节点，这个节点是临时节点还是持久节点？==</p>\n<p>我们在注册的时候会生成一个流水号id，我们将服务断开，发现刚断开时zookeeper还保存着该服务，等待一段时间后，该节点被删除。所以是<code>临时节点</code></p>\n<p>8、监听原理</p>\n<p>监听器原理详解</p>\n<p>1）首先要有一个main()线程</p>\n<p>2）在main线程中创建<code>Zookeeper客户端</code>，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。</p>\n<p>3）通过connect线程将注册的监听事件发送给Zookeeper。（告诉服务端我要监听什么）</p>\n<p>4）在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。</p>\n<p>5）Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。（告诉客户端监听的东西发生了变化）</p>\n<p>6）listener线程内部调用了process()方法；进行数据获取的一个处理。</p>\n<p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305102328542.png\" alt></p>\n<p>9、客户端向服务端写数据流程</p>\n<p>​\t9.1、写流程之写入请求直接发送给<code>Leader</code>节点</p>\n<p>​\t\t\t1、客户端发送写请求给Leader</p>\n<p>​\t\t\t2、Leader收到后会自己执行写请求；将写请求发送给Follower让其执行（同步）</p>\n<p>​\t\t\t3、Follower同步后会返回一个ack给Leader</p>\n<p>​\t\t\t4、集群上<code>超过半数服务器</code>完成了写请求，那么Leader会发送ack通知Client已完成请求（直到半数的所有节点（此处为3）写完就会进行应答）</p>\n<p>​\t\t\t5、剩下的Follower会继续写数据同步信息，直到集群中所有的服务器均完成同步</p>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305101752427.png\" alt=\"image-20230510175204390\" style=\"zoom:33%;\">\n<p>​\t9.2、写流程之写入请求发送给<code>follower</code>节点</p>\n<p>​\t\t\t1、客户端发送写请求给Follower</p>\n<p>​\t\t\t2、Follower将写请求转发给Leader</p>\n<p>​\t\t\t3、Leader处理写请求，然后将写请求发送给Follower让其执行（同步），Follower同步后会返回一个<code>ack</code>给Leader</p>\n<p>​\t\t\t4、当集群上<code>超过半数服务器</code>完成了写请求，Leader会发送一个<code>ack</code>给接收Client的Follower ，告诉其半数服务器已完成同步</p>\n<p>​\t\t\t5、随后Follower会发送ack通知Client已完成请求；剩下的Follower会<code>继续写数据</code>同步信息，直到集群中所有的服务器均完成同步</p>\n<p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305100115363.png\" alt></p>\n<h4 id=\"Consul\">Consul</h4>\n<p>微服务的服务注册中心；提供了微服务系统中心的服务治理，配置中心，控制总线等功能。这些功能中的每一个都可以根据需要单独使用，也可以一起使用，以构建全方位的服务网格。是基于raft协议的，提供图形界面 跨平台</p>\n<p>1、功能：</p>\n<p>服务发现：提供HTTP和DNS两种发现方式  + Key，Value的存储方式   + 安全的服务交流   + 健康检查   + 多数据中心  +  可视化web界面</p>\n<h5 id=\"四个注册中心比较\">四个注册中心比较</h5>\n<table>\n<thead>\n<tr>\n<th>组件</th>\n<th>语言</th>\n<th>CAP</th>\n<th>服务健康检查</th>\n<th>对外暴露接口</th>\n<th style=\"text-align:center\">SpringCloud集成</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Eureka</td>\n<td>Java</td>\n<td>AP</td>\n<td>可配支持</td>\n<td>HTTP</td>\n<td style=\"text-align:center\">已集成</td>\n</tr>\n<tr>\n<td>Consul</td>\n<td>Go</td>\n<td>CP</td>\n<td>支持</td>\n<td>Http/DNS</td>\n<td style=\"text-align:center\">已集成</td>\n</tr>\n<tr>\n<td>ZooKeeper</td>\n<td>Java</td>\n<td>CP</td>\n<td>支持</td>\n<td>客户端(TCP)</td>\n<td style=\"text-align:center\">已集成</td>\n</tr>\n<tr>\n<td>Nacos</td>\n<td>Java</td>\n<td>AP+CP</td>\n<td>支持</td>\n<td>Http/DNS/UDP</td>\n<td style=\"text-align:center\">已集成</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"服务调用\">服务调用</h3>\n<h4 id=\"Ribbon\">Ribbon</h4>\n<p>主要功能是提供客户端的软件负载均衡算法和服务调用；Ribbon是实现<code>负载均衡</code>的一套客户端工具，结合<code>RestTemplate</code>实现调用。在配置文件的<code>Load Banlancer </code>(简称LB) 后面列出所有的机器，Ribbon都会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器</p>\n<p><code>@LoadBlanced</code>:一种默认的负载均衡机制 ，就是<code>将用户的请求平摊的分配</code>到多个服务上，从而达到系统的 <code>高可用的</code> 目的；就是防止某个微服务挂掉了，导致整个微服务不可用</p>\n<blockquote>\n<p>为什么我们只输入了service名称就可以访问了呢？之前还要获取ip和端口</p>\n<p><code>LoadBalancerInterceptor</code>的负载均衡拦截器会在对<code>RestTemplate</code>的请求进行拦截，然后从<code>Eureka</code>中根据服务id获取服务列表，随后利用IRule内置的负载均衡算法得到真实的服务地址信息选取具体的服务，随后替换服务id，发送真实的请求</p>\n</blockquote>\n<p>常见的负载均衡算法：</p>\n<ul>\n<li>\n<p><strong>轮询</strong>：为第一个请求选择健康池中的第一个后端服务器，然后按顺序往后依次选择，直到最后一个，然后循环。</p>\n</li>\n<li>\n<p><strong>最小连接</strong>：<code>优先选择   连接数最少的进行分配</code>，也就是压力最小的后端服务器，在会话较长的情况下可以考虑采取这种方式。</p>\n</li>\n<li>\n<p><strong>散列</strong>：根据<code>请求源的 IP 的散列（hash）来选择</code>要转发的服务器。这种方式可以一定程度上保证特定用户能<code>连接到相同的服务器</code></p>\n</li>\n</ul>\n<p>Ribbon<code>本地</code>负载均衡客户端  VS. Nginx <code>服务端</code>负载均衡区别</p>\n<ul>\n<li><code>Nginx</code>是服务器负载均衡，客户端所有请求都会交给Nginx，然后由Nginx实现转发请求，<code>即负载均衡是由服务端实现的</code></li>\n<li><code>Ribbon</code> 本地负载均衡，在调用微服务接口时候，会在<code>注册中心</code>上获取注册信息服务列表之后<code>缓存到JVM本地</code>，从而在<code>本地实现RPC远程服务调用技术</code></li>\n</ul>\n<p>负载均衡的分类</p>\n<p>1、集中式负载均衡： 即在服务的消费方和提供方之间<code>使用独立的负载均衡设备</code>，由该设备负责把访问请求通过某种策略转发至服务的提供方。</p>\n<p>2、进程内 负载均衡：</p>\n<ul>\n<li>将负载均衡 逻辑  集成到消费方，消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个 合适 的服务器。</li>\n<li><code>Ribbon就属于是进程内负载均衡</code>, 它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址。</li>\n</ul>\n<p>Ribbon加载方式</p>\n<p>1、Ribbon<code>默认是采用懒加载</code>，即第一次访问时才会去创建LoadBalanceClient，请求时间会很长。</p>\n<p>2、<code>饥饿加载</code>则会在项目启动时创建，降低第一次访问的耗时，通过配置开启饥饿加载。</p>\n<p>Ribbon在工作 分成两步</p>\n<ul>\n<li>第一步先选择EurekaServer（其他所需请求的客户端），它优先选择在同一个区域内负载较少的server</li>\n<li>第二步再根据用户指定的策略，再从server取到的服务注册列表中选择一个地址。</li>\n</ul>\n<p>其中Ribbon提供了多种策略：比如 <strong>轮询，随机  和   根据响应时间加权</strong></p>\n<h4 id=\"Feign官网\">Feign\t\t<a href=\"https://cloud.spring.io/spring-cloud-static/Hoxton.SR1/reference/htmlsingle/#spring-cloud-openfeign\">官网</a></h4>\n<p>作用: 声明式远程方法调用</p>\n<p>Feign VS. OpenFeign</p>\n<table>\n<thead>\n<tr>\n<th>Feign</th>\n<th>OpenFeign</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Feign是Spring Cloud组件中的一个轻量级RestFul的Http服务客户端。<br>Feign内置了Ribbon，用来做客户端负载均衡，去调用服务注册中心的服务。<br>Feign的使用方式是：使用Feign的注解定义接口，调用这个接口就可以调用服务注册中心的服务。</td>\n<td>OpenFeign是Spring Cloud在Fiegn的基础上支持了SpringMVC的注解，如@RequestMapping等。<br>OpenFeign的@FeignClient可以解析SpringMVC的@RequestMapping注解下的接口，<br>并通过<code>动态代理</code>的方式产生实现类，实现类中做负载均衡并调用其他服务。</td>\n</tr>\n<tr>\n<td><s>GroupID：org.springframework.cloud                                               ArtifactID：spring-cloud-starter-feign</s></td>\n<td><s>GroupID：org.springframework.cloud                                                    ArtifactID：spring-cloud-starter-openfeign</s></td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"OpenFeign\">OpenFeign</h4>\n<blockquote>\n<p>OpenFeign整合了Ribbon，所以具有负载均衡的功能，所以OpenFeign 的效果等价于 <code>Ribbon+restTemplate进行客户端服务调用和负载均衡</code></p>\n</blockquote>\n<p>1、超时配置</p>\n<p>​\t默认Feign客户端只等待一秒钟，但是<code>服务端</code>处理需要<code>超过1秒钟</code>，导致Feign<code>客户端</code>不想等待了，直接返回报错。为了避免这种请况，有时候我们需要<code>设置Feign客户端的超时控制</code>。因为： Feign 默认是支持Ribbon ，Feign依赖里自己带了Ribbon**；**Feign客户端的负载均衡和超时控制都由Ribbon控制</p>\n<p>2、日志配置</p>\n<p>​\t通过配置来调整日志级别，从而达到 对Feign接口的调用情况进行  监控  和  输出</p>\n<p>​\t日志级别</p>\n<table>\n<thead>\n<tr>\n<th>类型</th>\n<th>作用</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1、NONE</td>\n<td>默认的，不显示任何日志</td>\n</tr>\n<tr>\n<td>2、BASIC</td>\n<td>仅记录请求方法、URL、响应状态码及执行时间</td>\n</tr>\n<tr>\n<td>3、HEADERS</td>\n<td>除了BASIC中定义的信息之外，还有请求和响应的头信息</td>\n</tr>\n<tr>\n<td>4、FULL</td>\n<td>除了HEADERS中定义的信息外，还有请求和响应的正文及元数据。</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"服务降级\">服务降级</h3>\n<p>背景：在分布式链路的条件之下，随着微服务的调用越来越多，会导致链路越来越长，只要有一个出现事故，从而就导致整个来拿路上都会出现事故</p>\n<p>服务雪崩：链路上某个微服务的调用响应时间过长或者不可用，对第一个微服务的调用就会占用越来越多的系统资源，进而引起系统崩溃，所谓的“雪崩效应”。</p>\n<h4 id=\"Hystrix\">Hystrix</h4>\n<p>1、是什么Hystrix 是一个用于处理分布式系统的<code>延迟和容错</code>的开源库；Hystrix能够<code>保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性</code>。</p>\n<p>2、作用原理 ： <code>当某个服务单元发生故障之后</code>，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个可处理的备选响应（FallBack），而不是长时间的等待或者抛出调用方无法处理的异常，就保证了服务调用方的线程不被长时间、不必要地占用，从而避免了故障在分布式系统中的蔓延，乃至雪崩。</p>\n<p>3、功能：\t\t<code>服务降级  +  服务熔断    +     接近实时的监控限流</code></p>\n<p>​\t服务降级：fallback；在系统出现问题的时候会提供一个兜底的解决方案或备选响应；从而能够向调用方返回一个符合预期的、可处理的备选响应。<code>放在客户端</code></p>\n<p>​\t服务熔断：break；达到<code>最大服务访问后</code>，直接拒绝访问，拉闸限电，然后调用服务<code>降级的方法</code>并返回友好提示 <code>break</code></p>\n<p>​\t服务限流：flowlimit；针对高并发操作可以有序进行限流</p>\n<p>4、服务熔断</p>\n<p>​\t是一种应对雪崩效应的微服务的链路保护机制；当链路的某个微服务出错不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回错误的响应信息。当检测到该节点微服务调用响应正常后，自动恢复调用链路。</p>\n<p>5、断路器的状态有： 开启 、关闭、半开启</p>\n<p>​\t1、当失败的调用到一定<code>阈值</code>，缺省时5秒内20次调用失败，就会启动熔断机制，熔断机制的注解是<code>@HystrixCommand</code></p>\n<p>​\t2、<code>半开启状态</code>：服务无法承受并发量的时候就会进行熔断，之后并发量在可以承受之内，那么就试着放开这些并发请求。 直到发现能够适应当前的并发量了，再把闸道合上。 <code>持续放开请求的状态就是半开状态</code>，然后再把断路器   <code>开启</code>  变成<code>关闭</code>的状态。</p>\n<p>​\t3、断路器何时开启：当满足一定的阈值的时候  或者 当失败率达到一定的时候；当开启的时候，所有请求都不会进行转发；</p>\n<p>一段时间之后，这个时候断路器是半开状态，会让其中一个请求进行转发；如果成功，断路器会关闭，若失败，继续开启。重复上述的步骤；</p>\n<p>​\t4、熔断器的三种状态</p>\n<p>​\t断路器<code>打开</code>之后，再有请求调用的时候，不会调用主逻辑，会直接调用降级的fallback方法，通过断路器，实现了自动的发现错误  并将  降级逻辑升级为主逻辑，减少响应延迟的效果。原来的主逻辑<code>hystrix</code>也为我们实现了<code>自动恢复功能</code>;此时；当断路器打开，对主逻辑进行熔断之后，hystrix会启动一个休眠时间窗，在这个时间窗内(内部设置时钟一般为MTTR（Mean time to repair，平均故障处理时间))，降级逻辑是临时的成为主逻辑。</p>\n<p>​\t当休眠时间窗到期，断路器将进入<code>半开状态</code>，释放给一次请求到原来的主逻辑上，如果此次请求正常返回，那么断路器将会<code>关闭</code>。主逻辑恢复，如果这次请求依然有问题，断路器继续进入打开状态，休眠时间窗重新计时;</p>\n<p>​\t熔断<code>关闭</code>的时候不会对服务进行熔断，服务正常调用</p>\n<p>5、熔断机制：</p>\n<ul>\n<li>\n<p><strong>circuitBreaker.enabled</strong>：是否开启断路器</p>\n</li>\n<li>\n<p><strong>circuitBreaker.requestVolumeThreshold</strong>：该属性设置滚动窗口（快照时间窗口，默认10s）中将使断路器跳闸的最小请求数量（默认是20），如果10s内请求数小于设定值，就算请求全部失败也不会触发断路器。</p>\n<p><em>请求总数阀值</em>：在快照时间窗内，必须满足请求总数阀值才有资格熔断。默认为20，意味着在10秒内，如果该hystrix命令的调用次数不足20次，即使所有的请求都超时或其他原因失败，断路器都不会打开</p>\n</li>\n<li>\n<p><strong>circuitBreaker.sleepWindowInMilliseconds</strong>：短路多久以后开始尝试是否恢复，默认5s ，窗口睡眠时间，即断路器触发多少秒（默认5s）后尝试恢复，进入半开状态。</p>\n</li>\n<li>\n<p><strong>circuitBreaker.errorThresholdPercentage</strong>：失败率达到多少后跳闸</p>\n<p><em>错误百分比阀值</em>：当请求总数在快照时间窗内超过了阀值，比如发生了30次调用，如果在这30次调用中，有15次发生了超时异常，也就是超过50%的错误百分比，在默认设定50%阀值情况下，这时候就会将断路器打开。</p>\n</li>\n<li>\n<p><strong>metrics.rollingStats.timeInMilliseconds</strong>：快照时间窗、滚动窗口\t路器确定是否打开需要统计一些请求和错误数据，而统计的时间范围就是快照时间窗，默认为最近的10秒。</p>\n</li>\n</ul>\n<p>​\t<a href=\"https://martinfowler.com/bliki/CircuitBreaker.html\">https://martinfowler.com/bliki/CircuitBreaker.html</a></p>\n<h3 id=\"网关\">网关</h3>\n<h4 id=\"Zuul\">Zuul</h4>\n<ol>\n<li>\n<p>Zuul1.x 是一个基于阻塞 I/O的API网关</p>\n</li>\n<li>\n<p>**模型:**Zuul 1.X是基于servlet之上的一个阻塞式处理模型。使用的是传统的<code>Servlet IO</code>处理模型，就是说其处理的所有请求都是用一个servlet（DispatcherServlet）容器进行阻塞式处理的。</p>\n</li>\n<li>\n<p>Zuul1.x 基于Servlet2.5使用阻塞架构它<code>不支持任何长连接</code> （如WebSocket）Zuul的设计模式和Nginx较像，<code>每次I/O操作都是从工作线程中选择一个执行，请求线程被阻塞到工作线程完成</code>，但是差别是Nginx用C++实现，Zuul用java实现，而<code>JVM</code>本身会有第一次加载较慢的情况，<code>使得Zuul的性能相对较差</code>。</p>\n</li>\n<li>\n<p>Zuul 2.x理念更加先进，<code>像基于Netty非阻塞和支持长连接</code>，但SpringCloud目前还没有整合。Zuul2.x的性能较Zuul 1.x有较大的提升。在性能方面，根据官方提供的基准测试，Spring Cloud Gateway的RPS（每秒请求次数）是Zuul的1.6倍。</p>\n</li>\n</ol>\n<h4 id=\"GateWay\">GateWay</h4>\n<p>1、介绍</p>\n<p>​\t是zuul的1.x 版本的代替；是在Spring生态系统之上架构的API网关服务；提供一种简单而有效的方式来对API进行路由;</p>\n<p>2、模型</p>\n<p>​\t是基于WebFlux框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架Netty(<code>异步非阻塞的</code>)，【就是说：  SpringCloud Gateway是<code>异步非阻塞式</code>，响应式的框架】；就是说： SpringCloud Gateway 使用的Webflux中的reactor-netty响应式编程组件，底层使用了Netty通讯框架。</p>\n<p>3、作用</p>\n<p>​\t <code>反向代理</code>、<code>鉴权</code>（需要校验用户是是否有请求资格，如果没有则进行拦截）、<code>流量控制（</code>当请求流量过高时，在网关中按照下流的微服务能够接受的速度来放行请求，避免服务压力过大）、<code>熔断</code>、<code>日志监控</code>、<code>动态路由（能够匹配任何请求属性）</code>和<code>负载均衡</code>（一切请求都必须先经过gateway，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。）、<code>支持长连接 WebSocket</code></p>\n<p>4、核心概念</p>\n<p>​\t1、Route(路由)： 路由是构建网关的基本模块，它由ID，目标URI（Uniform Resource Identifier，统一资源标识符），一系列的断言和过滤器组成，如果断言为true则匹配该路由。</p>\n<p>​\t2、Predicate(断言)：开发人员可以匹配Http请求中的所有内容（例如请求头或者请求参数），如果请求参数与断言相匹配则进行路由。<code>就是匹配的条件</code></p>\n<p>​\t 3、Filter(过滤)：指的是Spring框架中的  <code>GatewayFilter</code> 的实例，使用过滤器，可以在请求  被路由前或者之后对请求进行  修改。</p>\n<p>​\t\t\t过滤链</p>\n<p>​\t\t\t\t1、作用：路由过滤器可用于修改进入的HTTP请求和返回的HTTP响应；路由过滤器只能对指定的路由进行使用。</p>\n<p>​\t\t\t\t2、执行顺序：</p>\n<p>​\t\t\t\t\t\t每一个过滤器都必须指定一个int类型的<code>order</code>值，<strong><code>order</code>值越小，优先级越高，执行顺序越靠前</strong>。</p>\n<p>​\t\t\t\t\t\t<code>GlobalFilter</code>通过实现<code>Ordered</code>接口，或者添加<code>@Order</code>注解来指定order值，由我们自己指定</p>\n<p>​\t\t\t\t\t\t<code>路由过滤器和defaultFilter</code>的<code>order</code>由Spring指定，<code>默认是按照声明顺序从1递增</code>。</p>\n<p>​\t\t\t\t\t\t当过滤器的<code>order</code>值一样时，会按照 <code>默认过滤器  &gt; 路由过滤器 &gt; 全局过滤器 </code>的顺序执行。</p>\n<p>​\t\t\t限流过滤器：    限流：对应用服务器的请求做限制，避免因过多请求而导致服务器过载甚至宕机。</p>\n<p>​\t\t\t\t\t限流算法常见的包括三种：</p>\n<p>​\t\t\t\t\t\t1、计数器算法，又包括窗口计数器算法、滑动窗口计数器算法「将时间划分为多个窗口；在每个窗口内每有一次请求就将计数器加一，当时间到达下一个窗口时，计数器重置。如果计数器超过了限制数量，则本窗口内所有的请求都被丢弃」</p>\n<p>​\t\t\t\t\t\t2、漏桶算法(Leaky Bucket)「将每个请求视作&quot;水滴&quot;放入&quot;漏桶&quot;进行存储；&quot;漏桶&quot;以固定速率向外&quot;漏&quot;出请求来执行，如果&quot;漏桶&quot;空了则停止&quot;漏水”；如果&quot;漏桶&quot;满了则多余的&quot;水滴&quot;会被直接丢弃」</p>\n<p>​\t\t\t\t\t\t3、令牌桶算法（Token Bucket）「以固定的速率生成令牌，存入令牌桶中，如果令牌桶满了以后，多余令牌丢弃；请求进入后，必须先尝试从桶中获取令牌，获取到令牌后才可以被处理；如果令牌桶中没有令牌，则请求等待或丢弃」</p>\n<p>5、原理/过程</p>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305131100584.png\" alt=\"image-20230513105923489\" style=\"zoom:63%;\">\n<ul>\n<li>客户端向 Spring Cloud Gateway 发出请求。然后在 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler。</li>\n<li>Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。</li>\n<li>过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（“pre”）或之后（“post”）执行业务逻辑。</li>\n<li>Filter在“pre”类型的过滤器可以做参数<code>校验</code>、权限校验、流量监控、日志输出、协议转换等，在“post”类型的过滤器中可以做<code>响应</code>内容、响应头的修改，日志的输出，流量监控等有着非常重要的作用。</li>\n</ul>\n<p><strong>核心逻辑</strong>：<code>路由转发+执行过滤器链</code></p>\n<p>6、优化</p>\n<p>​\t在分布式集群的情况下，会有非常多的主机，端口，接口。显然我们无法为每一个接口都定义一个路由规则，所以通过 <code>微服务名</code> 进行负载均衡和动态路由；默认情况下Gateway会根据<code>注册中心</code>注册的服务列表，以注册中心上<code>微服务名</code>为路径创建动态路由进行转发，<code>从而实现动态路由</code>的功能。</p>\n<h3 id=\"服务配置\">服务配置</h3>\n<h4 id=\"Config\">Config</h4>\n<p>1、介绍</p>\n<p>​\t为微服务架构中的微服务<code>提供集中化的外部配置支持</code>（Git/GitHub）（意思就是可以配置远程的配置文件），配置服务器为各个不同微服务应用的所有环境提供了一个中心化的外部配置（Config Server）。将服务之间<code>公用</code>的放在<code>配置中心</code>，各自<code>特有</code>的再<code>单独配置</code>。其默认使用Git来存储配置文件，最推荐与github整合。</p>\n<p>2、作用</p>\n<p>​\t可以集中管理配置文件；动态化的配置更新；需要配置的服务会向配置中心统一拉取配置自己的信息；</p>\n<p>3、SpringCloud Config 分为服务端和客户端两部分</p>\n<p>​\t\t服务端：分布式配置中心；连接配置服务器并   为客户端提供获取配置信息</p>\n<p>​\t\t客户端：通过  指定的配置中心来管理应用资源和配置内容，并在启动的时候从配置中心获取和加载配置信息</p>\n<p>4、配置的读取规则</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">label:分支    name:服务名      profiles：环境</span><br><span class=\"line\">/&#123;application&#125;/&#123;profile&#125;[/&#123;label&#125;]</span><br><span class=\"line\">/&#123;application&#125;-&#123;profile&#125;.properties</span><br><span class=\"line\">/&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.properties</span><br><span class=\"line\"></span><br><span class=\"line\">/&#123;application&#125;-&#123;profile&#125;.yml</span><br><span class=\"line\">/&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.yml</span><br></pre></td></tr></table></figure>\n<p>5、问题：</p>\n<p>​\t背景：当每一次修改配置文件之后，客户端都不能立马生效，此时只有重启客户端才能有效果。</p>\n<p>​\t初步解决: 业务控制器上添加注解  <code>@RefreshScope</code>  并使用<code>curl -X POST &quot;http://XXXX/actuator/refresh&quot;发送post请求刷新后才会有效果</code>，但是每次都要发送一下post 请求也是难以忍受的</p>\n<p>​\t最终解决：<code>  BUS 总线</code></p>\n<h3 id=\"服务总线\">服务总线</h3>\n<h4 id=\"BUS\">BUS</h4>\n<p>1、介绍</p>\n<p>​\t <code>Spring Cloud Bus</code>配合Spring Cloud Config使用可以实现配置的<code>动态刷新</code>。整合了Java 的  事件处理机制和消息中间件的功能；是将分布式系统的节点和轻量级消息系统链接起来的框架，支持rabbitmq 和 kafka</p>\n<p>2、功能</p>\n<p>​\t管理和传播分布式系统间的消息、可以当作微服务间的通信通道</p>\n<p>3、消息总线</p>\n<p>​\t使用轻量级的<code>消息代理</code>来<code>构建</code>一个共用的消息主题，并让系统中<code>所有微服务实例都连接上来</code>。由于该主题中产生的消息会被<code>所有实例监听和消费</code>，所以称它为消息总线；总线上的各个实例，都可以方便地广播一些需要让其他连接在该主题上的实例都知道的消息。</p>\n<p>4、基本原理</p>\n<p>​\tConfig客户端所有的实例都会去监听MQ中同一个topic(默认是springCloudBus)。当一个服务刷新数据的时候，它会把这个信息放入到Topic中，这样其它监听同一Topic的服务就能得到通知，然后去更新自身的配置。</p>\n<p>5、Bus动态刷新全局广播的设计思想</p>\n<p>​\t1、利用消息总线<code>出发一个客户端</code>/bus/refresh，从而刷新所有客户端的配置</p>\n<p>​\t2、利用消息总线<code>接触服务器</code>ConfigServer的/bus/refresh断点，从而刷新所有客户端的配置</p>\n<blockquote>\n<p>2更好，原因是1 会破坏  微服务的职责单一性「微服务本身是业务模块，它不应该承担配置刷新的功能」，破坏了微服务各结点的对等性</p>\n</blockquote>\n<p>6、刷新定点通知 ： 只通知指定的微服务</p>\n<p>​\t发送POST 请求的公式：<code>http://localhost:配置中心的端口号/actuator/bus-refresh/&#123;destination&#125;</code></p>\n<p>​\t\t      <code>/bus/refresh</code>请求不再发送到具体的服务实例上，而是发给<code>config server</code>并通过<code>destination参数类</code>指定需要更新配置的服务或实例。</p>\n<p>7、总体的流程图</p>\n<p>​\tconfig server （服务器）和 bus  （总线）  会订阅  mq， 客户端   会订阅 并监听 mq</p>\n<p>1、配置文件没有改变的时候，客户端会通过服务器读取到远程的仓库配置文件进行配置</p>\n<p>2、配置文件进行本地修改之后，config server （服务器）和 bus  （总线）会 通过 远程仓库 获取到修改的配置文件，随后将刷新事件传递给mq</p>\n<p>3、此时客户端监听mq  监听到  刷新事件之后就会去重新获取 服务器读取到远程的仓库配置文件进行重新配置</p>\n<p>![image-20230830125343090](/Users/yangrui/Library/Application Support/typora-user-images/image-20230830125343090.png)</p>\n<h3 id=\"消息驱动\">消息驱动</h3>\n<h4 id=\"cloud-stream\">cloud stream</h4>\n<p>1、是什么</p>\n<p>​\t构建消息驱动微服务的框架，应用程序通过 <code>inputs </code>或者 <code>outputs </code>来与<code> Spring Cloud Stream</code>中**绑定器对象(主要作用：屏蔽底层mq的差异)**交互。随后可以通过我们配置来<code>binding</code>(绑定) ，而 <code>Spring Cloud Stream </code>的 <strong>binder对象负责与消息中间件交互</strong>。主要是使用Spring Integration来连接消息代理中间件以实现消息事件驱动</p>\n<p>2、作用</p>\n<p>​\t 使得开发人员不再关注具体的MQ的细节，只需要用一种适配绑定的方式，自动的在各种MQ内切换，就是说 其可以屏蔽底层的细节差异，让我<code>只需要操作一个Cloud Stream</code>，就可以<code>操作不同的MQ</code>。</p>\n<p>3、统一底层差异的原理</p>\n<p>​\t将绑定器作为中间层，实现应用程序和消息中间件之间的隔离，并向应用程序暴露统一的channel通道，使得应用程序不再需要考虑不同的消息中间件的具体实现</p>\n<p>4、绑定器</p>\n<p>Binder可以生成Binding，Binding用来绑定消息容器的生产者和消费者，它有两种类型，<code>INPUT和OUTPUT</code></p>\n<p>​\t<strong>input 对应于消费者（消费者从Stream接收消息)</strong></p>\n<p>​\t<strong>output对应于生产者（生产者从Stream发布消息）。</strong></p>\n<p>5、stream 的构成： binder&lt;屏蔽差异&gt;、channel&lt;存储转发消息，并对 queue 进行配置&gt;、sink（source）&lt;输入输出消息&gt;</p>\n<p>6、stream 的流程</p>\n<p>​\t消息的生产者 会向 Stream 发布消息(output)，并通过stream中的source 进行接受消息，经由channel 进行转发和存储，并对队列进行配置，最后交给binder 屏蔽中间件的差异，实现与中间件的连接，并发送消息给 <code>mq</code> 中间件；</p>\n<p>​\t消息的接收者就会通过stream 中的 binder获取 底层中间件发出的消息，随后由channel存储转发，随后通过sink 接收 消息，将消息发给消息的接收者</p>\n<p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202304301215225.png\" alt></p>\n<p>7、常见API 和注解</p>\n<table>\n<thead>\n<tr>\n<th>常用注解与API</th>\n<th>含义解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Middlewar</td>\n<td>底层的中间件，目前官方只支持RabbitMQ、Kafka</td>\n</tr>\n<tr>\n<td>Binder</td>\n<td>Binder是应用与消息中间件之间的封装，通过Binder可以很方便的连接中间件，可以动态的改变消息类型（对应Kafka的topic、RabbitMQ的exchange），这些都可以通过配置文件来实现</td>\n</tr>\n<tr>\n<td>@Input</td>\n<td>注解标识输入通道，通过该输入通道接收到的消息进入应用程序，消息的消费者</td>\n</tr>\n<tr>\n<td>@Output</td>\n<td>注解标识输出通道，发布的消息将通过该通道离开应用程序，消息的生产者</td>\n</tr>\n<tr>\n<td>@StreamListener</td>\n<td>监听队列，用于消费者的队列的消息接收</td>\n</tr>\n<tr>\n<td>@EnableBinding</td>\n<td>指信道channel和exchange绑定在一起</td>\n</tr>\n</tbody>\n</table>\n<p>8、分组消费</p>\n<p>​\t1、为了防止消息的重复消费，我们需要进行分组消费；就是说 在 cloud  stream 中同一个 group 中多个消费者是竞争关系，是可以保证消费的消息不会被重复消费。但是不同的组是可以消费同一个消息的，也就是可以重复消费的，所以解决重复消费的关键就是：如何进行分组—其实就是将所有的消费实例分配到一个组中即可。</p>\n<p>​\t2、进行分组的另一个好处就是可以 将消息  持久化，可以防止<code>消息丢失</code></p>\n<p>9、消费组</p>\n<p>​\t将部署的多个实例配置（<code>spring.cloud.stream.bindings.input.group:XXX</code>）到一个组中，随后提供给消费消费即可</p>\n<h3 id=\"分布式请求链路追踪\">分布式请求链路追踪</h3>\n<h4 id=\"Sleuth\">Sleuth</h4>\n<p>1、背景：在微服务框架中，一个客户端发起的请求在后端系统中会经过多次不同的服务节点调用来协同产生最后的请求结果；每个前一段请求都会形成一条复杂的<code>分布式服务调用链路</code>，链路中的<code>任何一环出现高延时或错误</code>都会引起<code>整个请求最后的失败</code></p>\n<p>2、作用：负责跟踪整理，可以配合 zipkin进行展现到web端</p>\n<p>3、关联： 每一条请求链路都会通过<code>Trace Id</code>唯一标识，<code>Span</code>标识发起的请求信息，各span通过<code>parent id</code>关联起来</p>\n<h2 id=\"SpringCloud-Alibaba\">SpringCloud Alibaba</h2>\n<p>中文：<a href=\"https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md\">https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md</a></p>\n<h3 id=\"功能\">功能</h3>\n<p>1、服务限流降级：默认支持 Servlet、Feign、RestTemplate、Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控。<code>sentinel</code></p>\n<p>2、服务注册与发现：适配 Spring Cloud 服务注册与发现标准，默认集成了 Ribbon 的支持。<code>nacos</code></p>\n<p>3、分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新。<code>nacos</code></p>\n<p>4、消息驱动能力：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力。<code>Stream</code></p>\n<p>5、阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。  <code>OOS</code></p>\n<p>6、分布式任务调度：提供秒级、精准、高可靠、<code>高可用的定时（基于 Cron 表达式）</code>任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 Worker（schedulerx-client）上执行。  <code>SchedulerX 分布式调度产品</code>        <code>RocketMQ  高性能，高吞吐量 的分布式消息和流计算平台</code></p>\n<h3 id=\"Nacos\">Nacos</h3>\n<blockquote>\n<p>服务注册和配置中心；Nacos——<strong>Na</strong>ming <strong>Co</strong>nfiguration <strong>S</strong>ervice</p>\n</blockquote>\n<p>1、介绍： 动态服务发现、配置管理和服务的管理平台</p>\n<p>2、作用： 注册中心 + 配置中心+ 自带ribbon的负载均衡</p>\n<p>3、特点：</p>\n<p>​\t1、nacos 可以在 CP +  AP之间切换</p>\n<p>​\t\t1.1、如果<code>不需要存储服务级别的信息且服务实例是通过nacos-client注册</code>，并能够保持心跳上报，那么就可以选择AP模式，Spring cloud 和 Dubbo 服务，都适用于AP模式;但是 AP模式为了服务的可能性而减弱了一致性，因此AP模式下<code>只支持注册临时实例</code></p>\n<p>​\t\t1.2、如果需要在<code>服务级别</code>编辑或者存储配置信息，那么需要 CP ，<code>K8S服务和DNS服务则适用于CP模式</code>;CP模式下则支持<code>注册持久化实例</code>，此时则是以 Raft 协议为集群运行模式，该模式下注册实例之前必须先注册服务，如果服务不存在，则会返回错误。</p>\n<p>​\t2、Eureka  zookeeper 是临时实例，consul 是持久实例，nacos 是服务实例</p>\n<p>4、Nacos 作为配置中心的使用</p>\n<p>​\t1、基本配置</p>\n<p>​\t\t配置两个，一个<code>bootstrap</code>和一个<code>application</code></p>\n<p>​\t\t原因：Nacos同springcloud-config一样，在项目初始化时，要保证先从配置中心进行配置拉取，拉取配置之后，才能保证项目的正常启动。</p>\n<p>​\t\tspringboot中<code>配置文件</code>的加载是<code>存在优先级顺序</code>的，<strong>bootstrap优先级高于application</strong>。<code>注意nacos只识别yaml，不识别yml</code></p>\n<p>​\t\t全局的放在：bootstrap.yml     自己的放在：application.yml;</p>\n<p>​\t2、分类配置</p>\n<p>​\t\t主要问题： 微服务项目的每一个微服务都会有相应的开发环境、测试环境、预发环境、正式环境，都要进行配置</p>\n<p>​\t\t解决：通过<code>Namespace、Group、Data ID</code> 进行分类配置即可</p>\n<p>​\t\t\t1、namespace是可以用于区分部署环境的，Group和DataID逻辑上区分不同的目标对象。</p>\n<p>​\t\t\t2、Nacos<code>默认的命名空间是public</code>，Namespace主要用来实现隔离。</p>\n<p>​\t\t\t3、Group<code>默认是DEFAULT_GROUP</code>，Group可以把不同的微服务划分到同一个分组里面去;</p>\n<p>​\t\t\t4、Service就是微服务；一个Service可以包含多个Cluster（集群），<code>Nacos默认Cluster是DEFAULT</code>，Cluster是对指定微服务的一个虚拟划分。</p>\n<p>​\t\t\t5、<code>Instance</code>是微服务的实例。</p>\n<p>​\t3、加载配置的3种方案</p>\n<p>​\t\t1、DataID方案是在默认namesapce和默认Group下，创建两个不同的DataID。</p>\n<p>​\t\t2、Group方案是在默认namespace下，新建两个DataID相同的配置文件，通过指定不同的分组来读取不同的配置。</p>\n<p>​\t\t3、Namespace方案，是相同的Group，相同的DataID，创建并指定不同的namespace来读取不同配置。</p>\n<p>​\t4、可以进行热更新</p>\n<p>​\t\t通过Spring Cloud原生注解**@RefreshScope** 实现配置<code>自动更新(就是热更新)</code></p>\n<p>​\t\t\t方式一：在<code>@Value注入的变量</code>，并在所在类上添加注解<code>@RefreshScope</code></p>\n<p>​\t\t\t方式二：使用<code>@ConfigurationProperties</code>注解代替<code>@Value</code>注解</p>\n<p>5、 Nacos作为注册中心</p>\n<p>​\t\t1、非集群的<code>nacos</code> :不用单独新建注册中心微服务模块，直接安装使用即可</p>\n<p>​\t\t2、<code>集群和持久化配置</code>： 如果这个注册中心挂了怎么办？为了保证高可用，需要用到nacos集群。<a href=\"https://nacos.io/zh-cn/docs/cluster-mode-quick-start.html\">集群官网文档</a></p>\n<p>​\t\t\t\t1、如何搭建集群配置：要将配置持久化到数据库中：MySQL           不用nacos内嵌的数据库 derby</p>\n<p>​\t\t\t\t2、默认Nacos使用<code>嵌入式数据库</code>实现数据的存储，我们重启Nacos后，以前的配置文件不会消失。</p>\n<p>​\t\t\t\t3、但是，如果启动多个默认配置下的Nacos节点，数据存储是存在一致性问题的。每个nacos都有自己独立的嵌入式数据库，存放的数据不一致。</p>\n<p>​\t\t\t\t4、为了解决这个问题，Nacos采用了<code>集中式存储</code>的方式来<code>支持集群化部署</code>，目前只支持MySQL的存储。</p>\n<p>​\t\t\t\t5、集群的负载均衡也要进行配置： <code>NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 </code>默认的<code>ZoneAvoidanceRule</code>并不能实现根据同集群优先来实现负载均衡;Nacos中提供了一个<code>NacosRule</code>的实现，可以优先从同集群中挑选实例</p>\n<p>6、nacos支持的三种部署模式：</p>\n<ul>\n<li>单机模式：用于测试和单机使用</li>\n<li>集群模式：用于生产环境，确保高可用</li>\n<li>多集群模式：用于多数据中心场景</li>\n</ul>\n<p>7、优先级</p>\n<p>​\t当nacos、服务本地同时出现相同属性时，优先级有高低之分：</p>\n<p><code>服务名-profile.yaml  &gt;     服务名称.yaml   &gt;       本地配置</code><br>\n<code>当前环境配置</code>        \t\t\t\t\t\t<code>nacos中的配置   </code>   \t\t\t\t <code>本地配置</code></p>\n<p>8、Nacos VS  Eureka</p>\n<ul>\n<li>\n<p>Nacos与eureka的共同点</p>\n<ul>\n<li>都支持服务注册和服务拉取</li>\n<li>都支持服务提供者心跳方式做健康检测</li>\n</ul>\n</li>\n<li>\n<p>Nacos与Eureka的区别</p>\n<ul>\n<li>Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式</li>\n<li>临时实例心跳不正常会被剔除，非临时实例则不会被剔除</li>\n<li>Nacos支持服务列表变更的消息推送模式，服务列表更新更及时</li>\n<li>Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Sentinel-中文\">Sentinel  <a href=\"https://github.com/alibaba/Sentinel/wiki/%E4%BB%8B%E7%BB%8D\">中文</a></h3>\n<p>1、背景：服务使用中会产生各种问题：服务雪崩、服务降级、服务熔断、服务限流</p>\n<p>2、作用：解决  流控问题、降级问题、热点key限流问题、服务熔断      <code>服务保护</code>的问题</p>\n<p>3、解决方法：</p>\n<ol>\n<li>超时处理：设定超时时间，请求<code>超过一定时间</code>没有响应就<code>返回错误信息</code>，不会无休止等待</li>\n<li>仓壁模式（与此类似，我们可以限定每个业务能使用的线程数，避免耗尽整个tomcat的资源，因此也叫<code>线程隔离</code>。）</li>\n<li>断路器模式：由<strong>断路器</strong>统计业务执行的异常比例，如果<code>超出阈值</code>则会<strong>熔断</strong>该业务，拦截访问该业务的一切请求。</li>\n<li><strong>流量控制</strong>（限流）：限制业务访问的QPS，避免服务因流量的突增而故障。</li>\n</ol>\n<blockquote>\n<p><strong>限流</strong>是对服务的保护，避免因瞬间高并发流量而导致服务故障，进而避免雪崩。是一种<strong>预防</strong>措施。 而<strong>超时处理、线程隔离、降级熔断</strong>是在部分服务故障时，将故障控制在一定范围，避免雪崩。是一种<strong>补救</strong>措施。</p>\n<p><code>Sentinel采用懒加载机制  所以  要先 执行一下  http://localhost:8401/testA  才可以在网页上显示</code></p>\n</blockquote>\n<p>​\t3.1、线程隔离</p>\n<p>​\t\t调用者在调用服务提供者时，给每个调用的请求分配独立线程池，出现故障时，最多消耗这个线程池内资源，避免把调用者的所有资源耗尽。但是如果在某一时刻，在一个调用链路中，服务<code>B出现故障（</code>可能就卡在那里了），而这时<code>服务A依然有大量的请求，在调用服务B</code>，那么，由于服务A没办法再短时间内完成处理，新来的请求就会导致线程数不断地增加，这样，<code>CPU的资源很快就会被耗尽</code>那么要防止这种情况，就<code>只能进行隔离</code>了，这里我们提==两种隔离方案==：</p>\n<p>​\t\t\t3.1.1、<strong>线程池隔离</strong>： 给每个服务调用业务分配一个线程池，利用线程池本身实现隔离效果；实际上就是对  每个服务的远程调用  单独开放线程池，比如服务A要调用服务B，那么只基于固定数量的线程池，这样即使在短时间内出现大量请求，由于没有线程可以分配，所以就不会导致资源耗尽了</p>\n<p>​\t\t\t3.1.2、<strong>信号量隔离</strong>不创建线程池，而是计数器模式，记录业务使用的线程数量，达到信号量上限时，禁止新的请求。信号量隔离是使用<code>Semaphore</code>类实现的，思想基本上与上面是相同的，也是限定指定的线程数量能够同时进行服务调用，但是它相对于线程池隔离，开销会更小一些，使用效果同样优秀，也支持超时等。Sentinel也正是采用的这种方案实现隔离的。</p>\n<blockquote>\n<p>方式一：线程池的隔离</p>\n<ul>\n<li>优点：支持主动超时  和 异步调用</li>\n<li>缺点：额外开销较大</li>\n<li>场景：适用于 低扇出（扇出：调用其他服务）</li>\n</ul>\n<p>方式二：信号量的隔离（sentinel默认使用）</p>\n<ul>\n<li>优点：轻量级，没有额外开销</li>\n<li>缺点：不支持主动超时  和 异步调用</li>\n<li>场景：适用于 高扇出 高频调用</li>\n</ul>\n<p>QPS：就是每秒的请求数</p>\n<p>线程数：是该资源能使用用的tomcat线程数的最大值—也就是通过限制线程数量，实现<strong>线程隔离</strong>（舱壁模式）。</p>\n</blockquote>\n<h4 id=\"簇点链路\">簇点链路</h4>\n<p>当请求进入微服务时，首先会访问DispatcherServlet，然后进入Controller、Service、Mapper，这样的一个调用链就叫做<strong>簇点链路</strong>。簇点链路中被监控的每一个接口就是一个<strong>资源</strong>。</p>\n<p>默认情况下sentinel会监控SpringMVC的每一个端点（Endpoint，就是controller中的方法），因此SpringMVC的每一个端点（Endpoint）就是调用链路中的一个资源。</p>\n<h4 id=\"一般的限流—流控规则\">一般的限流—流控规则</h4>\n<blockquote>\n<p>主要应对 普通  高并发请求的限流；此时的限流是统计访问某个资源的所有请求，判断是否超过QPS阈值。</p>\n</blockquote>\n<p>​\t1、<strong>流控模式: 直接、关联、链路</strong></p>\n<p>​\t\t1、 直接：只针对于当前接口,出发预知的时候对<code>当前资源直接限流</code>，是默认的模式；直接快速失败（默认）: <code>QPS直接快速失败 + 线程数直接快速失败</code>；</p>\n<p>​\t\t2、关联： 当关联的资源达到阈值时，就限流自己。高优先级资源触发阈值，对<code>低优先级资源限流</code>。</p>\n<p>​\t\t3、链路：跟直接快速失败区别不大，监控的是<code>资源入口</code>；是对不同来源的两个链路做监控。但是sentinel默认会给进入SpringMVC的所有请求设置同一个root资源，会导致链路模式失效。阈值统计时，只统计从指定资源进入当前资源的请求，是对<code>请求来源的限流</code></p>\n<p>​\t2、<strong>流控效果</strong></p>\n<p>​\t\t1、快速失败（默认）：直接失败，默认会抛出异常;既然不再接受新的请求，那么我们可以直接返回一个拒绝信息，告诉用户访问频率过高。</p>\n<p>​\t\t2、warm up 预热：为了保护系统，可慢慢的把流量放进来，慢慢的把阀值增长到设置的阀值。对超出阈值的请求同样是拒绝并抛出异常。但这种模式阈值会动态变化，从一个较小值逐渐增加到最大阈值。阈值一般是一个微服务能承担的最大QPS，但是一个服务刚刚启动时，一切资源尚未初始化（<strong>冷启动</strong>），如果直接将QPS跑到最大值，可能导致服务瞬间宕机</p>\n<p>​\t\t3、排队等待：让请求以均匀的速度通过，阀值类型必须设成QPS，否则无效;不接受新的请求，但是也不直接拒绝，而是进队列先等一下，如果规定时间内能够执行，那么就执行，要是超时就算了。</p>\n<h4 id=\"服务降级-2\">服务降级</h4>\n<p><strong>降级逻辑的两种方式</strong></p>\n<p>①方式一：<code>FallbackClass</code>，无法对远程调用的异常做处理</p>\n<p>②方式二：<code>FallbackFactory</code>，<code>可以</code>对远程调用的异常做处理，我们<code>选择这种</code></p>\n<h4 id=\"服务熔断降级\">服务熔断降级</h4>\n<p>​\t1、熔断策略</p>\n<p>​\t\tSentinel 熔断降级会在调用链路中某个资源出现<code>不稳定状态时</code>（例如调用超时或异常比例升高），<code>对这个资源的调用进行限制</code>，让请求快速失败，避免影响到其它的资源而导致级联错误。 当资源被降级后，在接下来的降级时间窗口之内，对该资源的调用都<code>自动熔断</code>（默认行为是抛出 <code>DegradeException</code>）。</p>\n<p>​\t\t1.1、慢调用比例，RT（平均响应时间，秒级）：选择以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大的响应时间），请求的响应时间大于该值则统计为慢调用；当单位统计时长（statIntervalMs）内请求数目<code>大于</code>设置的最小请求数目，<strong>并且</strong>慢调用的比例大于阈值，则接下来的熔断时长内请求会自动被熔断；经过熔断时长后<code>熔断器</code>会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断。</p>\n<p>​\t\t1.2、异常比例：当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。</p>\n<p>​\t\t1.3、异常数：当单位统计时长内的异常数目超过阈值之后会自动进行熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。</p>\n<p>​\t2、对比</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Sentinel</th>\n<th>Hystrix</th>\n<th>resilience4j</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>隔离策路</td>\n<td>信号量隔离并发线程数限流）</td>\n<td>线程池隔离/信号量隔离</td>\n<td>信号量隔离</td>\n</tr>\n<tr>\n<td>熔断降级策路</td>\n<td>基于响应时间、异常比率、异常数</td>\n<td>基于异常比率</td>\n<td>基于异常比率、响应时间</td>\n</tr>\n<tr>\n<td>实时统计实现</td>\n<td>滑动窗口(LeapArray)</td>\n<td>滑动窗口（RxJava）</td>\n<td>Ring Bit Buffer</td>\n</tr>\n<tr>\n<td>动态规则配置</td>\n<td>支持多种数据源</td>\n<td>支持多种数据源</td>\n<td>有限支持</td>\n</tr>\n<tr>\n<td>扩展性</td>\n<td>多个扩展点</td>\n<td>插件的形式</td>\n<td>接口的形式</td>\n</tr>\n<tr>\n<td>基于注解的支持</td>\n<td>支持</td>\n<td>支持</td>\n<td>支持</td>\n</tr>\n<tr>\n<td>限流</td>\n<td>基于QPS,支持基于调用关系的限流</td>\n<td>有限的支持</td>\n<td>Rate Limiter</td>\n</tr>\n<tr>\n<td>流量整形</td>\n<td>支持预热模式、匀速器模式、预热排队模式</td>\n<td>不支持</td>\n<td>简单的Rate Limiter</td>\n</tr>\n<tr>\n<td>系统白适应保护</td>\n<td>支持</td>\n<td>不支持</td>\n<td>不支持</td>\n</tr>\n<tr>\n<td>控制台</td>\n<td>提供开箱即用的控制台，可配置规则、查看秒级监控、机器发现等</td>\n<td>简单的监控查看</td>\n<td>不提供控制台，可对接其它监控系统</td>\n</tr>\n</tbody>\n</table>\n<p>==可以看到，不管是线程隔离还是熔断降级，都是对<strong>客户端</strong>（调用方）的保护。需要在<strong>调用方</strong> 发起远程调用时做线程隔离、或者服务熔断；而我们的微服务远程调用都是基于Feign来完成的，因此我们需要将Feign与Sentinel整合，在Feign里面实现线程隔离和服务熔断==</p>\n<p>3、<code>@SentinelResource</code>  的 <strong>fallback 和 blockHandler</strong>属性： <code>fallback</code>管运行异常，<code>blockHandler</code>管配置违规。</p>\n<p>​\t\t3.1、\t<code>fallback</code>对应服务降级，就是服务出错了应该怎么办（需要有个兜底方法）；</p>\n<p>​\t\t3.2、\t<code>blockHandler</code>对应服务熔断，就是我现在服务不可用，应该怎么办，怎么给客户一个提示（同样需要一个兜底方法）；</p>\n<p>​\t\t3.3、\t降级是兜底方法；熔断是对服务保护时间窗口期服务不可用。注意：<code>blockHandler</code>处理优先级更高</p>\n<p>4、差异化配置</p>\n<ul>\n<li>\n<p>要是 <code>fallback  blockHandler</code>  都不配置的时候  会出现报错页面</p>\n</li>\n<li>\n<p>只配置<code>fallback</code>    当业务出现异常的时候会执行 <code>兜底的方法</code></p>\n</li>\n<li>\n<p>只配置<code>blockHandler</code>      当前<code>sentinel</code>配置已经<code>违规</code>（RT数过多、异常过多），服务熔断后不可用，<code>需要给客户提示，进行一个熔断的兜底</code></p>\n</li>\n<li>\n<p><code>fallback  blockHandler</code> 都配置         同时配置fallback：处理业务异常（微服务自身异常，服务降级）和blockHandler：处理触发sentinel配置（微服务不可用，服务熔断）时。在没有违反sentinel规则时，出现业务异常（降级）走fallback方法；违反了sentinel规则时，直接微服务不可用（熔断），走blockHandler指定的自定义方法。</p>\n</li>\n</ul>\n<p>5、异常忽略属性</p>\n<ul>\n<li>选择性的配置当某些异常发生时，不触发fallback的兜底方法。</li>\n</ul>\n<h4 id=\"热点key限流\">热点key限流</h4>\n<blockquote>\n<p>热点参数限流是<strong>分别统计参数值相同的请求</strong>，判断是否超过QPS阈值。所谓的 热点参数限流  就是  精准限流</p>\n</blockquote>\n<p>1、热点：热点即经常访问的数据，很多时候我们希望统计或者限制某个热点数据中<code>访问频次最高的TopN</code>数据，并对其访问进行限流或者其它操作</p>\n<p>2、热点参数：热点参数限制会统计传入参数中的热点参数，并根据配置的限流阈值与模式，对包含热点参数的资源调用进行限制。热点参数限流可以看作是一种<code>特殊的流量控制</code>，仅对包含热点参数的资源调用生效。Sentinel利用LRU策略统计最近最常访问的热点参数，结合<code>令牌桶算法</code>来进行<code>参数级别的流控</code>。热点参数限流<code>支持集群模式</code>。</p>\n<p>3、<code>@SentinelResource  只会处理配置出错的地方，运行出错该走异常走异常</code></p>\n<p>​\t3.1、该注解可以对   <code>资源名称 或者 指定的URL地址</code>进行限流，并指定兜底方案进行后续处理</p>\n<p>4、Sentinel 系统可以自适应限流，会<code>从整体维度对应用入口流量进行控制</code>，结合应用的 Load、CPU 使用率、总体平均 RT、入口 QPS 和并发线程数等几个维度的监控指标，<code>通过自适应的流控策略，让系统的入口流量和系统的负载达到一个平衡，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。</code></p>\n<p>​\t4.1、系统保护规则是应用整体维度的，而不是资源维度的，并且<strong>仅对入口流量（进入应用的流量）生效</strong>。</p>\n<p>​\t4.2、模式</p>\n<p>​\t\t1、<strong>Load 自适应</strong>（仅对 Linux/Unix-like 机器生效）：系统的 load1 作为启发指标，进行自适应系统保护。当系统 load1 超过设定的启发值，且系统当前的并发线程数超过估算的系统容量时才会触发系统保护（BBR 阶段）。系统容量由系统的 maxQps * minRt 估算得出。设定参考值一般是 CPU cores * 2.5。</p>\n<p>​\t\t2、<strong>CPU usage</strong>（1.5.0+ 版本）：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0），比较灵敏。</p>\n<p>​\t\t3、<strong>平均 RT</strong>：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒。</p>\n<p>​\t\t4、<strong>并发线程数</strong>：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。</p>\n<p>​\t\t5、<strong>入口 QPS</strong>：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护。</p>\n<p>5、微服务新增的<code>限流规则</code>会在微服务关闭后就会丢失，那么就说明<code>配置的限流规则都是临时的</code>，所以可以将限流配置规则持久化进<code>Nacos</code>保存（也可以持久化到<code>文件，redis，数据库</code>等），只要nacos里面的配置不删除；流控规则就持续存在</p>\n<p>6、4种是否超过流量阈值的判断</p>\n<p>​\t1、漏桶算法：见网关部分</p>\n<p>​\t2、令牌桶算法：见网关部分</p>\n<p>​\t3、滑动时间窗口算法：见网关部分</p>\n<p>​\t4、固定时间窗口算法：对某一个时间段内的请求进行统计和计数，但是每一个时间段又会有高并发的极端情况，（比如：<code>14:15:59</code>的时候来了100个请求，随后<code>14:16:01</code>的时候又来了100个请求，此时的 时间段 是<code>1分钟</code>）所以当遇到临界点时，固定时间窗口算法存在安全隐患。</p>\n<p>7、规则持久化：</p>\n<p>​\t1、规则是否能持久化，取决于规则管理模式，sentinel支持三种规则管理模式：原始模式、pull模式、push模式</p>\n<p>​\t\t1.1、原始模式：Sentinel的默认模式，将规则保存在内存，重启服务会丢失。</p>\n<p>​\t\t1.2、pull模式：控制台将配置的规则推送到Sentinel客户端，而客户端会将配置规则保存在本地文件或数据库中。以后会定时去本地文件或数据库中查询，更新本地规则。</p>\n<p>​\t\t1.3、push模式：控制台将配置规则推送到远程配置中心，例如Nacos。Sentinel客户端监听Nacos，获取配置变更的推送消息，完成本地配置更新。就是添加到 nacos 中；会涉及修改源码（略）</p>\n<h3 id=\"Seata\">Seata</h3>\n<blockquote>\n<p><code>Seata：Simple Extensible Autonomous Transaction Architecture，简单可扩展自治事务框架</code></p>\n</blockquote>\n<p>1、背景：在微服务项目中；每个服务内部的数据一致性由<strong>本地</strong>事务来保证，但是<strong>全局</strong>的数据一致性问题没法保证。</p>\n<p>2、解决：分布式事务   ;==Spring提供的<code>本地</code>事务：<code>@Transactional</code>     Seata提供的<code>全局</code>事务：<code>@GlobalTransactional</code>==</p>\n<h4 id=\"Steata-架构：-一ID-三组件\">Steata 架构：  一ID+三组件</h4>\n<p>一ID（<strong>全局唯一的事务ID</strong>）：Transaction ID<code> XID</code>，在这个事务ID下的所有事务会被统一控制</p>\n<p><strong>三组件</strong>：</p>\n<ul>\n<li>\n<p><strong>Transaction Coordinator (TC)</strong>：事务协调器，维护全局事务的运行状态，负责协调并驱动<code>全局事务的提交或回滚</code>；（Server端，为单独服务器部署）</p>\n</li>\n<li>\n<p><strong>Transaction Manager ™</strong>：事务管理器，控制全局事务的边界，负责<code>开启一个全局事务</code>，并最终发起<code>全局提交或全局回滚的决议</code>；</p>\n</li>\n<li>\n<p><strong>Resource Manager (RM)</strong>：资源管理器，控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动<code>分支（本地）事务的提交和回滚</code></p>\n</li>\n</ul>\n<blockquote>\n<p>Seata分TC、TM和RM三个角色，<strong>TC（Server端）为单独服务端部署</strong>，<strong>TM和RM（Client端）由业务系统集成（微服务）</strong>。</p>\n</blockquote>\n<h5 id=\"典型的分布式控制流程\">典型的分布式控制流程</h5>\n<ol>\n<li>TM 开启分布式事务（TM 向 TC 注册全局事务记录），全局事务创建成功并生成一个<strong>全局唯一的 XID</strong>；</li>\n<li>XID 在微服务调用链路的上下文中传播；（也就是在多个TM，RM中传播）</li>\n<li>RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖；</li>\n<li>TM 向 TC 发起针对 XID 的全局提交或回滚决议；</li>\n<li>TC 汇总事务信息，决定分布式事务是提交还是回滚；</li>\n<li>TC 调度 XID 下管辖的全部分支事务(RM)完成提交或回滚请求,事务二阶段结束</li>\n</ol>\n<img src=\"https://cdn.nlark.com/yuque/0/2021/png/22423156/1636511629414-0e4a7788-1488-40b8-8b5f-93f330a481b7.png\" alt=\"img\" style=\"zoom:63%;\">\n<blockquote>\n<p>TC：seata服务器； （本地启动的seata ）</p>\n<p>TM：事物的发起者，业务的入口。 哪个微服务使用了**@GlobalTransactional**哪个就是TM</p>\n<p>RM：事务的参与者，一个数据库就是一个RM。</p>\n</blockquote>\n<h4 id=\"四种分布式事务\">四种分布式事务</h4>\n<blockquote>\n<p>&lt;祥见：分布式&gt;</p>\n</blockquote>\n<h5 id=\"概述\">概述</h5>\n<p>Seata基于上述架构提供了四种不同的分布式事务解决方案：</p>\n<p>1、XA模式：强一致性分阶段事务模式，牺牲了一定的可用性，无业务侵入；但是要求数据库本身支持这种模式才可以。</p>\n<p>2、TCC模式：最终一致的分阶段事务模式，有业务侵入</p>\n<p>3、AT模式：最终一致的分阶段事务模式，无业务侵入，也是Seata的<code>默认模式</code></p>\n<p>4、SAGA模式：长事务模式，有业务侵入</p>\n<blockquote>\n<p>无论哪种方案，都离不开TC，也就是事务的协调者。</p>\n</blockquote>\n<h2 id=\"高可用\">高可用</h2>\n","_path":"post/96a41905.html","_link":"http://rycan.top/post/96a41905.html","_id":"cloioo9e3003mni0p8m390j7b"}}