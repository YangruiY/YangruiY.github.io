{"type":"getPostByPath","data":{"title":"Netty","date":"2023-09-13T15:45:11.000Z","description":"面试精选","categories":[{"name":"FaceToFace","_id":"clndbzy5z0019150p2g3yh0rm"}],"tags":[{"name":"Netty","_id":"clndbzy6a003p150p8qvl430c"}],"content":"<meta name=\"referrer\" content=\"no-referrer\">\n<h1>Netty</h1>\n<h3 id=\"1、三大IO模型\">1、三大IO模型</h3>\n<blockquote>\n<p>IO模型</p>\n<p>分类 ：  <code> 同步阻塞、同步非阻塞、同步多路复用、</code><s>异步阻塞（没有此情况）</s> 、<code>异步非阻塞</code></p>\n<p>同步：线程自己去获取结果（一个线程）;\t异步：线程自己不去获取结果，而是由其它线程送结果（至少两个线程）</p>\n<p>数据读取的时候 会切换至操作系统内核态来完成，读取又分为两个阶段，分别为：<code>等待数据阶段  +   复制数据阶段</code></p>\n<p>阻塞IO： 用户程序<code>等待阶段</code>被阻塞 ；触发一次系统调用</p>\n<p>非阻塞  IO ：指的就是<code>等待阶段</code>用户程序不会阻塞，但是当有数据之后到达<code>复制数据阶段</code>还是阻塞了的</p>\n<p>多路复用 ：（等待数据阶段是阻塞的   复制数据的阶段也是阻塞的  触发两次系统调用）</p>\n<p>阻塞<code>IO</code> 与 多路复用</p>\n<p>异步IO: 不会阻塞，线程1 在 等待数据阶段不会获取结果，直接返回，复制阶段会有另一个线程专门来获取结果，触发的回调函数会带有结果返回用户线程。</p>\n<p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202306160108962.png\" alt=\"image-20230616010759817\" style=\"zoom:33%;\"><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305232047942.png\" alt=\"image-20230523204737809\" style=\"zoom:43%;\"></p>\n</blockquote>\n<p><strong>1、BIO</strong>： 同步并阻塞（传统阻塞型）</p>\n<p>1、就是： 每一个连接对应一个线程，只要<code>客户端</code>有连接请求，那么<code>服务器端</code>就需要启动一个线程进行处理</p>\n<p>2、不好的地方：如果这个连接没有事情做，就会造成线程开销，但是可以通过<code>线程池</code>改善；适用于<code>连接数目比较小</code>且固定的架构，这种方式对服务器资源要求比较高</p>\n<p>3、使用的流程：</p>\n<p>​\t\t1、服务器端启动一个 <code>ServerSocket</code>；</p>\n<p>​\t\t2、客户端启动 <code>Socket</code> 对服务器进行通信，<code>默认情况</code>下服务器端需要对每个客户建立一个线程与之通讯；</p>\n<p>​\t\t3、客户端发出请求后，先咨询服务器是否有线程响应，如果没有则会等待，或者被拒绝；如果有响应，客户端线程会等待请求结束后，再继续执行</p>\n<p>4、问题：</p>\n<p>​\t\t1、每个请求都需要创建独立的线程，与对应的客户端进行数据 <code>Read</code>，业务处理，数据 <code>Write</code></p>\n<p>​\t\t2、当并发数较大时，需要创建大量线程来处理连接，系统资源占用较大</p>\n<p>​\t\t3、连接建立后，如果当前线程暂时没有数据可读，则线程就阻塞在 <code>Read</code> 操作上，造成线程资源浪费、</p>\n<p><strong>2、NIO</strong>：\t同步非阻塞</p>\n<p>1、一<code>个线程处理多个请求</code>（连接），即<code>客户端</code>发送的连接请求都会注册到<code>多路复用器(selector)</code>上，<code>多路复用器</code>轮询到连接有 <code>I/O</code> 请求就进行处理；适用于<code>连接数目多且连接比较短</code>（轻操作）的架构，比如聊天服务器，弹幕系统，服务器间通讯等</p>\n<p>2、三大核心部分：<code>channel(通道)  Buffer(缓存区) Selector(选择器) </code></p>\n<p>3、<code>NIO</code> 是<strong>面向缓冲区，面向块编程</strong>的。数据读取到一个缓冲区中，然后可以在缓冲区中前后移动，这就增加了处理过程中的灵活性，NIO 允许应用程序在等待数据到达时继续执行其他任务，而不必一直阻塞等待数据的到来。</p>\n<p>4、<code>NIO</code> 的非阻塞模式讲的就是：使一个线程从某通道发送请求或者读取数据，但是它仅能得到<code>目前可用</code>的数据，如果<code>目前没有数据可用</code>时，就什么都不会获取，而不是保持线程阻塞，所以直至<code>数据在可以读取之前</code>，该线程可以继续做其他的事情。</p>\n<p>5、非阻塞写也是如此，一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。</p>\n<p>**3、AIO：**异步非阻塞模式</p>\n<p>1、采用了 <code>Proactor</code> 模式，只有有效的请求才启动线程，它的特点是先由操作系统完成后才通知服务端程序启动线程去处理，一般适用于连接数较多且连接时间较长的应用</p>\n<p>2、适用场景：适用于<code>连接数目多且连接比较长</code>（重操作）的架构，比如相册服务器，充分调用 <code>OS</code> 参与并发操作</p>\n<p><strong>4、比较</strong></p>\n<p>1、NIO 和 BIO 的比较</p>\n<ol>\n<li><code>BIO</code> 以<code>流</code>的方式处理数据，而 <code>NIO</code> 以<code>块</code>的方式处理数据， <code>块I/O</code> 的效率比<code>流I/O</code> 高很多。</li>\n<li><code>BIO</code> 是阻塞的，<code>NIO</code> 则是非阻塞的。</li>\n<li><code>BIO</code> 基于字节流(<code>Input/Output Stream</code>)和字符流(<code>reader/writer</code>)进行操作；而 <code>NIO</code> 基于 <code>Channel</code>（通道）和 <code>Buffer</code>（缓冲区）进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。<code>Selector</code>（选择器）用于监听多个通道的事件（比如：连接请求，数据到达等），因此使用单个线程就可以监听多个客户端通道。</li>\n<li><code>Buffer</code>和<code>Channel</code>之间的数据流向是<code>双向</code>的</li>\n</ol>\n<p>​    2、BIO、NIO、AIO 对比表</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>BIO</th>\n<th>NIO</th>\n<th>AIO</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>IO模型</td>\n<td>同步阻塞</td>\n<td>同步非阻塞（多路复用）</td>\n<td>异步非阻塞</td>\n</tr>\n<tr>\n<td>编程难度</td>\n<td>简单</td>\n<td>复杂</td>\n<td>复杂</td>\n</tr>\n<tr>\n<td>可靠性</td>\n<td>差</td>\n<td>好</td>\n<td>好</td>\n</tr>\n<tr>\n<td>吞吐量</td>\n<td>低</td>\n<td>高</td>\n<td>高</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2、NIO概念\">2、NIO概念</h3>\n<p><strong>1、缓冲区</strong></p>\n<p>​\t本质上是一个<strong>可以读写数据的内存块</strong>，缓冲区对象能够跟踪和记录缓冲区的状态变化情况。<code>Channel</code> 提供从文件/网络读取数据的渠道，但是读取或写入的数据都必须经由 <code>Buffer</code>,是线程不安全的</p>\n<p>​\t<strong>1.1ByteBuffer</strong>:结构： <code>capacity</code>&lt;容量&gt;  +  <code>position</code>&lt;读到哪/写到哪 的 指针/索引下标&gt;  +   <code>limit</code>&lt;限制&gt;</p>\n<p>​\t\t<span style=\"color:red\">开始的时候：limit会和capacity 都指向最后的同一个位置，poition 指向开始的位置</span></p>\n<p>​\t\t<span style=\"color:red\">写模式下，position 是写入位置，limit 等于容量，每次写入一个字节之后，position就会后移</span></p>\n<p>​\t\t<span style=\"color:red\">读模式下 ，调用flip 切换为读 ，flip 动作发生后，position 切换为读取位置，limit 切换为读取限制</span></p>\n<p>​\t\t<span style=\"color:red\">继续写的话，那么就从上次未读完的地方继续写，调用 compact 方法，是把未读完的部分向前压缩，然后切换至写模式</span></p>\n<p>​\t\t<span style=\"color:red\"> 从头开始写的话，调用  clear()，clear 动作发生后，状态和初始的时候是一样的</span></p>\n<p><strong>2、通道</strong></p>\n<p>​\t1、类似于流，但是是有区别的，通道可以同时读写（双向的），而流只能读或者只能写(就是说是<code>单向的</code>)；通道可以实现异步读写数据；通道可以从缓冲读数据，也可以写数据到缓冲</p>\n<p>​\t2、Stream VS channel</p>\n<p>​\t<code>stream</code> 不会自动缓冲数据，<code>channel</code> 会利用系统提供的发送缓冲区、接收缓冲区（更为底层）</p>\n<p>​\t<code>stream</code> 仅支持阻塞 API，<code>channel</code> 同时支持阻塞、非阻塞 API，网络中  <code>channel</code> 可配合 <code>selector</code> 实现多路复用</p>\n<p>​\t二者均为<code>全双工</code>，即读写可以<code>同时</code>进行</p>\n<p><strong>3、选择器</strong></p>\n<p>​\t<code>selector</code> 的作用就是配合<code>一个线程</code>来管理<code>多个 channel</code>，获取这些 <code>channel</code> 上发生的事件，这些 <code>channel 工作在非阻塞模式</code>下，不会让线程只在一个 <code>channel</code> 上工作。适合连接数特别多，但流量低<code>[就是非频繁]</code>的场景；节约了线程的数量；减少了线程上下文切换</p>\n<p><code>Selector</code> 能够检测多个注册的通道上是否有事件发生（注意：多个 <code>Channel</code> 以事件的方式可以注册到同一个 <code>Selector</code>），如果有事件发生，便获取事件；然后针对每个事件进行相应的处理。</p>\n<p>思想就是： 调用 <code>selector</code> 的 <code>select() </code>会进行阻塞，直到 <code>channel</code> 发生了<code>读写就绪</code>事件，<code>select</code> 方法才会返回<code>这些事件</code>并交给 <code>thread</code> 来处理</p>\n<p><code>select()</code>何时不会阻塞：</p>\n<p>​\t1、事件发生时（客户端发起连接请求，会触发 <code>accept</code> 事件、客户端发送数据过来、客户端正常、异常关闭时，都会触发 <code>read</code> 事件，另外如果发送的数据大于 <code>buffer</code> 缓冲区也会触发多次读取事件、<code>channel</code> 可写，会触发 <code>write</code> 事件、在 <code>linux</code> 下 <code>nio bug</code> 发生时）</p>\n<p>​\t2、调用<code> selector.wakeup()</code>   、调用 <code>selector.close()</code>、   <code>selector</code> 所在线程<code> interrupt</code></p>\n<p>说明</p>\n<ul>\n<li>当线程从某客户端 <code>Socket</code> 通道进行读写数据时，若没有数据可用时，该线程可以进行其他任务；非阻塞 <code>IO</code> 的 空闲时间 常用于在其他通道上执行 <code>IO</code> 操作，所以单独的线程可以管理多个输入和输出通道</li>\n<li>由于读写操作都是非阻塞的，这就可以充分提升 <code>IO</code> 线程的运行效率，避免由于频繁 <code>I/O</code> 阻塞导致的线程挂起。所以一个 <code>I/O</code> 线程可以并发处理 <code>N</code> 个客户端连接和读写操作，这从根本上解决了传统同步阻塞 <code>I/O</code> 的问题，架构的性能、弹性伸缩能力和可靠性都得到了极大的提升。</li>\n</ul>\n<p>​\t<strong>3.1</strong>、SelectionKey：表示<code>Selector</code>和网络通道的注册关系，有四种关系：读、写、接受、连接</p>\n<p>​\t<strong>3.2</strong>、<code>ServerSocketChannel</code> 在服务器端监听新的客户端 <code>Socket</code> 连接</p>\n<p>​\t<strong>3.3</strong>、<code>SocketChannel</code>，网络 <code>IO</code> 通道，具体负责进行读写操作。<code>NIO</code> 把缓冲区的数据写入通道，或者把通道里的数据读到缓冲区。</p>\n<p><strong>4、Selector、Channel 和 Buffer 三者的关系</strong></p>\n<ol>\n<li>\n<p>每个 <code>Channel</code> 都会对应一个 <code>Buffer</code>。</p>\n</li>\n<li>\n<p><code>Selector</code> 对应一个线程，一个线程对应多个 <code>Channel</code>（连接）。</p>\n</li>\n<li>\n<p>程序切换到哪个 <code>Channel</code> 是<code>由事件Event</code>  决定的</p>\n</li>\n<li>\n<p><code>Selector</code> 会根据不同的事件，在各个<code>通道</code>上切换。</p>\n</li>\n<li>\n<p><code>Buffer</code> 就是一个<code>内存块</code>，底层是<code>有一个数组</code>。</p>\n</li>\n<li>\n<p>数据的读取写入是通过 <code>Buffer</code>，这个和 <code>BIO</code>是不同的</p>\n<p><code>BIO</code> 中要么是输入流，或者是输出流，<code>不能双向</code>，</p>\n<p>但是 <code>NIO</code> 的 <code>Buffer</code> 是可以读也可以写，需要 <code>flip</code> 方法切换 ；</p>\n</li>\n<li>\n<p><code>Channel</code> 是双向的，可以返回底层操作系统的情况，比如 <code>Linux</code>，底层的操作系统通道就是双向的。</p>\n</li>\n</ol>\n<p>5、NIO进行网络编程的步骤： 用于处理客户端连接和事件监听的基本步骤。</p>\n<ol>\n<li>\n<p>当客户端连接时，使用 <code>ServerSocketChannel</code> 来接受连接请求，并获取对应的 <code>SocketChannel</code>，用于后续的数据读写操作。</p>\n</li>\n<li>\n<p>使用 <code>Selector</code> 对事件进行监听，通过调用 <code>select()</code> 方法来等待有事件发生的通道，返回已准备就绪的通道个数。</p>\n</li>\n<li>\n<p>将 <code>SocketChannel</code> 注册到 <code>Selector</code> 上，使用 <code>register()</code> 方法进行注册。一个 <code>Selector</code> 可以注册多个 <code>SocketChannel</code>，并指定感兴趣的事件类型（如读、写等）。</p>\n</li>\n<li>\n<p>通过遍历 <code>SelectionKey</code> 集合，获取已准备就绪的事件。可以通过 <code>selectedKeys()</code> 方法获得这个集合。</p>\n</li>\n<li>\n<p>通过 <code>SelectionKey</code> 反向获取对应的 <code>SocketChannel</code>，可以使用 <code>channel()</code> 方法获取。</p>\n</li>\n<li>\n<p>使用获取到的 <code>SocketChannel</code> 进行业务处理，如读取数据、写入响应等。</p>\n</li>\n</ol>\n<p>6、弊端： <code>NIO</code>工作量大，bug 多，需要自己构建协议，<code>epoll</code>的空转会导致<code>cpu</code>占用率达到100%</p>\n<h3 id=\"3、0Copy\">3、0Copy</h3>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202306160112802.png\" alt=\"image-20230616011241775\" style=\"zoom:33%;\">\n<p>上述<code>1、2、3、4</code>的说明</p>\n<p>1、java 本身并不具备 IO 读写能力，因此 <code>read</code> 方法调用后，要从 java 程序的<strong>用户态</strong>切换至<strong>内核态</strong>，去调用操作系统内核（Kernel）的读能力，将数据读入<strong>内核缓冲区</strong>。这期间用户线程阻塞，操作系统使用 DMA（Direct Memory Access； 也可以理解为硬件单元，用来解放 cpu 完成文件 IO）来实现文件读，期间也不会使用 cpu；</p>\n<p>2、从<strong>内核态</strong>切换回<strong>用户态</strong>，将数据从<strong>内核缓冲区</strong>读入<strong>用户缓冲区</strong>（即 byte[] buf），这期间 cpu 会参与拷贝，无法利用 DMA；</p>\n<p>3、调用 write 方法，这时将数据从<strong>用户缓冲区</strong>（<code>即 byte[] buf</code>）写入 <strong>socket 缓冲区</strong>，cpu 会参与拷贝；</p>\n<p>4、接下来要向网卡写数据，这项能力 java 又不具备，因此又得从<strong>用户态</strong>切换至<strong>内核态</strong>，调用操作系统的写能力，使用 DMA 将 <strong>socket 缓冲区</strong>的数据写入网卡，不会使用 cpu</p>\n<blockquote>\n<p>可以看到的是 中间环节较多，java 的 IO 实际<code>是缓存的复制</code>，<code>底层的真正读写</code>是<code>操作系统</code>来完成的</p>\n<p>用户态与内核态的切换发生了 3 次(<code>1、2、4</code>)，这个操作比较重量级 ；并且这个过程中  数据拷贝了共 4 次</p>\n</blockquote>\n<p>NIO 对上述4次拷贝的优化为：</p>\n<p>​\t1、使用 <code>DirectByteBuffer</code>方法，</p>\n<p>​\t\t原理是: 将堆外内存映射到 jvm 内存中来直接访问使用</p>\n<p>​\t\t减少第2步的1次拷贝，此时用户态与内核态的<code>切换次数</code>没有减少；</p>\n<p>​\t2、使用  <code>sendFile</code> 方法，java 中对应着两个 <code>channel</code> 调用 <code>transferTo/transferFrom </code>方法拷贝数据，</p>\n<p>​\t\t原理是：java 调用 <code>transferTo</code> 方法后，要从 java 程序的<strong>用户态</strong>切换至<strong>内核态</strong>，使用 DMA将数据读入<strong>内核缓冲区</strong>，不会使用 cpu；数据从<strong>内核缓冲区</strong>传输到 <strong>socket 缓冲区</strong>，cpu 会参与拷贝；最后使用 DMA 将 <strong>socket 缓冲区</strong>的数据写入网卡，不会使用 cpu</p>\n<p>​\t\t此时就只发生了<code>一次</code>用户态与内核态的切换，但是数据拷贝了 <code>3</code> 次</p>\n<p>​\t3、java 调用<code>transferTo</code>方法后，要从 java 程序的<strong>用户态</strong>切换至<strong>内核态</strong>，使用 DMA将数据读入<strong>内核缓冲区</strong>，不会使用 cpu；只会将一些 offset 和 length 信息拷入 <strong>socket 缓冲区</strong>，几乎无消耗；使用 DMA 将 <strong>内核缓冲区</strong>的数据写入网卡，不会使用 cpu</p>\n<blockquote>\n<p>整个过程仅只发生了一次用户态与内核态的切换，数据拷贝了 <code>2</code> 次。所谓的【<code>零拷贝</code>】，并不是真正无拷贝，而是在<code>不会拷贝重复数据到 jvm 内存中</code></p>\n<p>零拷贝的优点有:  更少的用户态与内核态的切换  +   不利用 cpu 计算，减少 cpu 缓存伪共享   +   零拷贝适合小文件传输</p>\n</blockquote>\n<p>在Java程序中，常用的零拷贝技术包括<code>内存映射（mmap）和sendFile</code>。这些技术可以减少数据在用户空间和内核空间之间的拷贝次数，从而提高性能。</p>\n<ol>\n<li>\n<p>mmap：通过内存映射，将文件映射到内核缓冲区，并且<code>用户空间可以共享内核空间的数据</code>。在进行网络传输时，可以<code>减少内核空间到用户空间的拷贝次数</code>，适用于小数据量的读写操作。</p>\n</li>\n<li>\n<p>sendFile：sendFile的基本原理是<code>数据不经过用户态，直接从内核缓冲区进入SocketBuffer</code>，由于和用户态完全无关，可以<code>减少一次上下文切换</code>。sendFile适用于大文件传输，可以利用DMA（直接内存访问）方式，减少CPU拷贝。</p>\n</li>\n</ol>\n<p>区别：</p>\n<ul>\n<li><code>mmap</code>适合小数据量读写，<code>sendFile</code>适合大文件传输。</li>\n<li><code>mmap</code>需要4次上下文切换和3次数据拷贝，<code>sendFile</code>需要3次上下文切换和最少2次数据拷贝。</li>\n<li><code>sendFile</code>可以利用<code>DMA</code>方式减少CPU拷贝，而<code>mmap</code>不能。</li>\n</ul>\n<p>零拷贝的好处是减少数据复制，同时还带来其他<code>性能优势</code>，例如减少上下文切换、减少CPU缓存伪共享以及无需进行CPU校验和计算。在Java中，<code>transferTo</code>方法底层使用了零拷贝技术。</p>\n<h3 id=\"4、Netty的模型\">4、Netty的模型</h3>\n<p>1、优点：方便、高性能、安全、吞吐量高、延时低、资源消耗低、封装好了很多底层的网络细节，使用直接调API即可</p>\n<h4 id=\"线程模型\"><strong>线程模型</strong></h4>\n<blockquote>\n<p>线程模型、Netty模型、异步模型和Netty的基于事件驱动的异步非阻塞I/O模型是相关概念，它们描述了不同层面上的特性和设计原则。</p>\n<ol>\n<li>线程模型：描述了在应用程序中使用的线程数量和线程之间的关系。常见的线程模型包括单线程模型、多线程模型和主从线程模型等。线程模型决定了应用程序如何利用计算资源和处理并发请求。</li>\n<li>Netty模型：指的是Netty框架的整体架构和设计模式。Netty采用了事件驱动的模型，通过EventLoop和ChannelHandler等组件来处理网络事件和数据。Netty模型通过合理地组织和调度事件处理，提供了高性能和可扩展性的网络编程框架。</li>\n<li>异步模型：强调了在应用程序中进行I/O操作时的异步执行方式。异步模型允许应用程序在I/O操作进行的同时继续执行其他任务，而不需要等待I/O操作的完成。这样可以提高系统的并发性能和响应性。</li>\n<li>Netty的基于事件驱动的异步非阻塞I/O模型：这是Netty框架特有的特点，它结合了事件驱动、异步和非阻塞的设计原则。Netty通过EventLoop和ChannelHandler等组件，基于事件驱动的方式处理网络事件和数据，使用异步非阻塞的I/O操作来提高性能和可扩展性。</li>\n</ol>\n<p>整体上，Netty的架构模型是基于Reactor模式的，通过事件驱动和异步非阻塞的方式处理网络事件和数据。EventLoop负责监听事件并分发给对应的ChannelHandler进行处理，ChannelPipeline中的ChannelHandler按顺序处理事件和数据。</p>\n<p>Netty是基于<code>异步非阻塞I/O模型</code>的网络应用框架，它采用了事件驱动的方式处理IO操作，并通过Java <code>NIO的非阻塞特性和多路复用</code>实现高性能的网络通信。</p>\n</blockquote>\n<p><strong>1、BIO模型（传统阻塞I/O服务模型）</strong></p>\n<p>​\t\t1.1、特点：使用阻塞I/O模式进行数据的输入和输出,每个连接都需要独立的线程进行阻塞式的I/O操作，即当数据未准备好时，线程会被阻塞，直到数据准备就绪。</p>\n<p>​\t\t1.2、原理：每当用户发送IO请求的时候，内核就会暂停当前线程并添加到等待队列中，数据准备好后，<code>内核会将</code>数据<code>复制</code>到用户线程<code>缓冲区</code>，并将<code>用户线程</code>从<code>等待队列</code>中<code>唤醒</code>，使其<code>继续执行</code>后续指令</p>\n<p>​\t\t1.3、弊端：</p>\n<p>​\t\t\t1、并发很大的时候就会  创建大量的线程，会占用很大的系统资源</p>\n<p>​\t\t\t2、连接创建后，如果当前线程<code>暂时没有数据可读</code>，该线程会阻塞在 <code>Handler</code>对象中的<code>read</code> 操作，导致之前处理线程的资源浪费</p>\n<p><strong>2、Reactor 模式： 又称为 <code>反应器模式</code>   <code>分发者模式</code>   <code>通知者模式</code></strong></p>\n<p>2.1、复用资源的两种方式</p>\n<p>1、基于<code>线程池</code>复用线程资源：不必再为每个连接创建线程,将连接完成后的<code>业务处理任务</code>分配<code>给线程进行处理</code>，一个线程可以处理多个连接的业务。（解决上述弊端1）</p>\n<p>2、基于 <code>I/O</code> 复用模型：多个客户端进行连接，先把连接请求给<code>服务处理器ServiceHandler</code>。多个连接共用一个阻塞对象<code>ServiceHandler</code>（解决上述弊端2）</p>\n<p><strong>2.2、reactor模式</strong></p>\n<p><strong>1、思想</strong>：IO 复用+ 线程池；是通过事件驱动和非阻塞IO以及事件处理器（也称为Reactor）来监听和分发事件来实现  高并发的网络编程</p>\n<p><strong>2、原理</strong>：就是通过把一个或者多个输入同时传递给<code>服务处理器（ServiceHandler）的模式</code>（基于事件驱动），服务器端会处理传入的请求，并把他们<code>同步分派</code>到相应的处理线程，因此 <code>Reactor</code> 模式也叫 <code>Dispatcher</code> 模式，<code>Reactor</code> 模式使用 <code>IO</code> 复用监听事件，收到事件后，分发给某个线程（进程），这点就是<code>网络服务器高并发处理关键</code>；因为原先的请求会导致<code>多个Handler</code>阻塞，现在只用一个<code>ServiceHandler</code>阻塞</p>\n<p><strong>3、工作流程</strong>：Reactor启动，开始监听事件。当有事件发生时，Reactor会接收到通知。Reactor根据事件类型和事件源，将事件分发给相应的事件处理器。事件处理器执行相应的逻辑，可能包括读取数据、处理业务逻辑、发送数据等。处理完事件后，事件处理器将结果返回给Reactor。Reactor将结果发送给相应的客户端。</p>\n<p><strong>4、优点</strong>：能够处理大量并发连接，因为它使用了非阻塞I/O和事件驱动的方式；提高了系统的并发能力和性能</p>\n<p><strong>5、Reactor模式的主要组成部分包括：</strong></p>\n<p>​\t5.1、<code>Reactor</code>：负责监听和分发事件，通常是一个单独的线程。它使用一个事件循环来等待事件的发生，并根据事件类型调用相应的事件处理器进行处理。</p>\n<p>​\t5.2、<code>事件处理器（Handlers）</code>：负责具体的事件处理逻辑。每个事件处理器通常对应一种或多种事件类型,当事件发生时,Reactor会调用相应的事件处理器进行处理。</p>\n<p>​\t5.3、<code>事件（Events）</code>：表示发生的网络事件，如连接建立、数据可读、数据可写等。</p>\n<p><strong>6、实现方式</strong></p>\n<p>​\t6.1、单 <code>Reactor</code> 单线程： 单个Reactor负责监听和分发所有的事件，包括连接请求和数据读写等。它使用一个事件循环（Event Loop）在单个线程上处理所有的事件，通过非阻塞IO来实现异步处理。</p>\n<p>​\t<s>步骤流程：<code>Reactor</code> 对象通过 <code>Select</code> 监控客户端请求事件，收到事件后通过 <code>Dispatch</code> 进行分发；如果是建立连接请求事件，则由 <code>Acceptor</code> 通过 <code>Accept</code> 处理连接请求，然后创建一个 <code>Handler</code> 对象处理连接完成后的后续业务处理；如果不是建立连接事件，则 <code>Reactor</code> 会分发调用连接对应的 <code>Handler</code> 来响应；<code>Handler</code> 会完成 <code>Read</code> → 业务处理 → <code>Send</code>    的完整业务流程</s></p>\n<p>​\t\t适用于低并发场景，因为单线程无法同时处理多个事件。</p>\n<p>​\t6.2、单 <code>Reactor</code>多线程： 单个Reactor负责监听和分发事件，多个工作线程负责实际的事件处理。Reactor通过Selector监听事件，当有事件发生时，通过任务分发机制将事件分发给工作线程进行处理。通过线程池来管理和调度处理器线程，实现并发处理。</p>\n<p>​\t\t<s>步骤流程：<code>Reactor</code> 对象通过 <code>Select</code> 监控客户端请求事件，收到事件后，通过 <code>Dispatch</code> 进行分发；如果是建立连接请求，则由 <code>Acceptor</code> 通过 <code>accept</code> 处理连接请求，然后创建一个 <code>Handler</code> 对象处理完成连接后的各种事件；如果不是连接请求，则由 <code>Reactor</code> 分发调用连接对应的 <code>handler</code> 来处理（也就是说连接已经建立，后续客户端再来请求，那基本就是数据请求了，直接调用之前为这个连接创建好的handler来处理）；<code>handler</code> 只负责响应事件，不做具体的业务处理（这样不会使<code>handler</code>阻塞太久），通过 <code>read</code> 读取数据后，会分发给后面的 <code>worker</code> 线程池的某个线程处理业务。【业务处理是最费时的，所以将业务处理交给线程池去执行】；<code>worker</code> 线程池会分配独立线程完成真正的业务，并将结果返回给 <code>handler</code>；<code>handler</code> 收到响应后，通过 <code>send</code> 将结果返回给 <code>client</code></s></p>\n<p>​\t\t适用于中等并发场景，能够充分利用多核CPU的优势。</p>\n<p>​\t6.3、主从 <code>Reactor</code>多线程： <code>SubReactor</code>是可以有<code>多个</code>的，如果只有一个<code>SubReactor</code>的话那和<code>单 Reactor 多线程</code>就没什么区别了；包括一个主Reactor和多个从Reactor。主Reactor负责监听和分发连接请求，从Reactor负责监听和分发数据读写事件。主Reactor负责接收连接请求并将其分发给从Reactor，从Reactor负责处理实际的数据读写事件。</p>\n<p>​\t\t<s>步骤流程：<code>Reactor</code> 主线程 <code>MainReactor</code> 对象通过 <code>select</code> 监听连接事件，收到事件后，通过 <code>Acceptor</code> 处理连接事件；当 <code>Acceptor</code> 处理连接事件后，<code>MainReactor</code> 将连接分配给 <code>SubReactor</code>；<code>subreactor</code> 将连接加入到连接队列进行监听，并创建 <code>handler</code> 进行各种事件处理；当有新事件发生时，<code>subreactor</code> 就会调用对应的 <code>handler</code> 处理；<code>handler</code> 通过 <code>read</code> 读取数据，分发给后面的 <code>worker</code> 线程处理；<code>worker</code> 线程池分配独立的 <code>worker</code> 线程进行业务处理，并返回结果；<code>handler</code> 收到响应的结果后，再通过 <code>send</code> 将结果返回给 <code>client</code>；<code>Reactor</code> 主线程可以对应多个 <code>Reactor</code> 子线程，即 <code>MainRecator</code> 可以关联多个 <code>SubReactor</code></s></p>\n<p>​\t\t适用于高并发场景，能够有效地处理大量的并发连接。</p>\n<h4 id=\"Netty模型\">Netty模型</h4>\n<p>1、原理： 基于事件驱动和异步非阻塞的网络编程模型，通过事件和回调机制<code>处理</code>网络事件和数据，使用线程池<code>管理</code>处理网络事件的线程，并通过Pipeline模型处理网络数据。</p>\n<p>2、Netty的任务队列</p>\n<blockquote>\n<p><code>scheduleTaskQueue</code> 是Netty中的一个任务队列，用于存储定时任务的队列。在Netty中，可以使用<code>EventLoop</code>的<code>schedule()</code>方法来创建定时任务。这些定时任务会被添加到<code>scheduleTaskQueue</code>中，然后在指定的延迟时间后执行。<code>scheduleTaskQueue</code>是一个优先级队列，它根据任务的执行时间进行排序。这样可以确保在定时任务触发时，能够按照预期的顺序进行执行。主要交给 <code>NioEvenLoop</code>进行处理</p>\n</blockquote>\n<p>3、模型介绍：</p>\n<ul>\n<li><code>Netty</code> 抽象出两组线程池，<code>BossGroup</code> 专门负责接收客户端连接，<code>WorkerGroup</code> 专门负责网络读写操作。</li>\n<li><code>NioEventLoop</code> 表示一个不断循环执行处理任务的线程，每个 <code>NioEventLoop</code> 都有一个 <code>Selector</code>，用于监听绑定在其上的 <code>socket</code>网络通道。</li>\n<li><code>NioEventLoop</code> 内部采用串行化设计，从消息的 <strong>读取-&gt;解码-&gt;处理-&gt;编码-&gt;发送</strong>，始终由 <code>IO</code> 线程 <code>NioEventLoop</code> 负责</li>\n</ul>\n<p><code>NioEventLoopGroup</code> 下包含多个 <code>NioEventLoop</code></p>\n<ul>\n<li>每个 <code>NioEventLoop</code> 中包含有一个 <code>Selector</code>，一个 <code>taskQueue</code></li>\n<li>每个 <code>NioEventLoop</code> 的 <code>Selector</code> 上可以注册监听多个 <code>NioChannel</code></li>\n<li>每个 <code>NioChannel</code> 只会绑定在唯一的 <code>NioEventLoop</code> 上</li>\n<li>每个 <code>NioChannel</code> 都绑定有一个自己的 <code>ChannelPipeline</code></li>\n</ul>\n<h4 id=\"异步模型\">异步模型</h4>\n<blockquote>\n<p>异步的作用：提升的是：<code>效率</code>  \t<code>吞吐量（异步没有缩短响应时间反而有所增加，但是提升了吞吐量）</code>  <code>合理进行任务拆分</code></p>\n</blockquote>\n<p>​\t1、介绍:<code>Netty</code> 的异步模型是建立在 <code>future</code> 和 <code>callback</code> 之上的。<code>Netty</code> 中的 <code>I/O</code> 操作是异步的，包括 <code>Bind、Write、Connect</code> 等操作会简单的返回一个 <code>ChannelFuture</code>；调用者并不能立刻获得结果，而是通过 <code>Future-Listener</code> 机制，用户可以方便的<code>主动获取或者通过通知机制</code>获得 <code>IO</code> 操作结果。Netty的异步模型允许调用者在发起异步操作后继续执行其他任务，而不需要等待操作完成。调用者可以通过监听器或回调方法来处理操作完成的通知，并获取到操作的结果。这种异步模型能够提高系统的并发性能和响应性。</p>\n<p>​\t2、<code>Future</code>作用： 表示异步的执行结果,可以通过它提供的方法来检测执行是否完成，<code>ChannelFuture</code>表示一个异步的I/O操作的结果，它可以用于检查操作是否完成、获取操作的结果或添加监听器来处理操作完成的通知。通过添加监听器（Callback）到<code>ChannelFuture</code>上，当操作完成时，Netty会自动调用相应的回调方法，通知调用者操作的结果</p>\n<p>​\t3、 <code>Future-Listener</code> 机制：当 <code>Future</code> 对象刚刚创建时，处于非完成状态，调用者可以通过返回的 <code>ChannelFuture</code> 来获取操作执行的状态，<code>注册监听函数</code>来执行完成后的操作。</p>\n<p>​\t4、Future &amp; Promise</p>\n<p>1、<code> netty 中的 Future</code> 与<code> jdk 中的 Future</code> 同名，但是是<code>两个接口</code>，<code>netty 的 Future</code> 继承自 <code>jdk 的 Future</code>，而 <code>Promise</code> 又对 <code>netty Future</code> 进行了扩展</p>\n<p>2、<code>jdk Future</code> 只能<code>同步等待任务结束</code>（或成功、或失败）才能得到结果</p>\n<p>3、<code>netty Future</code> 可以<code>同步等待任务结束</code>得到结果，也可以<code>异步方式</code>得到结果，但都是<code>要等任务结束</code></p>\n<p>4、<code>netty Promise</code> 不仅有 <code>netty Future </code>的功能，而且脱离了<code>任务独立存在</code>，只作为<code>两个线程间传递结果的容器</code></p>\n<h3 id=\"5、核心组件\">5、核心组件</h3>\n<p>1、<code>BootStrap、ServerBootStrap</code> ：一个 <code>Netty</code> 应用通常由一个 <code>Bootstrap</code> 开始，主要作用是<code>配置</code>整个 <code>Netty</code> 程序，<code>串联</code>各个组件；<code>Netty</code> 中 <code>Bootstrap</code> 类是客户端程序的启动引导类，<code>ServerBootstrap</code> 是服务端启动引导类。</p>\n<p>2、<code>Future、ChannelFuture</code>: <code>Netty</code> 中所有的 <code>IO</code> 操作都是<code>异步</code>的，不能立刻得知消息是否被正确处理。需要等待它执行完成或者直接注册一个监听，具体的实现就是通过 <code>Future</code> 和 <code>ChannelFutures</code>，他们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件</p>\n<p>3、<code>Channel</code>:可获得当前网络连接的通道的状态,可获得网络连接的配置参数,可以提供异步的网络 <code>I/O</code> 操作(如建立连接，读写，绑定端口)，异步调用意味着任何 <code>I/O</code> 调用都不会立即返回，并且不保证在调用结束时所请求的 <code>I/O</code> 操作已完成；调用立即返回一个 <code>ChannelFuture</code> 实例，通过注册监听器到 <code>ChannelFuture</code> 上，可以 <code>I/O</code> 操作成功、失败或取消时回调通知调用方</p>\n<p>4、<code>Selector</code>:通过 <code>Selector</code> 一个线程可以<code>监听</code>多个连接的 <code>Channel</code> 事件。当向一个 <code>Selector</code> 中注册 <code>Channel</code> 后，<code>Selector</code> 内部的机制就可以自动不断地查询（<code>Select</code>）这些注册的 <code>Channel</code> 是否有已就绪的 <code>I/O</code> 事件（例如可读，可写，网络连接完成等），这样程序就可以很简单地使用一个线程高效地管理多个 <code>Channel</code></p>\n<p>5、<code>ChannelHandler（通道处理器）</code>：用于处理Channel上的事件和执行业务逻辑。它可以被添加到Channel的<code>ChannelPipeline</code>中，用于拦截和处理进出的数据。</p>\n<p>​\t5.1、<code>入站</code>处理器： <code>ChannelInboundHandlerAdapter</code> 的子类，主要用来<code>读取客户端数据，写回结果</code>；是按照 <code>addLast</code> 的<code>顺序</code>执行的</p>\n<p>​\t5.2、<code>出站</code>处理器： <code>ChannelOutboundHandlerAdapter</code> 的子类，主要对<code>写回结果进行加工</code>；是按照 <code>addLast</code> 的<code>逆序</code>执行的</p>\n<p>6、<code>Pipeline 和 ChannelPipeline</code>: <code>ChannelPipeline</code> 是保存 <code>ChannelHandler</code> 的 <code>List</code>，用于处理或拦截 <code>Channel</code> 的入站事件和出站操作）;<code>ChannelPipeline</code> 实现了一种<code>高级形式的拦截过滤器模式</code>，使<code>用户可以完全控制</code>事件的处理方式以及 <code>Channel</code> 中各个的 <code>ChannelHandler</code> 如何相互交互</p>\n<p>7、<code>ChannelHandlerContext</code>:包含一个具体的事件处理器 <code>ChannelHandler</code>，同时 <code>ChannelHandlerContext</code> 中也绑定了对应的 <code>pipeline</code> 和 <code>Channel</code> 的信息，方便对 <code>ChannelHandler</code> 进行调用。</p>\n<p>8、<code>ByteBuf（字节缓冲区）</code>：Netty中的数据容器，用于高效地存储和操作字节数据。</p>\n<p>​\t8.1、优势：</p>\n<p>​\t\t1、池化 - 可以重用池中 <code>ByteBuf</code> 实例，更节约内存，减少内存溢出的可能</p>\n<p>​\t\t2、<code>读写指针分离</code>，不需要像 <code>ByteBuffer</code> 一样需要进行<code>切换读写模式</code></p>\n<p>​\t\t3、可以<code>自动扩容</code></p>\n<p>​\t\t4、<code>支持链式调用</code>，使用更流畅</p>\n<p>​\t\t5、很多地方体现<code>零拷贝</code>，例如 slice、duplicate、CompositeByteBuf</p>\n<p>9、<code>EventLoop（事件循环）</code>：负责处理Channel上的事件，驱动网络事件的处理和I/O操作。每个Channel都会被分配到一个EventLoop上，一个EventLoop可以处理多个Channel</p>\n<h3 id=\"6、Protobuf\">6、Protobuf</h3>\n<p><code>Google Protobuf</code>是什么：是Google开发的一种<code>高效的数据交换格式</code>，也是一种轻量级的<code>数据序列化</code>框架。它可以将结构化的数据序列化、反序列化，并可用于网络传输、数据存储等场景。</p>\n<p><code>Netty</code>中的<code>Google Protobuf</code>:<code>Netty</code>提供了一种基于<code>Google Protobuf</code>的编解码器，可以方便地将<code>Protobuf</code>格式的消息进行<code>序列化和反序列化</code>，并与Netty的数据处理管道（<code>Pipeline</code>）无缝集成</p>\n<p><code>Netty</code> 本身自带的 <code>ObjectDecoder</code> 和 <code>ObjectEncoder</code> 可以用来实现 <code>POJO</code> 对象或各种业务对象的<code>编码和解码</code>，底层使用的仍是Java序列化技术,而Java序列化技术本身效率就不高，存在如下问题   无法跨语言  +   序列化后的体积太大，是二进制编码的5倍多  +    序列化性能太低</p>\n<p>解决方案：<code>Protobuf</code> 是以 <code>message</code> 的方式来管理数据的；使用 <code>protobuf</code> 编译器能自动生成代码，<code>Protobuf</code> 是将类的定义使用 <code>.proto</code> 文件进行描述。</p>\n<h3 id=\"7、TCP-粘包拆包\">7、TCP 粘包拆包</h3>\n<p>1、背景：TCP在发送数据报的时候会选择 <code>Nagle</code> 算法（将多次间隔较小且数据量小的数据，合并成一个大的数据块，然后进行封包）进行发送；直到满足条件才发出去（下面三个条件，满足一个就行了：数据达到了TCP的最大报文段大小（MSS）。接收方对之前发送的数据进行了确认。发送方的缓冲区已经没有剩余空间。 ）</p>\n<p>2、问题: 接收端就难于分辨出完整的数据包了，因为<code>面向流</code>的通信是<code>无消息保护边界的</code>；由于 <code>TCP</code> 未进行消息保护边界,所以<code>需要</code>在接收端<code>处理消息边界问题</code>，也就是我们所说的粘包、拆包问题</p>\n<p>3、什么是粘包、拆包</p>\n<p>​\t1、服务端一次接受到了两个数据包，==<code>D1</code> 和 <code>D2</code> 粘合在一起==，称之为 <code>TCP</code> 粘包</p>\n<p>​\t2、服务端分两次读取到了数据包，第一次读取到了完整的 <code>D1</code> 包和 <code>D2</code>包的部分内容，==第二次读取到了 <code>D2</code> 包的剩余内容==，这称之为 <code>TCP</code> 拆包</p>\n<p>4、产生粘包的原因：</p>\n<ul>\n<li>\n<p>应用层：接收方 <code>ByteBuf</code> 设置太大（Netty 默认 1024）</p>\n</li>\n<li>\n<p>滑动窗口：<code>假设</code>发送方 256 bytes 表示一个完整报文，但由于<code>接收方处理不及时</code>且<code>窗口大小足够大</code>，这 256 bytes 字节<code>就会缓冲在接收方的滑动窗口中</code>，当滑动窗口中<code>缓冲了多个报文就会粘包</code></p>\n</li>\n<li>\n<p>Nagle 算法：会造成粘包</p>\n</li>\n</ul>\n<p>5、半包现象产生的原因：</p>\n<ul>\n<li>应用层：接收方 ByteBuf <code>小于</code>实际发送数据量</li>\n<li>滑动窗口：<code>假设</code>接收方的窗口只剩了 128 bytes，发送方的报文大小是 256 bytes，这时放不下了，只能先发送前 128 bytes，等待 ack 后才能发送剩余部分，这就造成了半包</li>\n<li><code>MSS 限制</code>：当发送的数据超过 MSS 限制后，会将数据切分发送，就会造成半包「<code>MSS（Maximum Segment Size）限制</code>  是指 <code>TCP </code>协议中的窗口大小，它表示在 TCP 协议中，每个数据包可以传输的最大有效载荷大小」</li>\n</ul>\n<p>6、发生的本质原因是： TCP 是<code>流式协议，消息无边界</code></p>\n<p>7、解决方案：关键就是要解决服务器端每次读取数据长度的问题，这个问题解决，就不会出现服务器多读或少读数据的问题，从而避免的 <code>TCP</code> 粘包、拆包。</p>\n<p>​\t1、使用<code>自定义协议+编解码器</code>来解决</p>\n<p>​\t2、短链接，发一个包建立一次连接，这样连接建立到连接断开之间就是消息的边界，<code>缺点</code>效率太低；可以解决<code>粘包</code>问题,但是难以解决<code>半包</code>问题,因为接收方的缓冲区大小是有限的</p>\n<p>​\t3、每一条消息采用固定长度，但是数据包的大小不好把握，长度定的太大，浪费；长度定的太小，对某些数据包又显得不够</p>\n<p>​\t4、每一条消息采用分隔符，例如<code> \\n</code>，<code>缺点</code>需要转义；处理字符数据比较合适，但如果内容本身包含了分隔符（字节数据常常会有此情况），那么就会解析错误</p>\n<p>8、滑动窗口</p>\n<p>​\t1、TCP 以一个<code>段（segment）</code>为单位，每发送一个段就需要进行一次确认应答（ack）处理，但如果这么做，缺点是包的往返时间越长性能就越差，为了解决这个问题；引入了窗口概念，<code>窗口大小</code>即<code>决定了无需等待应答而可以继续发送的数据最大值</code></p>\n<p>​\t2、窗口实际就起到一个<code>缓冲区</code>的作用，同时也能起到<code>流量控制</code>的作用</p>\n<h3 id=\"8、Netty-实现RPC\">8、Netty 实现RPC</h3>\n<p>RPC：<code>RPC（Remote Procedure Call）</code>—远程过程调用，是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程 ,两个或多个应用程序都分布在不同的服务器上，它们之间的调用都像是本地方法调用一样</p>\n<blockquote>\n<p>常见的 <code>RPC</code> 框架：比较知名的如阿里的 <code>Dubbo</code>、<code>Google</code> 的 <code>gRPC</code>、<code>Go</code> 语言的 <code>rpcx</code>、<code>Apache</code> 的 <code>thrift</code>，<code>Spring</code> 的 <code>SpringCloud</code>。</p>\n</blockquote>\n<p>RPC的调用  具体流程：</p>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202306132254686.png\">\n<ul>\n<li>调用者(<code>Caller</code>)，调用远程API(<code>Remote API</code>)</li>\n<li>调用远程API会通过一个RPC代理(<code>RpcProxy</code>)</li>\n<li>RPC代理再去调用<code>RpcInvoker</code>(这个是PRC的调用者)</li>\n<li><code>RpcInvoker</code>通过RPC连接器(<code>RpcConnector</code>)</li>\n<li>RPC连接器用两台机器规定好的PRC协议(<code>RpcProtocol</code>)把数据进行编码</li>\n<li>接着RPC连接器通过RpcChannel通道发送到对方的PRC接收器(<code>RpcAcceptor</code>)</li>\n<li>PRC接收器通过PRC协议进行解码拿到数据</li>\n<li>然后将数据传给<code>RpcProcessor</code></li>\n<li><code>RpcProcessor</code>再传给<code>RpcInvoker</code></li>\n<li><code>RpcInvoker</code>调用<code>Remote API</code></li>\n<li>最后推给被调用者(<code>Callee</code>)</li>\n</ul>\n<p>RPC调用流程（服务消费方和服务提供方之间的远程调用过程）：</p>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202306132249146.png\" alt=\"image-20230613224952992\">\n<ol>\n<li>服务消费方（<code>client</code>）以本地调用方式调用服务</li>\n<li><code>client stub</code> 接收到调用后负责将方法、参数等封装成能够进行网络传输的消息体</li>\n<li><code>client stub</code> 将消息进行编码并发送到服务端</li>\n<li><code>server stub</code> 收到消息后进行解码</li>\n<li><code>server stub</code> 根据解码结果调用本地的服务</li>\n<li>本地服务执行并将结果返回给 <code>server stub</code></li>\n<li><code>server stub</code> 将返回导入结果进行编码并发送至消费者</li>\n<li><code>client stub</code> 接收到消息并进行解码</li>\n<li>服务消费方（<code>client</code>）得到结果</li>\n</ol>\n<p>小结：<code>RPC</code> 的目标就是将 <code>2 - 8</code> 这些步骤都封装起来，用户无需关心这些细节，可以像调用本地方法一样即可完成远程服务调用</p>\n<h3 id=\"9、网络编程\">9、网络编程</h3>\n<h4 id=\"多路复用\">多路复用</h4>\n<p>单线程可以配合 <code>Selector</code> 完成对多个 <code>Channel</code> 可读写<code>事件的监控</code>，这称之为<code>多路复用</code></p>\n<p><code>多路复用</code>仅针对<code>网络 IO</code>,普通文件 IO <code>无法</code>利用多路复用；如果不用 <code>Selector</code> 的非阻塞模式，线程大部分时间都在做无用功，而 <code>Selector</code> 能够保证</p>\n<p>1、有可连接事件时才去连接\t2、有可读事件才去读取\t3、有可写事件才去写入；限于网络传输能力，<code>Channel</code> 未必时时可写，一旦 <code>Channel</code> 可写，会触发 <code>Selector</code> 的可写事件</p>\n<h4 id=\"处理边界的方案\">处理边界的方案</h4>\n<p>方案一： 固定消息长度，数据包大小一样，服务器按预定长度读取，<code>缺点</code>是浪费带宽</p>\n<p>方案二： 按分隔符<code>\\n</code>拆分，<code>缺点</code>是效率低</p>\n<p>方案三： TLV 格式，即 Type 类型、Length 长度、Value 数据，类型和长度已知的情况下，就可以方便获取消息大小，分配合适的 buffer，<code>缺点</code>是 <code>buffer</code> 需要提前分配，如果内容过大，则影响 <code>server</code> 吞吐量</p>\n<h4 id=\"连接假死\">连接假死</h4>\n<p>原因</p>\n<ul>\n<li>网络设备出现故障，例如网卡，机房等，底层的 TCP 连接已经断开了，但应用程序没有感知到，仍然占用着资源。</li>\n<li>公网网络不稳定，出现丢包。如果连续出现丢包，这时现象就是客户端数据发不出去，服务端也一直收不到数据，就这么一直耗着</li>\n<li>应用程序线程阻塞，无法进行数据读写</li>\n</ul>\n<p>问题</p>\n<ul>\n<li>假死的连接占用的资源不能自动释放</li>\n<li>向假死的连接发送数据，得到的反馈是发送超时</li>\n</ul>\n<p>服务器端解决</p>\n<ul>\n<li>如果能收到客户端数据，说明没有假死。因此策略就可以定为<code>，每隔一段时间就检查这段时间内是否接收到客户端数据</code>，没有就可以判定为连接假死</li>\n</ul>\n<p>客户端定时心跳</p>\n<ul>\n<li>客户端可以定时向服务器端发送数据，只要这个时间间隔小于服务器定义的空闲检测的时间间隔，那么就能防止前面提到的误判，客户端可以定义心跳处理器</li>\n</ul>\n<h3 id=\"10、基于Netty手写RPC框架\"><code>10、基于Netty手写RPC框架</code></h3>\n","_path":"post/1c6ba3e2.html","_link":"http://rycan.top/post/1c6ba3e2.html","_id":"clndbzy65002j150pfpky5cga"}}