{"type":"getPostById","data":{"title":"单机redis基本概念","date":"2023-07-03T09:43:34.000Z","description":"单机模式下的redis基本介绍","categories":[{"name":"redis","_id":"cljmtkunp000ihi0p92vsho1q"}],"tags":[{"name":"redis","_id":"cljmtkunp000khi0ph1oh3nne"}],"content":"<meta name=\"referrer\" content=\"no-referrer\">\n\n\n<h2 id=\"NoSQL的特点：\"><a href=\"#NoSQL的特点：\" class=\"headerlink\" title=\"NoSQL的特点：\"></a><code>NoSQL</code>的特点：</h2><ol>\n<li><p>相对于<code>MySQL</code>这样的<code>关系型</code>数据库来说，<code>非关系型</code>的数据库是基于内存操作的，读写速度非常的快，性能好些，</p>\n</li>\n<li><p>关系型数据库一般使用<code>主从集群</code>的模式保证数据的一致性，进行数据的备份，即为垂直扩展；<code>非关系型</code>数据库是将<code>数据进行拆分</code>，分别存储在不同的机器上，用来保存海量的数据，解决<code>内存大小受限的问题</code>，即为水平扩展</p>\n</li>\n</ol>\n<h2 id=\"resdis的基本数据类型\"><a href=\"#resdis的基本数据类型\" class=\"headerlink\" title=\"resdis的基本数据类型\"></a><code>resdis</code>的基本数据类型</h2><h3 id=\"String\"><a href=\"#String\" class=\"headerlink\" title=\"String\"></a><code>String</code></h3><ul>\n<li>value 的数据类型有<code>string  int  float</code>   底层的存储方式是<code>字节数组形式</code></li>\n<li>其基本编码方式是RAW，基于简单动态字符串（SDS）实现，存储上限为512mb。</li>\n<li>底层实现⽅式：动态字符串sds 或者 long<ul>\n<li>String的内部存储结构⼀般是sds（Simple Dynamic String，可以动态扩展内存），但是如果⼀个String类型的value的值是数字，那么Redis内部会把它转成long类型来存储，从⽽减少内存的使用。</li>\n<li>如果存储的字符串是整数值，并且大小在LONG_MAX范围内，则会采用INT编码：直接将数据保存在RedisObject的ptr指针位置（刚好8字节），不再需要SDS了</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"hash\"><a href=\"#hash\" class=\"headerlink\" title=\"hash\"></a><code>hash</code></h3><ul>\n<li>与<code>Redis</code>中的<code>Zset</code>非常类似：<ul>\n<li>都是键值存储</li>\n<li>都需求根据键获取值</li>\n<li>键必须唯一</li>\n</ul>\n</li>\n<li>区别<ul>\n<li><code>zset</code>的键是<code>member</code>，值是<code>score</code>；<code>hash</code>的键和值都是任意值</li>\n<li><code>zset</code>要根据<code>score</code>排序；<code>hash</code>则无需排序</li>\n</ul>\n</li>\n<li><p>底层实现方式：压缩列表<code>ziplist</code> 或者<code>字典dict</code>，当Hash中数据项比较少的情况下，Hash底层才⽤压缩列表<code>ziplist</code>进⾏存储数据，随着数据的增加，底层的<code>ziplist</code>就可能会转成<code>dict</code></p>\n</li>\n<li><p>value 是一个无序字典,也是一个键值对，类似<code>Java</code>的 <code>HashMap</code></p>\n<ul>\n<li>应用： 可以针对每个字段做增删改查的操作</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"list\"><a href=\"#list\" class=\"headerlink\" title=\"list\"></a><code>list</code></h3><ul>\n<li><p>List结构类似一个双端链表，可以从首、尾操作列表中的元素：</p>\n</li>\n<li><p>类似于<code>Java</code>中的<code>LinkedList</code> ,可以看成是一个双向链表的结构，可以<code>正向的检索</code>和<code>反向的检索</code></p>\n<ul>\n<li>应用：<code>朋友圈点赞列表，评论列表</code></li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<details><summary>常见的四种统计</summary>\n<pre>聚合统计       统计多个集合元素的聚合结果  <font color=\"red\">交差并等集合统计</font></pre>\n<pre>排序统计       在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议使用ZSet</pre>\n<pre>二值统计       集合元素的取值就只有0和1两种。在钉钉上签到打卡的场景中，我们只用记录有签到(1)或没有签单(0)</pre>\n<pre>基数统计       指统计一个集合中<font color=\"red\">不重复的元素个数</font>，就是对集合去重复后剩余元素的计算</pre>\n</details>\n\n</blockquote>\n<h3 id=\"set\"><a href=\"#set\" class=\"headerlink\" title=\"set\"></a><code>set</code></h3><ul>\n<li>底层数据结构： <code>HashTable</code>，也就是Redis中的<code>Dict</code>，不过Dict是双列集合（可以存键、值对）</li>\n<li>为了查询效率和唯一性，set采用<code>HT编码（Dict）</code>。<code>Dict</code>中的key用来存储元素，value统一为null。</li>\n<li><p>当存储的所有数据都是整数，并且元素数量不超过<code>set-max-intset-entries</code>时，<code>Set</code>会采用<code>IntSet</code>编码，以节省内存</p>\n</li>\n<li><p>类似于<code>hashset</code> ;内部实现相当于一个特殊的字典，字典中所有的value都是一个值NULL，可以看做是一个<code>value</code>为<code>null</code>的<code>HashMap</code></p>\n<ul>\n<li>应用： <code>微信抽奖小程序。（SRANDMEMBER） 微信朋友圈共友点赞。（SINTER）   QQ推荐可能认识的人。（SDIFF）</code></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"SortSet\"><a href=\"#SortSet\" class=\"headerlink\" title=\"SortSet\"></a><code>SortSet</code></h3><ul>\n<li><p>底层数据结构：</p>\n<p><code>SkipList</code>：可以排序，并且可以同时存储score和ele值（member）</p>\n<p><code>HT（Dict</code>）：可以键值存储，并且可以根据key找value</p>\n</li>\n<li><p><code>ziplist</code>本身没有排序功能，而且没有键值对的概念，因此需要有zset通过编码实现：</p>\n<ul>\n<li>ZipList是连续内存，因此score和element是紧挨在一起的两个entry， element在前，score在后</li>\n<li>score越小越接近队首，score越大越接近队尾，按照score值升序排列</li>\n</ul>\n</li>\n<li><p>当元素数量不多时，HT和SkipList的优势不明显，而且更耗内存。因此zset还会采用ZipList结构来节省内存，不过需要同时满足两个条件：</p>\n<ul>\n<li>元素数量小于zset_max_ziplist_entries，默认值128</li>\n<li>每个元素都小于zset_max_ziplist_value字节，默认值64</li>\n</ul>\n</li>\n<li><p>每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表,score的值可以是 整型 和浮点型并且可以重复</p>\n<ul>\n<li>应用：<code>排行榜</code></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"HyprLoglog\"><a href=\"#HyprLoglog\" class=\"headerlink\" title=\"HyprLoglog\"></a><code>HyprLoglog</code></h3><ul>\n<li><p>一种概率数据结构，只会根据输入的元素来计算<code>计数</code>，<code>不会存储</code>输入的<code>元素本身</code></p>\n<ul>\n<li>应用： <code>网站文章的UV        网站某关键词的搜索数量        用户每天搜索的不同词条数目</code></li>\n</ul>\n</li>\n<li><p>什么是UV           Unique Visitor，独立访客，一般理解为客户端IP      <font color=\"red\">需要去重考虑</font></p>\n</li>\n<li><p>什么是PV            Page View，页面浏览量        不用去重</p>\n</li>\n<li><p>什么是DAU        Daily Active User，日活跃量用户，<font color=\"red\">登录或者使用了某个产品的用户数（去重复登录的用户）</font>    常用于反映网站、互联网应用或者网络游戏的运营情况</p>\n</li>\n<li><p>什么是MAU        Monthly Active User，月活跃用户量</p>\n</li>\n</ul>\n<h4 id=\"HyprLoglog的引入：\"><a href=\"#HyprLoglog的引入：\" class=\"headerlink\" title=\"HyprLoglog的引入：\"></a>HyprLoglog的引入：</h4><ul>\n<li>去重方式<ul>\n<li>HashSet</li>\n<li>bitmap： 所占内存巨大；bitmaps还是不适用大数据量下(亿级)的基数计数场景，<font color=\"red\">但是bitmaps方法是精确计算的。</font></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"解决：\"><a href=\"#解决：\" class=\"headerlink\" title=\"解决：\"></a>解决：</h4><ul>\n<li>概率算法<ul>\n<li><font color=\"red\">通过牺牲准确率来换取空间</font>，对于不要求<font color=\"blue\">绝对准确率</font>的场景下可以使用，因为<font color=\"red\">概率算法不直接存储数据本身</font>，通过一定的概率统计方法预估基数值，同时保证误差在一定范围内，由于又不储存数据故此可以大大节约内存.</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h4><ul>\n<li><p>只是进行不重复的基数统计，不是集合也不保存数据，只记录数量而不是具体内容</p>\n</li>\n<li><p>有误差，但是很低 0.81% （<code>1.04/sqrt(16384)</code>）；HyperLogLog提供不精确的去重计数方案</p>\n</li>\n</ul>\n<h4 id=\"亿级UV的Redis设计方案\"><a href=\"#亿级UV的Redis设计方案\" class=\"headerlink\" title=\"亿级UV的Redis设计方案\"></a>亿级UV的Redis设计方案</h4><h5 id=\"不可行的方案：\"><a href=\"#不可行的方案：\" class=\"headerlink\" title=\"不可行的方案：\"></a>不可行的方案：</h5><ul>\n<li>MySQL   高并发下，3000万的数据就需要分库分表了</li>\n<li>redis的hash结构存储   太占内存了</li>\n</ul>\n<h5 id=\"可行的方案：\"><a href=\"#可行的方案：\" class=\"headerlink\" title=\"可行的方案：\"></a>可行的方案：</h5><ul>\n<li>HyperLogLog   在Redis里面，每个HyperLogLog键只需要花费12KB内存，就可以计算接近2^64^个不同元素的基数</li>\n</ul>\n<blockquote>\n<p>为什么是只需要花费12Kb?</p>\n<p>Redist使用了2^14^=16384个桶，按照上面的标准差，误差为0.81%，精度相当高。Redis使用一个log型哈希值的前14个比特用来确定桶编号，剩下的50个比特用来做基数估计。而2^6^=64，所以只需要用6个比特表示下标值，在一般情况下，一个HyperLogLog数据结构(每个桶占6位)占用内存的大小为16384*6/8=12kB,Redis将这种情况称为密集(dense)存储。</p>\n</blockquote>\n<h3 id=\"Bitmap\"><a href=\"#Bitmap\" class=\"headerlink\" title=\"Bitmap\"></a><code>Bitmap</code></h3><ul>\n<li>一种统计<code>二值状态</code>的数据类型，由多个二进制位组成，本质是数组，但是值只能是0和1，默认是 0;可以极大的节约存储空间, 每一个二进制位都对应一个偏移量<ul>\n<li>应用：<code>打卡、签到        用户是否登陆过Y、N，比如软件的每日签到功能        电影、广告是否被点击播放过</code></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Stream\"><a href=\"#Stream\" class=\"headerlink\" title=\"Stream\"></a><code>Stream</code></h3><p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202307022236895.png\" alt=\"image-20230525002759535\"></p>\n<ul>\n<li>流，可以使用<code>流</code>来<code>实时记录</code>和<code>同时聚合事件</code>,就是  <code>Redis</code> 版的<code>MQ消息中间件+阻塞队列</code> ,它支持消息的持久化、支持自动生成全局唯一ID、支持ack确认消息的模式、支持消费组模式等，让消息队列更加的稳定和可靠。</li>\n</ul>\n<blockquote>\n<ul>\n<li>Message Content：消息内容</li>\n<li>Consumer group：消费组，通过<code>XGROUP CREATE</code>命令创建，同一个消费组可以有多个消费者</li>\n<li>Last_delivered_id：游标，每个消费组会有个游标<code>last_delivered_id</code>，任意一消费者读取了消息都会使</li>\n<li>游标 <code>last_delivered_id</code> 往前移动。</li>\n<li>Consumer：消费者，消费组中的消费者</li>\n<li>Pending_ids：消费者会有一个状态变量，用于记录被当前消费已读取但<code>未ack</code>的<code>消息Id</code>，如果客户端没有<code>ack</code>，这个变量里面的消息ID会越来越多，一旦某个消息被ack它就开始减少。这个<code>pending_ids</code>变量被官方称之为 <code>PEL(Pending Entries List)</code>，记录了当前已经被客户端读取的消息，但是还没有 ack (Acknowledge character：确认字符），它用来确保客户端至少消费了消息一次，而不会在网络传输的中途丢失了没处理</li>\n</ul>\n</blockquote>\n<h3 id=\"Bitfield\"><a href=\"#Bitfield\" class=\"headerlink\" title=\"Bitfield\"></a>Bitfield</h3><ul>\n<li>将一个<code>redis</code>字符串看作是<strong>一个由二进制位组成的数组</strong> 并能对变长位宽和任意没有字节对齐的指定整型<code>位域进行寻址和修改</code></li>\n<li>主要作用 ： 它能够将很多<code>小的整数储存到一个长度较大的位图</code>中，又或者<code>将一个非常庞大的键分割为多个较小的键来进行储存</code><ul>\n<li>位域修改</li>\n<li>溢出控制</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"持久化\"><a href=\"#持久化\" class=\"headerlink\" title=\"持久化\"></a>持久化</h2><h3 id=\"RDB\"><a href=\"#RDB\" class=\"headerlink\" title=\"RDB\"></a>RDB</h3><h4 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a><font color=\"red\">定义</font></h4><ul>\n<li>将某一时刻的数据以快照&lt;全量快照——所有的数据&gt;的方式写到磁盘上，也就是在指定的时间间隔内将内存中的数据集写入磁盘。写的是二进制文件</li>\n<li>之后恢复的时候直接将硬盘中的快照文件读回内存即可</li>\n<li>默认情况下，redis 会将数据库快照保存在<code>dump.rdb</code> 的文件中，<ul>\n<li>随后 可以设置配置文件<code>自动保存</code>数据集，也可以<code>手动</code>&lt;<code>save   bgsave</code>&gt;的形式保存数据集</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"何时触发\"><a href=\"#何时触发\" class=\"headerlink\" title=\"何时触发\"></a><font color=\"red\">何时触发</font></h4><ul>\n<li>配置文件中默认的快照配置</li>\n<li>手动<code>save/bgsave</code>命令</li>\n<li>执行<code>flushdb/fulshall</code>命令也会产生dump.rdb文件，但是也会将命令记录到dump.rdb文件中，恢复后依旧是空，无意义</li>\n<li>执行<code>shutdown且没有</code>设置开启AOF持久化</li>\n<li>主从复制时，<code>主节点自动触发</code></li>\n</ul>\n<h4 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a><font color=\"red\">优点</font></h4><ul>\n<li>适用于灾难恢复。  可以传输到远程数据中心或Amazon S3(可能已加密）的压缩文件</li>\n<li>提高了Redis 的性能。</li>\n<li>RDB在使用大数据集时仍能更快地重启。</li>\n<li>在副本上，RDB支持重启和故障转移后的部分重新同步。</li>\n<li>RDB文件非常适合备份。</li>\n<li>对数据完整性和一致性要求不高。</li>\n</ul>\n<h4 id=\"缺点\"><a href=\"#缺点\" class=\"headerlink\" title=\"缺点\"></a><font color=\"red\">缺点</font></h4><ul>\n<li>在一定间隔时间做一次备份，所以如果<code>redis</code>意外<code>down</code>掉的话，就会丢失从当前至最近一次快照期间的数据，<strong>快照之间的数据会丢失</strong></li>\n<li>内存数据的全量同步，如果数据量太大会导致<code>IO严重影响服务器性能</code></li>\n<li>RDB依赖于主进程的<code>fork</code>，在更大的数据集中，这可能会导致服务请求的瞬间延迟。fork的时候内存中的数据被克隆了一份，大致2倍的膨胀性，需要考虑</li>\n</ul>\n<h3 id=\"AOF\"><a href=\"#AOF\" class=\"headerlink\" title=\"AOF\"></a>AOF</h3><h4 id=\"定义-1\"><a href=\"#定义-1\" class=\"headerlink\" title=\"定义\"></a><font color=\"red\">定义</font></h4><ul>\n<li><p><font color=\"red\">以日志的形式来记录每个写操作</font>，随后追加到文件缓冲区中，再根据配置文件将文件写到磁盘中</p>\n</li>\n<li><p>之后redis重启的话就根据日志文件的内容将<code>写指令</code>  <code>从前到后执行一次</code>以完成数据的恢复工作</p>\n</li>\n<li><p>默认情况下，redis是没有开启AOF的。开启AOF功能需要设置配置：<code>appendonly yes</code></p>\n</li>\n</ul>\n<h4 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a><font color=\"red\">流程</font></h4><ol>\n<li><p>Client作为命令的来源，会有多个源头以及源源不断的请求命令。</p>\n</li>\n<li><p>在这些命令到达Redis Server 以后并不是直接写入AOF文件，会将这些命令先放入AOF缓存中进行保存。这里的AOF缓冲区存在的目的是当这些命令达到一定量以后再写入磁盘，避免频繁的磁盘IO操作。</p>\n</li>\n<li><p>AOF缓冲会根据AOF缓冲区<strong>同步文件的三种写回策略</strong>将命令写入磁盘上的AOF文件。</p>\n</li>\n<li><p>随着写入AOF内容的增加为避免文件膨胀，会根据规则进行命令的合并(<strong>又称AOF重写</strong>)，从而起到AOF文件压缩的目的。</p>\n</li>\n<li><p>当Redis Server服务器重启的时候会对AOF文件载入数据。</p>\n</li>\n</ol>\n<p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305251051356.png\" alt=\"image-20230525105157332\" style=\"zoom:53%;\"></p>\n<h4 id=\"写回策略\"><a href=\"#写回策略\" class=\"headerlink\" title=\"写回策略\"></a><font color=\"red\">写回策略</font></h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">配置项</th>\n<th style=\"text-align:center\">写回时机</th>\n<th style=\"text-align:center\">优点</th>\n<th style=\"text-align:center\">缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>Always</code></td>\n<td style=\"text-align:center\">同步写回</td>\n<td style=\"text-align:center\">可靠性高，数据基本不丢失</td>\n<td style=\"text-align:center\">每个写命令都要同步记录，性能影响较大</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>Everysec</code></td>\n<td style=\"text-align:center\">每秒写回</td>\n<td style=\"text-align:center\">性能适中</td>\n<td style=\"text-align:center\">宕机时丢失一秒内的数据</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>No</code></td>\n<td style=\"text-align:center\">操作系统控制的写回</td>\n<td style=\"text-align:center\">性能好</td>\n<td style=\"text-align:center\">宕机时丢失数据较多</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"变化\"><a href=\"#变化\" class=\"headerlink\" title=\"变化\"></a><font color=\"red\">变化</font></h4><ul>\n<li><p>redis7之前 aof 文件有且只有一个</p>\n</li>\n<li><p>redis7之后 aof文件变为了multi part</p>\n<ul>\n<li><strong>BASE: 表示基础AOF</strong>，它一般由子进程通过重写产生，该文件<code>最多只有一个</code>。</li>\n</ul>\n</li>\n</ul>\n<pre><code>- **INCR:表示增量AOF**，它一般会在AOFRW开始执行时被创建，该文件`可能存在多个`。\n- **HISTORY:表示历史AOF**，它由`BASE`和`INCR`变化而来，每次`AOFRW`成功完成时，本次`AOFRW`之前对应的`BASE`和`INCR` 都将变为`HISTORY`，`HISTORY类型的AOF会被Redis自动删除`。\n</code></pre><p>为了管理这些AOF文件，我们引入了一个<code>manifest (清单)</code>文件来跟踪、管理这些AOF。</p>\n<h4 id=\"优点-1\"><a href=\"#优点-1\" class=\"headerlink\" title=\"优点\"></a><font color=\"red\">优点</font></h4><ul>\n<li>使用AOF 持久化会使 Redis 更加持久。</li>\n<li>AOF 日志是一个仅附加日志，因此<code>不会出现寻道问题</code>，也<code>不会在断电时出现损坏问题</code>。</li>\n<li><p>当AOF 变得太大时，Redis 能够在<code>后台自动重写AOF</code>。</p>\n<ul>\n<li>重写是安全的，因为当 Redis继续附加到旧文件时，会使用创建当前数据集所需的最少操作集生成一个全新的文件，一旦第二个文件准备就绪，Redis 就会切换两者并开始附加到新的那一个。</li>\n</ul>\n</li>\n<li><p>AOF<code>以易于理解和解析的格式</code>依次包含所有操作的日志。可以轻松导出AOF文件。</p>\n</li>\n</ul>\n<h4 id=\"缺点-1\"><a href=\"#缺点-1\" class=\"headerlink\" title=\"缺点\"></a><font color=\"red\">缺点</font></h4><ul>\n<li><code>相同数据集</code>的数据而言<code>AOF文件要远大于</code>RDB文件，<code>恢复速度</code> <code>慢</code>于RDB</li>\n<li>AOF运行效率要<code>慢于RDB</code>，每秒<code>同步策略效率较好</code>，<code>不同步效率和RDB相同</code></li>\n</ul>\n<h4 id=\"重写机制\"><a href=\"#重写机制\" class=\"headerlink\" title=\"重写机制\"></a><font color=\"red\">重写机制</font></h4><ul>\n<li>当AOF文件的大小超过所设定的峰值时，Redis就会<strong><code>自动</code></strong>启动AOF文件的<code>内容压缩</code>。只保留可以恢复数据的最小指令集。</li>\n</ul>\n<p>何时触发：</p>\n<ul>\n<li>自动触发<ul>\n<li>满足配置文件中的选项后，Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时</li>\n</ul>\n</li>\n<li><strong>手动触发</strong><ul>\n<li>客户端向服务器发送<code>bgrewriteaof</code>命令</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>AOF文件重写并不是对原文件进行重新整理，而是直接读取服务器现有的键值对，然后用一条命令去代替之前记录这个键值对的多条命令，生成一个新的文件后去替换原来的AOF文件</p>\n</blockquote>\n<p>原理：</p>\n<ol>\n<li>在重写开始前，redis会创建一个“重写子进程”，这个子进程会读取现有的AOF文件，并将其包含的指令进行分析压缩并写入到一个临时文件中。</li>\n<li>与此同时，主进程会将新接收到的写指令一边累积到内存缓冲区中，一边继续写入到原有的AOF文件中（这样做是保证原有的AOF文件的可用性，避免在重写过程中出现意外。）</li>\n<li>当“重写子进程”完成重写工作后，它会给父进程发一个信号，父进程收到信号后就会将内存中缓存的写指令追加到新AOF文件中</li>\n<li>当追加结束后，redis就会用新AOF文件来代替旧AOF文件，之后再有新的写指令，就都会追加到新的AOF文件中</li>\n<li>重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件，这点和快照有点类似</li>\n</ol>\n<h3 id=\"混合模式持久化\"><a href=\"#混合模式持久化\" class=\"headerlink\" title=\"混合模式持久化\"></a>混合模式持久化</h3><p>两者在同时开启，也就是混合模式下，重启只会加载 <code>aof</code> 文件</p>\n<h4 id=\"同时开启两种持久化方式\"><a href=\"#同时开启两种持久化方式\" class=\"headerlink\" title=\"同时开启两种持久化方式\"></a><span style=\"color:red\">同时开启两种持久化方式</span></h4><ul>\n<li>当redis重启的时候会<code>优先载入AOF文件</code>来恢复原始的数据，因为在通常情况下<code>AOF文件保存的数据集</code>要比RDB文件保存的数据集<code>要完整</code>。</li>\n<li>RDB的数据不是实时的，同时使用两者时服务器重启也只会找AOF文件。 redis  的 作者也不建议只使用AOF方式备份，因为<code>RDB更适合用于备份数据库</code>（AOF在不断的变化不好备份），留着RDB是为了作为一个以防万一的手段。</li>\n</ul>\n<blockquote>\n<p>推荐使用<code>两者混合使用</code></p>\n<ul>\n<li><p>RDB镜像做<code>全量持久化</code>，AOF做<code>增量持久化</code></p>\n</li>\n<li><p>建议<code>先</code>使用RDB进行快照存储，<code>然后</code>使用AOF持久化记录所有的写操作，当重写策略满足或手动触发重写的时候，将最新的数据存储为新的RDB记录。</p>\n</li>\n<li><p>这样的话，重启服务的时候会从RDB和AOF两部分恢复数据，既保证了数据完整性，又提高了恢复数据的性能。</p>\n</li>\n<li><p>简单来说:混合持久化方式产生的文件一部分是RDB格式，一部分是AOF格式。====&gt;    AOF包括了RDB头部+AOF混写</p>\n</li>\n</ul>\n</blockquote>\n<h3 id=\"纯缓存模式\"><a href=\"#纯缓存模式\" class=\"headerlink\" title=\"纯缓存模式\"></a>纯缓存模式</h3><p><code>同时关闭</code>RDB+AOF，只用缓存</p>\n<h2 id=\"事务\"><a href=\"#事务\" class=\"headerlink\" title=\"事务\"></a>事务</h2><p>本质是一组命令的集合，一个事务中的所有命令都会序列化，<code>按顺序地串行化执行而不会被其他命令插入，不许加塞</code></p>\n<h3 id=\"特点\"><a href=\"#特点\" class=\"headerlink\" title=\"特点\"></a><font color=\"red\">特点</font></h3><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>单独的隔离操作</th>\n<th>Redis的事务仅仅是保证事务里的操作会被连续独占的执行，redis命令执行是单线程架构，在执行完事务内所有指令前是不可能再去同时执行其他客户端的请求的</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>没有隔离级别的概念</td>\n<td>因为事务提交前任何指令都不会被实际执行，也就不存在”事务内的查询要看到事务里的更新，在事务外查询不能看到”这种问题了</td>\n</tr>\n<tr>\n<td>不保证原子性</td>\n<td>Redis的事务不保证原子性，也就是不保证所有指令同时成功或同时失败，只有决定是否开始执行全部指令的能力，没有执行到一半进行回滚的能力</td>\n</tr>\n<tr>\n<td>排它性</td>\n<td>Redis会保证一个事务内的命令依次执行，而不会被其它命令插入</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"流程-1\"><a href=\"#流程-1\" class=\"headerlink\" title=\"流程\"></a><font color=\"red\">流程</font></h3><p>（1）开启：以<code>MULTI</code>命令开启一个事务</p>\n<p>（2）入队：将多个命令加入到事务队列中，接到这些命令并<code>不会立即执行</code>。</p>\n<p>（3）执行：由<code>EXEC</code>命令执行事务队列中的命令。</p>\n<h2 id=\"管道-pipeline\"><a href=\"#管道-pipeline\" class=\"headerlink\" title=\"管道(pipeline)\"></a>管道(pipeline)</h2><ul>\n<li><p>是什么： 客户端和redis服务器之间的进行建立连接的一种<code>双向通道</code>,它可以让客户端<code>在一次请求</code>中<code>发送多个命令</code>并<code>一次性</code>接收<code>多个命令的响应结果</code></p>\n</li>\n<li><p>目的: 可以让客户端减少网络通信的次数，从而<code>提高</code> Redis 的<code>吞吐量和性能</code></p>\n</li>\n<li>总结：<ul>\n<li>Pipeline与原生批量命令(<code>mset</code>)对比：<ul>\n<li>原生批量命令（例如mset、mget）具有<code>原子性</code>，pipeline是<code>非原子性</code>。</li>\n<li>原生批量命令一次<code>只能执行一种命令</code>，pipeline<code>支持批量执行不同命令</code>。</li>\n<li>原生批命令是<code>redis服务端</code>实现，而pipeline需要<code>redis服务端和客户端</code>共同完成。</li>\n</ul>\n</li>\n<li>Pipeline与<code>事务</code>对比：<ul>\n<li>事务具有原子性，pipeline不具有原子性。</li>\n<li>pipeline<code>一次性</code>将命令发送给服务器，事务是<code>一条一条</code>的发，事务只有在接收到EXEC命令后才会执行。</li>\n<li>执行事务时会阻塞其他命令的执行，而执行管道中的命令不会。</li>\n</ul>\n</li>\n<li>使用Pipeline注意事项：<ul>\n<li>pipeline缓冲的指令只会<code>依次执行，不保证原子性</code>，如果执行中指令发生异常，还会继续执行后续的指令。</li>\n<li>使用pipeline传输的命令也不能太多，如果数据量大客户端的阻塞时间可能会过久，同时服务端此时也被迫回复一个队列答复，占用很多内存。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"发布订阅\"><a href=\"#发布订阅\" class=\"headerlink\" title=\"发布订阅\"></a>发布订阅</h2><ul>\n<li><p>发布和订阅（Publish/Subscribe，简称 Pub/Sub）是一种消息传递模式；</p>\n</li>\n<li><p>发布者（Publisher）可以将消息发送到一个或多个频道（Channel），订阅者（Subscriber）可以订阅一个或多个频道，以接收发布者发送的消息。当发布者在某个频道上发布一条消息时，所有订阅该频道的订阅者都会收到这条消息。</p>\n</li>\n<li><code>Redis Pub/Sub</code> 是基于消息传递的<code>异步通信模型</code>，可以用于构建实时系统、聊天室、实时广播等应用场景。</li>\n</ul>\n<p>==注意==</p>\n<ul>\n<li><code>发布的消息</code>在Redis系统<code>不能持久化</code>，因此必须<code>先执行订阅</code>，再<code>等待消息发布</code>，如果先发布了消息且该消息<code>没有订阅者接收</code>，那么该消息被<code>直接丢弃</code>。</li>\n<li>消息只管发送，对于发布者而言消息是<code>即发即失</code>的，也<code>没有ACK机制</code>，无法保证消息是否消费成功。</li>\n<li>Redis5.0新增了<code>Stream数据结构，不但支持多播，还支持数据持久化，比Pub/Sub更加强大</code>。</li>\n</ul>\n<h2 id=\"BigKey\"><a href=\"#BigKey\" class=\"headerlink\" title=\"BigKey\"></a>BigKey</h2><blockquote>\n<details>\n <summary>使用 <font color=\"red\">scan</font> 而不是 <font color=\"red\">keys *</font></summary>\n <pre>1.生产环境中使用 ： <font color=\"red\">keys * </font>这个指令有致命的弊端，在实际环境中最好不要使用；一般使用的是<font color=\"red\">Scan</font></pre>\n<pre>2.SCAN命令是一个基于<font color=\"red\">游标</font>的迭代器，每次被调用之后，都会向用户<font color=\"red\">返回一个新的游标</font>，<font color=\"red\">用户在下次迭代时需要使用这个新游标作为SCAN命令的游标参数</font>，以此来延续之前的迭代过程。</pre>\n     <pre>3.命令<font color=\"red\">SCAN cursor [MATCH pattern] [COUNT count]</font></pre>\n     <pre>3.1 cursor - 游标。</pre>\n     <pre>3.2 pattern - 匹配的模式。</pre>\n     <pre>3.3 count - 指定从数据集里返回多少元素，默认值为 10 。</pre>\n<pre><font color=\"red\" size=\"4\">4.SCAN的遍历顺序</font></pre>\n<pre><font color=\"red\">非常特别，它不是从第一维数组的第零位一直遍历到末尾，而是采用了高位进位加法来遍历。之所以使用这样特殊的方式进行遍历，是考虑到字典的扩容和缩容时避免槽位的遍历重复和遗漏。</font></pre>\n<pre>5.返回值</pre>\n<pre>5.1 SCAN返回一个包含<font color=\"blue\">两个元素的数组</font></pre>\n<pre>5.2 第一个元素是用于进行下一次迭代的新游标，</pre>\n<pre>5.3 第二个元素则是一个数组，这个数组中包含了所有被迭代的元素。<font color=\"red\">如果新游标返回零表示迭代已结束。</font></pre></details>\n\n\n\n</blockquote>\n<h3 id=\"多大算BigKey\"><a href=\"#多大算BigKey\" class=\"headerlink\" title=\"多大算BigKey\"></a>多大算BigKey</h3><ul>\n<li><p>通常我们说的BigKey，不是在值的Key很大，而是指的Key对应的value很大</p>\n</li>\n<li><p>list、hash、set和zset，value的实际上个数超过5000就是bigkey</p>\n</li>\n<li><p>string是value，理论上最大是512MB，但是  实际上 ≥10KB就是bigkey</p>\n</li>\n<li><p>非字符串的bigkey,不要使用del删除，使用<code>hscan、sscan、zscan</code>方式<code>渐进式删除</code>，同时要注意防止<code>bigkeyi过期时间自动删除问题</code></p>\n</li>\n</ul>\n<h3 id=\"危害\"><a href=\"#危害\" class=\"headerlink\" title=\"危害\"></a>危害</h3><ul>\n<li>内存不均，集群迁移困难</li>\n<li>超时删除，大key删除作梗  ：对元素较多的hash、list、zset等做运算会耗时较旧，使主线程被阻塞</li>\n<li>网络流量阻塞 ：对BigKey执行读请求时，少量的QPS就可能导致带宽使用率被占满，导致Redis实例，乃至所在物理机变慢</li>\n<li>数据倾斜 ：   BigKey所在的Redis实例内存使用率远超其他实例，无法使数据分片的内存资源达到均衡</li>\n</ul>\n<h3 id=\"如何发现\"><a href=\"#如何发现\" class=\"headerlink\" title=\"如何发现\"></a>如何发现</h3><ul>\n<li><font color=\"red\">--bigkeys参数 + （查询大于10kb的所有key）memory usage  [给出一个 `key` 和它的值在 内存 中所占用的字节数。] </font>\n\n\n\n</li>\n</ul>\n<h3 id=\"如何删除\"><a href=\"#如何删除\" class=\"headerlink\" title=\"如何删除\"></a>如何删除</h3><ul>\n<li>String   一般用del，如果<code>过于庞大</code>使用<code>unlink key</code>删除</li>\n<li>hash    使用<code>hscan</code>每次获取少量<code>field-value</code>，再使用<code>hdel</code>删除每个<code>field</code></li>\n<li>list       使用<code>ltrim</code>渐进式逐步删除   [让列表只保留指定区间内的元素，不在指定区间之内的元素都将被除。]</li>\n<li>set      使用<code>sscan</code>每次获取部分元素，在使用<code>srem</code>命令删除每个元素</li>\n</ul>\n<ul>\n<li>zset     使用<code>zscan</code>每次获取部分元素，在使用<code>zremrangebyrank</code>命令删除每个元素</li>\n</ul>\n<h3 id=\"生产调优-—-”惰性释放“\"><a href=\"#生产调优-—-”惰性释放“\" class=\"headerlink\" title=\"生产调优   — ”惰性释放“\"></a>生产调优   — ”惰性释放“</h3><p>redis有两种删除的方式</p>\n<ul>\n<li><code>del</code>    阻塞型删除</li>\n</ul>\n<blockquote>\n<p>即 服务器停止处理新命令，以便以同步方式回收与对象关联的所有内存。</p>\n<ul>\n<li><p>如果删除的键与一个小对象相关联，则执行DEL命令所需的时间非常短  ，Redis中的O(1)或O(Iog_N)命令。</p>\n</li>\n<li><p>但是，如果键与包含数百万个元素的聚合值相关联，则服务器可能会阻塞很长时间（甚至几秒钟）才能完成操作。</p>\n</li>\n</ul>\n</blockquote>\n<ul>\n<li><code>unlink</code> 非阻塞型删除</li>\n</ul>\n<blockquote>\n<p>基于上述原因，Redis还提供了非阻塞删除原语，例如<code>UNLINK</code>(非阻塞DEL)以及<code>FLUSHALL</code>和<code>FLUSHDB</code>命令的<code>ASYNC</code>选项，以便在后台回收内存。这些命令在恒定时间内执行。另一个线程将尽可能快地逐步释放后台中的对象。<code>FLUSHALL</code>和<code>FLUSHDB</code>的<code>DEL</code>、<code>UNLINK</code>和<code>ASYNC</code>选项是用户控<br>制的。</p>\n</blockquote>\n<h3 id=\"优化配置\"><a href=\"#优化配置\" class=\"headerlink\" title=\"优化配置\"></a>优化配置</h3><blockquote>\n<p><code>lazyfree-lazy-server-del        yes</code></p>\n<p><code>replica-lazy-flush              yes</code></p>\n<p><code>lazyfree-lazy-user-del          yes</code></p>\n</blockquote>\n<h2 id=\"布隆过滤器\"><a href=\"#布隆过滤器\" class=\"headerlink\" title=\"布隆过滤器\"></a>布隆过滤器</h2><p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305262258692.png\" alt=\"image-20230526225827642\" style=\"zoom:43%;\"></p>\n<h3 id=\"是什么\"><a href=\"#是什么\" class=\"headerlink\" title=\"是什么\"></a><font color=\"red\">是什么</font></h3><ul>\n<li>由<code>一个初值都为零的bit数组</code>和<code>多个哈希函数构成</code>，用来快速判断集合中是否存在某个元素  <code>&lt;bit数组+hash函数&gt;</code></li>\n</ul>\n<h3 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a><font color=\"red\">目的</font></h3><ul>\n<li>可以减少内存占用；因为他 <code>不保存数据信息</code>，只是在内存中做一个是否存在的标记flag</li>\n</ul>\n<h3 id=\"特点-1\"><a href=\"#特点-1\" class=\"headerlink\" title=\"特点\"></a><font color=\"red\">特点</font></h3><ul>\n<li>可以<code>高效地插入和查询</code>，占用空间少，返回的结果是不确定的</li>\n<li>一个元素的判断结果：判断结果为<code>存在</code>时，元素<code>不一定存在</code>，但是判断结果为<code>不存在</code>时，则<code>一定不存在</code></li>\n<li>布隆过滤器可以添加元素，但是<font color=\"red\">不能删除元素，</font>由于涉及<code>hashcode</code>判断依据，删除元素会导致误判率增加。</li>\n</ul>\n<h3 id=\"原理-1\"><a href=\"#原理-1\" class=\"headerlink\" title=\"原理\"></a><font color=\"red\">原理</font></h3><ul>\n<li>实质就是<font color=\"red\">一个大型位数组和几个不同的无偏hash函数</font>(无偏表示分布均匀)。由一个初值都为零的bit数组和多个哈希函数构成，用来快速判断某个数据是否存在。</li>\n<li>但是跟 HyperLogLog 一样，它也一样有那么一点不精确，也存在一定的误判概率</li>\n</ul>\n<h3 id=\"数据结构\"><a href=\"#数据结构\" class=\"headerlink\" title=\"数据结构\"></a><font color=\"red\">数据结构</font></h3><ul>\n<li><p><font color=\"blue\">添加key时</font><br>使用<code>多个</code>hash函数对<code>key</code>进行<code>hash</code>运算得到一个整数索引值，对<code>位数组</code>长度进行<code>取模</code>运算得到一个位置，每个<code>hash函数</code>都会得到一个不同的位置，将这<code>几个位置</code>都置<code>1</code>就完成了<code>add</code>操作。</p>\n</li>\n<li><p><font color=\"blue\">查询key时</font><br>只要有其中一位是零就表示这个key不存在，但如果都是1，则不一定存在对应的值</p>\n</li>\n</ul>\n<h3 id=\"数据不精准的原因分析\"><a href=\"#数据不精准的原因分析\" class=\"headerlink\" title=\"数据不精准的原因分析\"></a>数据不精准的原因分析</h3><ul>\n<li>直接原因就在于 <code>哈希函数</code>会导致<code>哈希冲突</code></li>\n<li>当有变量被加入集合时，通过<code>N个映射函数</code>将这个变量映射成位图中的<code>N个点</code>,把它们都要置为 1，当查询某个变量的时候我们只要看看这些点是不是都是 <code>1</code>，就可以大概率知道集合中有没有它了；如果这些点，<font color=\"red\">有任何一个为零则被查询变量一定不在;</font>如果都是 1，则被查询变量很<font color=\"red\">可能存在</font>，<font color=\"red\">为什么说是可能存在，而不是一定存在呢?那是因为`映射函数本身就是散列函数`，`散列函数是会有碰撞的`。</font>\n\n\n</li>\n</ul>\n<blockquote>\n<p>正是基于布隆过滤器的<code>快速检测特性</code>，我们可以在把数据写入数据库时，使用布隆过滤器做个标记。</p>\n<p>当缓布缺失后，应用查询数据库时，可以通过查询布隆过滤器快速判断数据是否存在。</p>\n<p>如果不存在，就不用再去据库中查询了。</p>\n<p>这样一来，即使发生缓存穿透了，大量请求只会查询Redis和布隆过滤器，而不会积压到数据库，也就不会影响数据库的正常运行。</p>\n<p>布隆过滤器可以使用Redis实现，本身就能承担较大的并发访问压力</p>\n</blockquote>\n<h3 id=\"如何使用\"><a href=\"#如何使用\" class=\"headerlink\" title=\"如何使用\"></a><font color=\"red\">如何使用</font></h3><ul>\n<li>初始化bitmap       所有的值均设置为0</li>\n<li>添加数据                为了尽量使得地址不冲突，<font color=\"red\">会使用多个 hash 函数对 key 进行运算</font>，算得一个下标索引值，然后对位数组长度进行<font color=\"red\">取模运算</font>得到一个位置，每个 hash 函数都会算得一个不同的位置。再把位数组的这几个位置都置为 1 就完成了 add 操作。</li>\n<li>判断是否存在         先把这个 key 通过相同的<font color=\"red\">多个 hash 函数进行运算</font>，查看对应的位置是否都为 1，<font color=\"red\">只要有一个位为零，那么说明布隆过滤器中这个 key 不存在；</font><font color=\"red\">如果这几个位置全都是 1，那么说明极有可能存在；</font></li>\n</ul>\n<h3 id=\"即使误判也不要删除\"><a href=\"#即使误判也不要删除\" class=\"headerlink\" title=\"即使误判也不要删除\"></a><font color=\"red\">即使误判也不要删除</font></h3><ul>\n<li>误判的根源在于相同的 bit 位被多次映射且置 1</li>\n<li>布隆过滤器的每一个 bit 并不是独占的，很有可能多个元素共享了某一位。如果我们直接删除这一位的话，会影响其他的元素</li>\n<li>删掉元素会导致误判率增加。</li>\n</ul>\n<h3 id=\"建议\"><a href=\"#建议\" class=\"headerlink\" title=\"建议\"></a><font color=\"red\">建议</font></h3><ul>\n<li>使用时最好不要让实际元素数量远大于初始化数量，最好避免扩容</li>\n<li>当实际元素数量超过初始化数量时，应该对布隆过滤器进行重建，重新分配一个size 更大的过滤器，再将所有的历史元素批量add到新分配的布隆过滤器中</li>\n</ul>\n<h3 id=\"使用场景\"><a href=\"#使用场景\" class=\"headerlink\" title=\" 使用场景\"></a><font color=\"red\"> 使用场景</font></h3><ol>\n<li><font color=\"red\"> 解决缓存穿透的问题，和redis结合bitmap使用</font>\n\n<ul>\n<li><p>思路</p>\n<ul>\n<li><p>把已存在数据的key存在布隆过滤器中，相当于redis前面挡着一个布隆过滤器。当有新的请求时，先到布隆过滤器中查询是否存在:</p>\n<p>如果布隆过滤器中<code>不存在</code>该条数据则直接返回;如果布隆过滤器中<code>已存在</code>，才去查询缓存redis，如果redis里没查询到则再查询Mysql数据库</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><font color=\"red\"> 黑名单校验，识别垃圾邮件</font>\n\n<ul>\n<li>思路<ul>\n<li>发现存在黑名单中的，就执行特定操作。比如:识别垃圾邮件，只要是邮箱在黑名单中的邮件，就识别为垃圾邮件。把所有黑名单都放在布隆过滤器中，在收到邮件时，判断邮件地址是否在布隆过滤器中即可。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"优点：\"><a href=\"#优点：\" class=\"headerlink\" title=\"优点：\"></a><font color=\"red\">优点：</font></h3><ul>\n<li>高效地插入和查询，内存中占用bit空间小</li>\n</ul>\n<h3 id=\"缺点：\"><a href=\"#缺点：\" class=\"headerlink\" title=\"缺点：\"></a><font color=\"red\">缺点：</font></h3><ul>\n<li><code>不能删除</code>元素[<code>布谷鸟过滤器</code>可以删除]，因为删除元素会导致误判率增加，因为hash冲突同一个位置可能存的东西是多个共有的</li>\n<li>存在误差，<code>不能精准过滤</code></li>\n</ul>\n","_path":"post/691706ae.html","_link":"http://rycan.top/post/691706ae.html","_id":"cljmtkuny0029hi0p5bm49mto"}}