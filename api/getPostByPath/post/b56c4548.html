{"type":"getPostByPath","data":{"title":"集群下的redis","date":"2023-07-03T09:50:42.000Z","description":"redis 集群","categories":[{"name":"redis","_id":"clkxtzmnr001k0g0papsiguyh"}],"tags":[{"name":"redis","_id":"clkxtzmny003a0g0pghdt2c9p"}],"content":"<meta name=\"referrer\" content=\"no-referrer\">\n<h2 id=\"单机redis的隐患\">单机redis的隐患</h2>\n<p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202307050044253.png\" alt></p>\n<h2 id=\"redis的主从复制\">redis的主从复制</h2>\n<h3 id=\"复制的引入\">复制的引入</h3>\n<p>单节点Redis的<code>并发能力是有上限</code>的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。</p>\n<p>主机数据更新后根据配置和策略， 自动同步到备机的<code> master/slaver 机制</code>，<code>Master</code> 以<code>写</code>为主，<code>Slave</code> 以<code>读</code>为主，主从复制节点间数据是<code>全量</code>的。</p>\n<h3 id=\"复制的好处\">复制的好处</h3>\n<ul>\n<li>可以提高系统的<code>可用性、可靠性和扩展性</code>，使 <code>redis</code> 在发生故障时可以快速地恢复数据。</li>\n</ul>\n<h3 id=\"复制的过程-原理\">复制的过程/原理</h3>\n<p>（1）从节点向主节点发送 <code>SYNC</code> 命令,从节点<code>第一次</code>与主节点进行连接会进行一次全量复制（完全同步），slave自身原有数据会被master数据覆盖清除</p>\n<p>（2）随后，在主节点接收到<code> SYNC 命令</code>后，开始在<code>后台保存快照</code>，生成 RDB 文件，收集并缓存此过程中记录所有执行的写命令。</p>\n<p>（3）主节点在执行完<code>rdb持久化</code>后，将生成的 <code>RDB 文件和所有的缓存命令</code>发送给<code>从节点</code>，完成一次完全同步。</p>\n<p>（4）从节点接收到主节点发送的 RDB 文件和写命令，会将其进行<code>存盘</code>，并<code>加载到内存</code>中，<code>完成复制初始化</code>，来保持与主节点的数据一致。</p>\n<p>（5）从节点持续监听主节点发来的新命令，并将其执行，以保持与主节点的数据同步。</p>\n<p>（6）为了<code>保持</code>主节点和从节点之间的<code>通信</code>，<code>master</code>会发出<code>PING</code>包的周期默认是10秒（：<code>repl-ping-replica-period 10</code>在661行）</p>\n<p>（6）当主节点发生故障时，从节点会尝试与其他主节点建立连接，并选举出一个新的主节点（手动  slave of|自动 哨兵），从而成为新的从节点，<code>[注意：默认情况下，不会在slave节点中自动选一个master]</code></p>\n<p>（7）从机下线重新连接的时候，<code>master</code>会进行检查<code>backlog</code>里面的<code>offset</code>,<code>master</code>只会把已经缓存的<code>offset后面</code>的数据复制给<code>slave</code></p>\n<h3 id=\"复制的两种方式\">复制的两种方式</h3>\n<ul>\n<li>\n<p>全量复制：slave 服务器在接收到数据库文件数据后，将其存盘并加载到内存中。</p>\n</li>\n<li>\n<p>增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave，完成同步。</p>\n</li>\n<li>\n<p>但是只要是重新连接 master，一次完全同步（全量复制) 将被自动执行。</p>\n</li>\n</ul>\n<blockquote>\n<p>Redis 复制是异步的，因此从节点可能存在数据不一致的情况。</p>\n<p>为了避免数据不一致，可以设置 Redis 的<code>复制偏移量</code>（<code>replication offset</code>），当从节点与主节点连接断开后，从节点可以通过该偏移量快速地同步数据。</p>\n</blockquote>\n<h3 id=\"复制的作用\">复制的作用</h3>\n<ul>\n<li>\n<p>读写分离,性能扩展</p>\n</li>\n<li>\n<p>容灾快速恢复</p>\n</li>\n</ul>\n<h3 id=\"三种主从复制的实现方式\">三种主从复制的实现方式</h3>\n<ul>\n<li>一主二仆    从机宕机之后重启变为主机，主机重启之后仍然是主机</li>\n<li>薪火相传   &lt;<code>上一个Slave</code>可以是<code>下一个slave</code>的<code>Master</code>，Slave同样可以接收其他 slaves的连接和同步请求，那么<code>该slave</code>作为了链条中<code>下一个</code>的<code>master</code>, 可以<code>有效减轻master的写压力</code>,去中心化降低风险。&gt;     主机挂了，从机还是从机，无法写数据了</li>\n<li>反客为主：当一个 master 宕机后，后面的 slave 可以立刻升为 master，其后面的 slave 不用做任何修改。\n<ul>\n<li>用 <code>slaveof no one</code> 指令将从机变为主机。</li>\n<li>而<code>哨兵模式</code>是反客为主的<code>自动版</code>，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"缺点-引出哨兵和集群\">缺点(引出<code>哨兵和集群</code>)</h3>\n<p>（1）<em>数据同步延迟</em>：由于 Redis 复制是异步的，从节点的数据可能会与主节点存在一定的延迟，因此从节点可能无法实时获取到最新的数据。</p>\n<p>（2）<em>单点故障</em>：当主节点发生故障时，需要手动进行故障转移或者使用<code>集群</code>来保证系统的可用性。</p>\n<p>（3）<em>网络通信问题</em>：当网络出现故障或者通信延迟过高时，复制的效率会受到影响，从节点可能无法及时接收到主节点发送的数据。</p>\n<p>（4）<em>内存消耗问题</em>：当从节点处理不过来主节点发送过来的写命令时，从节点会自动触发执行全量复制，这会导致从节点内存消耗变大</p>\n<p>（5）<em>数据安全问题</em>：当主节点的数据被误删或者篡改时，从节点也会受到影响，因此需要采取一定的措施来保证数据的安全性</p>\n<h3 id=\"主从关系问答\">主从关系问答</h3>\n<h4 id=\"slave-可以执行写命令吗\">slave 可以执行写命令吗</h4>\n<ul>\n<li><code>不可以</code>。master负责写命令，也可以执行读命令，slave负责读命令。<code>即使</code>slave是另一台slave的master，<code>也不能执行写命令</code>。</li>\n</ul>\n<h4 id=\"slave是从头开始复制还是从切入点开始复制\">slave是从<code>头开始复制</code>还是从<code>切入点开始复制</code>?</h4>\n<ul>\n<li>\n<p>都可以</p>\n</li>\n<li>\n<p>当从节点<code>第一次连接主节点</code>时，如果主节点还<code>没有持久化数据</code>，从节点将<code>从头开始复制</code>。即主节点会将自己的全部数据发送给从节点，从节点将接收并保存全部数据。</p>\n</li>\n<li>\n<p>当从节点与主节点<code>已经建立了连接</code>，并且已经有了初始数据同步，如果从节点断开与主节点的连接后重新连接，从节点可以选择从上次同步的位置（复制偏移量）继续同步数据，这样可以避免从头开始复制所带来的性能影响和数据冗余。（<code>master</code>会检查<code>backlog</code>里面的<code>offset</code>，<code>master</code>和<code>slave</code>都会保存一个复制的<code>offset</code>和一个<code>masterId</code>）</p>\n</li>\n</ul>\n<p>​\t\t==特殊情况：==</p>\n<ul>\n<li>如果从节点断开与主节点的连接时间过长，主节点可能已经自动执行了 BGSAVE 命令，生成了新的 RDB 文件，此时从节点需要从头开始复制。</li>\n<li>如果从节点的内存不足，也可能需要从头开始复制，以避免内存溢出。</li>\n</ul>\n<h4 id=\"主节点SHUTDOWN后，从节点会上位吗？\">主节点SHUTDOWN后，从节点会上位吗？</h4>\n<ul>\n<li>主节点关闭后，从节点<code>不会变成主节点</code>，它们会等待主节点重新启动，但是从节点的数据可以正常读取。主节点重启后，<code>主从关系依旧存在</code>。</li>\n</ul>\n<h2 id=\"哨兵Sentinel\">哨兵Sentinel</h2>\n<p>是 Redis 的<code>高可用的</code>解决方案之一，它可以用于<code>监控和管理</code> Redis 主从复制集群，并在主节点发生故障时<code>自动</code>将从节点升级为新的主节点，从而保证系统的<code>高可用性</code>和<code>可靠性</code>。</p>\n<h3 id=\"作用\">作用</h3>\n<ol>\n<li>\n<p><strong>主从监控</strong>：监控主从redis库运行是否正常</p>\n</li>\n<li>\n<p><strong>消息通知</strong>：哨兵可以将故障转移的结果发送给客户端</p>\n</li>\n<li>\n<p><strong>故障转移</strong>：如果master异常，则会进行主从切换，将其中一个slave作为新master</p>\n</li>\n<li>\n<p><strong>配置中心</strong>：客户端通过连接哨兵来获得当前Redis服务的主节点地址</p>\n</li>\n</ol>\n<h3 id=\"哨兵的流程原理\">哨兵的流程原理</h3>\n<ol>\n<li>当哨兵检测到主节点不可用时，会将主节点标记为下线状态(sdown)，并向其他哨兵发送通知，通知其他哨兵主节点已经下线，其他哨兵也标记主节点下线后(odown)，确定主节点不可用</li>\n</ol>\n<blockquote>\n<details>\n<summary>  下线方式： </summary>\n<pre>1. 主观下线(sdown)：指的是单个Sentinel实例对服务器做出的下线判断，即单个sentinel认为某个服务下线（有可能是接收不到订阅，之间的网络不通等等原因）。就是说如果服务器在给定的毫秒数之内没有回应PING命令或者返回一个错误消息， 那么这个Sentinel会主观的(单方面的)认为这个master不可以用了。</pre>\n<pre>2. 客观下线(odown)：客观下线需要多个哨兵达成一致意见才能认为主节点真正不可用。quorum(票数)这个参数是进行客观下线的一个依据。</pre>\n</details>\n</blockquote>\n<ol start=\"2\">\n<li>哨兵在检测到主节点不可用后，会进入选举状态，此时开始选举哨兵的领导者。</li>\n</ol>\n<blockquote>\n<details>\n <summary>  选举算法： </summary>\n <pre>\n监视该主节点的所有哨兵都有可能被选为领导者，选举使用的算法是<font color=\"red\">Raft算法</font>；Raft算法的<font color=\"red\">基本思路是先到先得</font></pre>\n</details>\n</blockquote>\n<ol start=\"3\">\n<li>哨兵leader开始推动故障切换流程并选举出一个新的master</li>\n</ol>\n<blockquote>\n<details>\n <summary>  选举规则： </summary>\n<pre>1. 优先级 (slave-priority  、 replica-priority)  （数值越小优先级越高） </pre>\n<pre>2. 复制偏移量（大的优先）</pre>\n<pre>3. run id（小的优先，每个redis实例启动后都会随机生成一个40位的run id）</pre>\n</details>\n</blockquote>\n<ol start=\"4\">\n<li>选举出新的master后由Sentinel leader完成failover工作(故障切换)</li>\n</ol>\n<blockquote>\n<details><summary>具体流程：</summary>\n <pre>1.执行<font color=\"red\">slaveof no one</font>命令让选出来的从节点成为新的主节点，并通过<font color=\"red\">slaveof</font>命令让其他节点（包括原来的master）成为新主节点的从节点。</pre>\n<pre><font color=\"red\">2.Sentinel leader</font>会向被重新配置的实例发送一个<font color=\"red\">CONFIG REWRITE</font> 命令,从而确保这些配置会持久化在硬盘里(写入配置文件)。</pre>\n<pre><font color=\"red\">值得注意的是：</font> 原来的master重连也将变成从节点</pre>\n</details>\n</blockquote>\n<h3 id=\"故障恢复的流程\">故障恢复的流程</h3>\n<ul>\n<li>\n<p>从下线的主服务的所有从服务里面挑选个<code>从服务</code>，将其转成主服务（<code>选举的原则</code>就是上面👆🏻的）</p>\n</li>\n<li>\n<p>从服务挑选出新的主服务之后，<code>sentinel</code> 向原主服务的从服务发送自己是新主机的命令，其他的将变为该服务的从服务</p>\n</li>\n<li>\n<p>下线的主机再次上线时，<code>sentinel</code>会向其发送<code>slaveof</code>命令：让其成为新主的从节点</p>\n</li>\n</ul>\n<p>==注意==</p>\n<ul>\n<li>\n<p>哨兵节点的数量应为多个，哨兵本身应该集群，保证高可用，哨兵节点的数量应该是奇数</p>\n</li>\n<li>\n<p>哨兵集群+主从复制，并<code>不能保证数据零丢失</code>（引出集群cluster，集群可以解决这一问题），此时数据如果数据还是丢失了，那么丢失的<code>原因</code>就是：<code>master宕机后，哨兵需要在一定时间内选出新的master并执行failover操作，这段时间内从节点无法写入数据，造成数据丢失</code>。</p>\n</li>\n</ul>\n<h2 id=\"集群-cluster模式\">集群(cluster模式)</h2>\n<h3 id=\"font-color-red-定义-font\"><font color=\"red\">定义</font></h3>\n<p>集群是一个提供在多个Redis节点间<code>共享数据的程序集</code>，<code>Redis集群可以支持多个master</code></p>\n<ul>\n<li>\n<p>Redis 集群（包括很多小集群）实现了对 Redis 的<code>水平扩容</code>，即启动 N 个 redis 节点，将整个数据库分布存储在这 <code>N 个节点</code>中，<code>每个节点</code>存储总数据的 <code>1/N</code>，即一个小集群存储 1/N 的数据，每个小集群里面维护好自己的 1/N 的数据。</p>\n</li>\n<li>\n<p>Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有<code>一部分节点失效或者无法进行通讯</code>， 集群也可以继续处理命令请求。</p>\n</li>\n<li>\n<p>该模式的 redis 集群特点是：<code>分治、分片</code>。</p>\n</li>\n</ul>\n<h3 id=\"font-color-red-作用-font\"><font color=\"red\">作用</font></h3>\n<ul>\n<li>Redis集群支持<code>多个master</code>，每个master又可以挂载<code>多个slave</code>\n<ol>\n<li>读写分离</li>\n<li>支持数据的高可用</li>\n<li>支持海量数据的读写存储操作</li>\n</ol>\n</li>\n<li>由于Cluster自带<code>Sentinel</code>的故障转移机制，内置了高可用的支持，<code>无需再去使用哨兵功能</code></li>\n<li>客户端与Redis的节点连接，不再需要连接集群中所有的节点，<code>只需要任意连接集群中的一个可用节点即可</code></li>\n<li>由 槽位<code>slot</code>负责分配到<code>各个物理服务节点</code>，由对应的集群来负责维护节点、插槽和数据之间的关系</li>\n</ul>\n<h3 id=\"连接集群的方式\">连接集群的方式</h3>\n<ul>\n<li>普通方式登录：<code>redis-cli -p 6379</code>\n<ul>\n<li>弊端  ： 可能直接进入，读主机，存储数据时，会出现 <code>MOVED </code>重定向操作，所以应该<code>以集群方式登录</code></li>\n</ul>\n</li>\n<li>集群登录:  <code>redis-cli -c -p 6379</code>  &lt;推荐&gt;</li>\n</ul>\n<h3 id=\"优点\">优点</h3>\n<ul>\n<li>\n<p>实现扩容</p>\n</li>\n<li>\n<p>分摊压力</p>\n</li>\n<li>\n<p>无中心配置相对简单</p>\n</li>\n</ul>\n<h3 id=\"缺点\">缺点</h3>\n<ul>\n<li>\n<p>多键操作是不被支持的,加上组的多键操作是复杂的</p>\n</li>\n<li>\n<p>多键的 Redis 事务是不被支持的，lua 脚本不被支持。</p>\n</li>\n<li>\n<p>由于集群方案出现较晚，很多公司已经采用了其他的集群方案，而代理或者客户端分片的方案想要迁移至 redis cluster，需要整体迁移而不是逐步过渡，复杂度较大。</p>\n</li>\n</ul>\n<h3 id=\"font-color-red-槽位slot-font\"><font color=\"red\">槽位<code>slot</code></font></h3>\n<p>Redis集群<code>没有使用一致性hash</code> 而是引入了哈希槽的概念。</p>\n<p>Redis集群有<code>16384</code>个哈希槽每个key通过<code>CRC16校验</code>后对<code>16384取模</code>来决定放置哪个槽，集群的每个节点负责一部分<code>hash</code>槽</p>\n<h3 id=\"font-color-red-分片-font\"><font color=\"red\">分片</font></h3>\n<ul>\n<li>是什么： 使用Redis集群时我们会将<code>存储的数据分散到多台redis机器上</code>，这称为<code>分片</code>。就是<code>集群中</code>的每个Redis实例都被认为是整个数据的一个分片。</li>\n<li>如何找到给定key的分片\n<ul>\n<li>对key进行<code>CRC16(key)</code>算法处理并通过对总分片数量取模。</li>\n<li>然后，使用确定性<code>哈希函数</code>，这意味着<code>给定的key</code>将多次始终映射到同一个分片</li>\n<li>至此我们就可以推断将来<code>读取到的特定key</code>的位置</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"font-color-red-分片和槽位的优势-font\"><font color=\"red\">分片和槽位的优势</font></h3>\n<ul>\n<li>\n<p><code>方便扩缩容</code>和<code>数据分派查找</code></p>\n</li>\n<li>\n<p>容易添加或者删除节点，将一个结点的哈希槽移动到另一个节点并不会停止服务，所以无论<code>添加删除或者改变</code>某个节点的哈希槽的数量都不会造成集群不可用的状态。</p>\n</li>\n</ul>\n<h3 id=\"font-color-red-槽位映射方案-font\"><font color=\"red\">槽位映射方案</font></h3>\n<h4 id=\"哈希取余分区-textcolor-red-和服务器数量有关\">哈希取余分区$\\textcolor{red}{&lt;和服务器数量有关&gt;}$</h4>\n<ul>\n<li>优点:\n<ul>\n<li>简单有效，只需要预估好数据规划的节点就可以了</li>\n<li>使用<code>Hash算法让固定的一部分请求落到同一台服务器上</code>，这样每台服务器固定处理一部分请求 (并维护这些请求的信息)， 起到<code>负载均衡+分而治之</code>的作用。</li>\n</ul>\n</li>\n<li>缺点:\n<ul>\n<li>扩容或者缩容就比较麻烦；一旦某个redis机器宕机了，由于台数数量变化，会导致hash取余全部数据重新洗牌。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"一致性哈希算法分区\">一致性哈希算法分区</h4>\n<ul>\n<li>\n<p>作用</p>\n<ul>\n<li>可以解决分布式缓存数据变动和映射问题，当服务器个数发生变动时，尽量减少影响客户端到服务器的映射关系</li>\n</ul>\n</li>\n<li>\n<p>步骤</p>\n</li>\n</ul>\n<blockquote>\n<ol>\n<li>\n<p>由<code>一致性Hash算法</code>构建一致性哈希环</p>\n<p>上面的节点取模法是对<code>节点（服务器）</code>的数量进行取模。而一致性Hash算法是对2<sup>32</sup>取模，简单来说，一致性Hash算法将整个<code>哈希值空间组织成一个虚拟的圆环</code><br>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202307030011466.png\" alt=\"image-20230525170513937\" style=\"zoom:35%;\"></p>\n</li>\n<li>\n<p>服务器IP节点映射: 将集群中各个IP节点映射到环上的某一个位置<br>\n将各个服务器使用Hash进行运算，可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置<br>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202307030011366.png\" alt=\"image-20230525170644816\" style=\"zoom:35%;\"></p>\n</li>\n<li>\n<p>key落到服务器的落键规则<br>\n当我们需要存储一个kv键值对时，首先计算key的hash值，hash(key)，将这个key使用相同的函数Hash计算出哈希值并确定此数据在环上的位置，<strong>从此位置沿环顺时针“行走”</strong>，第一台遇到的服务器就是其应该定位到的服务器，并将该键值对存储在该节点上。<br>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202307030011119.png\" alt=\"image-20230525170901005\" style=\"zoom:35%;\"></p>\n</li>\n</ol>\n</blockquote>\n<ul>\n<li>\n<p>优点</p>\n<blockquote>\n<ol>\n<li>\n<p>一致性哈希算法的容错性</p>\n<p>一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器〈即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响</p>\n</li>\n</ol>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202307030011988.png\" alt=\"image-20230525170927600\" style=\"zoom:40%;\">\n<ol start=\"2\">\n<li>\n<p>一致性哈希算法的扩展性</p>\n<p>数据量增加了，需要增加一台节点NodeX，X的位置在A和B之间，那收到影响的也就是A到X之间的数据，重新把A到X的数据录入到X上即可，不会导致hash取余全部数据重新洗牌。</p>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202307030012869.png\" alt=\"image-20230525170937921\" style=\"zoom:40%;\">\n</li>\n</ol>\n</blockquote>\n</li>\n<li>\n<p>缺点</p>\n<blockquote>\n<ol>\n<li>\n<p>一致性哈希算法的数据倾斜问题</p>\n<p>一致性Hash算法在服务<strong>节点太少时</strong>，容易因为节点分布不均匀而造成<strong>数据倾斜</strong>（被缓存的对象大部分集中缓存在某一台服务器上)问题</p>\n</li>\n</ol>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202307030011410.png\" alt=\"image-20230525171000852\" style=\"zoom:40%;\">\n</blockquote>\n</li>\n</ul>\n<h4 id=\"哈希槽分区\">哈希槽分区</h4>\n<ul>\n<li>\n<p>作用</p>\n<ul>\n<li>\n<p>解决均匀分配的问题，在数据和节点之间又加入了一层，把这层称为哈希槽(slot)，用于管理数据和节点之间的关系，现在就相当于节点上放的是槽，槽里面放的是数据。</p>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202307030011361.png\" alt=\"image-20230525171159628\" style=\"zoom:25%;\">\n</li>\n</ul>\n</li>\n<li>\n<p>为什么Redis集群的最大槽数是16384个</p>\n<blockquote>\n<p>Redis集群并没有使用一致性hash而是引入了哈希槽的概念,作用是：每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽</p>\n</blockquote>\n<ul>\n<li>(1)如果槽位为65536，发送心跳信息的消息头达8k（65536÷8÷1024=8kb），发送的<code>心跳包过于庞大</code></li>\n<li>(2)redis的集群主节点数量基本<code>不可能超过1000个</code></li>\n<li>(3)槽位越小，节点少的情况下，<code>压缩比高，容易传输</code></li>\n</ul>\n</li>\n</ul>\n<p>==注意==</p>\n<blockquote>\n<p>Redis集群不保证强一致性： 在特定的条件下，Redis集群可能会丢掉一些被系统收到的写入请求命令</p>\n</blockquote>\n<h2 id=\"集群下的批处理\">集群下的批处理</h2>\n<p>集群下的批处理命令的多个key必须落在一个插槽中，否则就会导致执行失败</p>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305272026267.png\" alt=\"image-20230527202618094\" style=\"zoom:43%;\">\n<p>==第一种方案==：串行执行，所以这种方式没有什么意义，当然，执行起来就很简单了，缺点就是耗时过久。</p>\n<p>==第二种方案==：串行slot，简单来说，就是执行前，客户端先计算一下对应的key的slot，一样slot的key就放到一个组里边，不同的，就放到不同的组里边，然后对每个组执行pipeline的批处理，他就能串行执行各个组的命令，这种做法比第一种方法耗时要少，但是缺点呢，相对来说复杂一点，所以这种方案还需要优化一下</p>\n<p>==第三种方案==：并行slot，相较于第二种方案，在分组完成后串行执行，第三种方案，就变成了并行执行各个命令，所以他的耗时就非常短，但是实现呢，也更加复杂。</p>\n<p>==第四种方案==：hash_tag，redis计算key的slot的时候，其实是根据key的有效部分来计算的，通过这种方式就能一次处理所有的key，这种方式耗时最短，实现也简单，但是如果通过操作key的有效部分，那么就会导致所有的key都落在一个节点上，产生数据倾斜的问题。</p>\n<p>我们==推荐使用第三种方式==</p>\n","_path":"post/b56c4548.html","_link":"http://rycan.top/post/b56c4548.html","_id":"clkxtzmo6005e0g0pc8gn3b8k"}}