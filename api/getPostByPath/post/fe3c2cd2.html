{"type":"getPostByPath","data":{"title":"分布式","date":"2023-09-13T15:45:22.000Z","description":"面试精选","categories":[{"name":"FaceToFace","_id":"clnwo8rpx00118d0p7vl89er6"}],"tags":[{"name":"分布式","_id":"clnwo8rqp00758d0p1blwgutr"}],"content":"<meta name=\"referrer\" content=\"no-referrer\">\n<h1>分布式事务</h1>\n<h4 id=\"本地事务\">本地事务</h4>\n<p>在单一数据节点中，事务仅限于对单一数据库资源的访问控制，称之为本地事务。在不开启任何分布式事务管理器的前提下，让每个数据节点各自管理自己的事务。 它们之间没有协调以及通信的能力，也并不互相知晓其他数据节点事务的成功与否。</p>\n<h4 id=\"两阶段提交事务XA\">两阶段提交事务XA</h4>\n<p>两阶段提交保证操作的原子性和数据的强一致性；基于XA协议实现的分布式事务对业务侵入很小,但是在高并发的性能至上场景中，基于XA协议的分布式事务并不是最佳选择</p>\n<h4 id=\"柔性事务\">柔性事务</h4>\n<p>如果将实现了ACID的事务要素的事务称为刚性事务的话，那么基于BASE事务要素的事务则称为柔性事务。</p>\n<p>BASE是基本可用、柔性状态和最终一致性这三个要素的缩写。基本可用（Basically Available）保证分布式事务参与方不一定同时在线。柔性状态（Soft state）则允许系统状态更新有一定的延时，这个延时对客户来说不一定能够察觉。最终一致性（Eventually consistent）通常是通过消息传递的方式保证系统的最终一致性。</p>\n<p>在ACID事务中对隔离性的要求很高，在事务执行过程中，必须将所有的资源锁定。柔性事务的理念是通过业务逻辑将互斥锁操作从资源层面上移至业务层面。通过放宽对强一致性要求，来换取系统吞吐量的提升。</p>\n<h2 id=\"四种分布式事务\">四种分布式事务</h2>\n<h5 id=\"AT模式「默认」\"><a href=\"http://seata.io/zh-cn/docs/dev/mode/at-mode.html\">AT模式</a>「默认」</h5>\n<p>强一致性分阶段事务模式，牺牲了一定的可用性，无业务侵入</p>\n<blockquote>\n<p>如何做到对业务的 0 侵入</p>\n</blockquote>\n<p>1、一阶段加载：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。</p>\n<p>​\t此阶段Seata 会拦截“业务 SQL”，解析 SQL 语义，找到“业务 SQL”要更新的业务数据，在业务数据被更新前，将其保存成“before image”（前置镜像），执行“业务 SQL”更新业务数据，在业务数据更新之后，其保存成“after image”，最后生成行锁。以上操作全部在一个数据库事务内完成，这样保证了一阶段操作的原子性。</p>\n<p>2、二阶段提交：异步化。“业务 SQL”在<strong>一阶段</strong>已经提交至数据库，<strong>二阶段如果顺利提交的话</strong>，那么Seata框架只需将一阶段保存的快照数据和行锁删掉，完成数据清理即可</p>\n<p>3、二阶段回滚：通过一阶段的回滚日志进行反向补偿（前面insert，后面回滚时就delete）。Seata 需要回滚一阶段已经执行的“业务 SQL”，还原业务数据。回滚方式便是用“before image”还原业务数据；但在还原前首先要校验脏写，对比“数据库当前业务数据”和 “after image”。如果两份数据完全一致就说明没有脏写，可以还原业务数据，如果不一致就说明有脏写，出现脏写就需要转人工处理,删除中间数据（before image 、after image、行锁等）</p>\n<h5 id=\"TCC\">TCC</h5>\n<p>补偿事务TCC就是Try、Confirm、Cancel，它对业务有侵入性，一共分为三个阶段，我们依次来解读一下。</p>\n<ul>\n<li>\n<p><strong>Try阶段：</strong></p>\n<p>比如我们需要在借书时，将书籍的库存-1，并且用户的借阅量也-1，但是这个操作，除了直接对库存和借阅量进行修改之外，还需要将减去的值，单独存放到冻结表中，但是此时不会创建借阅信息，也就是说只是预先把关键的东西给处理了，预留业务资源出来。</p>\n</li>\n<li>\n<p><strong>Confirm阶段：</strong></p>\n<p>如果Try执行成功无误，那么就进入到Confirm阶段，接着之前，我们就该创建借阅信息了，只能使用Try阶段预留的业务资源，如果创建成功，那么就对Try阶段冻结的值，进行解冻，整个流程就完成了。当然，如果失败了，那么进入到Cancel阶段。</p>\n</li>\n<li>\n<p><strong>Cancel阶段：</strong></p>\n<p>不用猜了，那肯定是把冻结的东西还给人家，因为整个借阅操作压根就没成功。就像你付了款买了东西但是网络问题，导致交易失败，钱不可能不还给你吧。</p>\n<p>跟XA协议相比，TCC就没有协调者这一角色的参与了，而是自主通过上一阶段的执行情况来确保正常，充分利用了集群的优势，性能也是有很大的提升。但是缺点也很明显，它与业务具有一定的关联性，需要开发者去编写更多的补偿代码，同时并不一定所有的业务流程都适用于这种形式。</p>\n</li>\n</ul>\n<h5 id=\"SAGA\">SAGA</h5>\n<p>用于处理长事务，每个执行者需要实现事务的正向操作和补偿操作</p>\n<h5 id=\"XA\">XA</h5>\n<p>1、XA分布式事务协议 - 2PC（两阶段提交实现） &lt;PC实际上指的是Prepare和Commit，也就是说它分为两个阶段，一个是准备一个是提交&gt;</p>\n<p>​\t\t1.1、整个过程的参与者一共有两个角色，一个是事务的执行者，一个是事务的协调者，实际上整个分布式事务的运作都需要依靠协调者来维持;</p>\n<p>​\t\t1.2、在准备和提交阶段，会进行：</p>\n<p>​\t\t\t<strong>准备阶段：</strong>   一个分布式事务是由协调者来开启的，首先协调者会向所有的事务执行者发送事务内容，等待所有的事务执行者答复。各个事务执行者开始执行事务操作，但是不进行提交，并将undo和redo信息记录到事务日志中。如果事务执行者执行事务成功，那么就告诉协调者成功Yes，否则告诉协调者失败No，不能提交事务。</p>\n<p>​\t\t\t**提交阶段：**当所有的执行者都反馈完成之后，进入第二阶段。协调者会检查各个执行者的反馈内容，如果所有的执行者都返回成功，那么就告诉所有的执行者可以提交事务了，最后再释放锁资源。如果有至少一个执行者返回失败或是超时，那么就让所有的执行者都回滚，分布式事务执行失败。</p>\n<p>​\t\t1.3、存在以下几个问题：</p>\n<p>​\t\t\t1.3.1、事务协调者是非常核心的角色，一旦出现问题，将导致整个分布式事务不能正常运行。</p>\n<p>​\t\t\t1.3.2、如果提交阶段发生网络问题，导致某些事务执行者没有收到协调者发来的提交命令，将导致某些执行者提交某些执行者没提交，这样肯定是不行的。</p>\n<p>2、<strong>XA分布式事务协议 - 3PC（三阶段提交实现）</strong></p>\n<p>​\t2.1、三阶段提交是在二阶段提交基础上的改进版本，主要是加入了超时机制，同时在协调者和执行者中都引入了超时机制。</p>\n<p>​\t2.2、三个阶段分别进行：</p>\n<p>​\t\t\t**CanCommit阶段：**协调者向执行者发送CanCommit请求，询问是否可以执行事务提交操作，然后开始等待执行者的响应。执行者接收到请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态，否则返回No</p>\n<p>​\t\t\t**PreCommit阶段：**协调者根据执行者的反应情况来决定是否可以进入第二阶段事务的PreCommit操作。如果所有的执行者都返回Yes，则协调者向所有执行者发送PreCommit请求，并进入Prepared阶段，执行者接收到请求后，会执行事务操作，并将undo和redo信息记录到事务日志中，如果成功执行，则返回成功响应。如果所有的执行者至少有一个返回No，则协调者向所有执行者发送abort请求，所有的执行者在收到请求或是超过一段时间没有收到任何请求时，会直接中断事务。</p>\n<p>​\t\t\t**DoCommit阶段：**该阶段进行真正的事务提交。协调者接收到所有执行者发送的成功响应，那么他将从PreCommit状态进入到DoCommit状态，并向所有执行者发送doCommit请求，执行者接收到doCommit请求之后，开始执行事务提交，并在完成事务提交之后释放所有事务资源，并最后向协调者发送确认响应，协调者接收到所有执行者的确认响应之后，完成事务（如果因为网络问题导致执行者没有收到doCommit请求，执行者会在超时之后直接提交事务，虽然执行者只是猜测协调者返回的是doCommit请求，但是因为前面的两个流程都正常执行，所以能够在一定程度上认为本次事务是成功的，因此会直接提交）协调者没有接收至少一个执行者发送的成功响应（也可能是响应超时），那么就会执行中断事务，协调者会向所有执行者发送abort请求，执行者接收到abort请求之后，利用其在PreCommit阶段记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源，执行者完成事务回滚之后，向协调者发送确认消息， 协调者接收到参与者反馈的确认消息之后，执行事务的中断。</p>\n<p>​\t\t2.3、缺点：</p>\n<p>​\t\t\t2.3.1、3PC在2PC的第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。</p>\n<p>​\t\t\t2.3.2、一旦参与者无法及时收到来自协调者的信息之后，会默认执行Commit，这样就不会因为协调者单方面的故障导致全局出现问题。</p>\n<p>但是我们知道，实际上超时之后的Commit决策本质上就是一个赌注罢了，如果此时协调者发送的是abort请求但是超时未接收，那么就会直接导致数据一致性问题。</p>\n<h1>CAP理论</h1>\n<blockquote>\n<p><strong>CAP理论关注粒度是数据，而不是整体系统设计的策略</strong></p>\n</blockquote>\n<h5 id=\"是什么\">是什么</h5>\n<p>CAP 定理（CAP theorem）又被称作布鲁尔定理（Brewer’s theorem）：在一个分布式系统中，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三者中的两个，另外一个必须被牺牲。</p>\n<ul>\n<li>C 一致性（Consistency）：对数据的更新操作成功并返回客户端后，所有节点在同一时间的数据完全一致；对某个指定的客户端来说，读操作保证能够返回   最新的   写操作结果，对于服务端而言，则是对于数据的更新如何复制分布到整个系统，以保证数据最终一致。</li>\n<li>A 可用性（Availability）：服务一直可用，而且是正常响应时间。非故障的节点在合理的时间内返回合理的响应（不能是错误和超时的响应）</li>\n<li>P 分区容错性（Partition Tolerance）：当出现网络分区后会产生分区现象（节点之间可能会导致  丢包、连接中断、拥塞），系统仍然能够对外提供满足一致性和可用性的服务，分区容错性要求应用虽然是一个分布式系统，但看上去切好像是在一个可以运转正常的整体。</li>\n</ul>\n<h5 id=\"两类架构产生的原因\">两类架构产生的原因</h5>\n<p>1、<strong>CP</strong>产生的原因：由于分区现象的原因，使得不同节点之间的通道被中断了，导致节点之间的数据更新不同步的现象,最终导致客户端访问的时候拿不到准确的数据，违背了可用性的要求</p>\n<p>2、<strong>AP</strong>产生的原因：由于分区现象的原因，使得不同节点之间的通道被中断了，导致节点之间的数据更新不同步的现象,但是为了保可用性，还是会在客户端访问的时候返回之前未更新的数据，违背了一致性的要求</p>\n<h5 id=\"特点——BASE理论\">特点——BASE理论</h5>\n<p>CAP 理论中的 C 在实践中是不可能完美实现的，在数据复制的过程中，节点之间的数据并不是强一致的。即使无法做到强一致性，但应用可以采用适合的方式达到最终一致性。最终一致性具有如下特点：</p>\n<ul>\n<li>基本可用（Basically Available）：分布式系统在出现故障时，允许损失部分可用性，即 保证   核心可用。</li>\n<li>软状态（Soft State）：允许系统存在中间状态，而该中间状态不会影响系统整体可用性。这里的中间状态就是 CAP 理论中的数据不一致。</li>\n<li>==最终一致性==（Eventual Consistency）：系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。</li>\n</ul>\n<p>1、CA：单点集群，满足一致性，可用性的系统，通常扩展性不强大。</p>\n<p>2、CP：满足一致性，分区容错性的系统，对数据一致性要求高，所以性能负担大。</p>\n<p>3、AP：满足可用性，分许容错性的系统，通常可能对一致性要求低一些。</p>\n<h1>解决分布式事务的思路</h1>\n<p>分布式事务最大的问题是各个子事务的一致性问题，因此可以借鉴CAP定理和BASE理论，有两种解决思路：</p>\n<p>1、AP模式：各子事务分别执行和提交，允许出现结果不一致，然后采用弥补措施恢复数据即可，实现最终一致。</p>\n<p>2、CP模式：各个子事务执行后互相等待，同时提交，同时回滚，达成强一致。但事务等待过程中，处于弱可用状态。</p>\n<p>但不管是哪一种模式，都需要在子系统事务之间互相通讯，协调事务状态，也就是需要一个<strong>事务协调者(TC)</strong>；这里的子系统事务，称为<strong>分支事务</strong>；有关联的各个分支事务在一起称为<strong>全局事务</strong>。</p>\n<h1><s>分布式一致性的解决</s></h1>\n<h3 id=\"Paxos算法\">Paxos算法</h3>\n<p>​    是一种基于消息传递   且  具有高度容错特性的一致性算法；可以快速正确的在一个分布式系统中对某个数据值达成一致，并且保证不论发生任何异常，都不会破坏整个系统的一致性</p>\n<h4 id=\"算法描述：\">算法描述：</h4>\n<p>​\t\t1、 先将所有节点划分为Proposer（提议者），Acceptor（接受者），和Learner（学习者）。（注意：每个节点都可以身兼数职）</p>\n<p>​\t\t2、Prepare准备阶段</p>\n<p>​\t\t\t\tProposer向多个Acceptor发出Propose请求Promise（承诺）</p>\n<p>​\t\t\t\tAcceptor针对收到的Propose请求进行承诺</p>\n<p>​\t\t3、Accept接受阶段</p>\n<p>​\t\t\t\tProposer收到多数Acceptor承诺后，向Acceptor发出Propose请求</p>\n<p>​\t\t\t\tAcceptor针对收到的Propose请求进行Accept处理</p>\n<p>​\t\t4、Learn学习阶段：Proposer将形成的决议发送给所有Learners</p>\n<h4 id=\"算法流程\">算法流程</h4>\n<p>1）Prepare:</p>\n<p>Proposer生成全局唯一且递增的Proposal ID，向所有Acceptor发送Propose请求，这里无需携带提案内容，只携带Proposal ID即可。</p>\n<p>2）Promise:</p>\n<p>Acceptor收到Propose请求后，做出两个承诺，一个应答</p>\n<ul>\n<li>不再接受Proposal ID小于等于（注意：这里是&lt;= ）当前请求的Propose请求。</li>\n<li>不再接受Proposal ID小于（注意：这里是&lt; ）当前请求的Accept请求。</li>\n<li>不违背以前做出的承诺下，回复已经Accept过的提案中Proposal ID最大的那个提案的Value和Proposal ID，没有则返回空值。</li>\n</ul>\n<p>3）Propose:</p>\n<p>Proposer收到多数Acceptor的Promise应答后，从应答中选择Proposal ID最大的提案的Value（半数以上），作为本次要发起的提案。</p>\n<p>如果所有应答的提案Value均为空值，则可以自己随意决定提案Value。</p>\n<p>然后携带当前Proposal ID，向所有Acceptor发送Propose请求。</p>\n<p>4）Accept:</p>\n<p>Acceptor收到Propose请求后，在不违背自己之前做出的承诺下，接受并持久化当前Proposal ID和提案Value。</p>\n<p>5）Learn:</p>\n<p>Proposer收到多数Acceptor的Accept后，决议形成，将形成的决议发送给所有Learner。</p>\n<h4 id=\"算法缺陷\">算法<strong>缺陷</strong></h4>\n<p>在网络复杂的情况下，一个应用 Paxos 算法的分布式系统，可能很久 无法收敛，甚至陷入活锁的情况。</p>\n<p>原因：有一个以上的 Proposer，多个 Proposers 相互争夺 Acceptor，造成 迟迟无法达成一致的情况。</p>\n<p>改进：从系统中选出一个节点作为 Leader，只有 Leader 能够发起提案。这样，一次 Paxos 流程中只有一个Proposer，不会出现活锁的情况</p>\n<h3 id=\"ZAB算法\">ZAB算法</h3>\n<p>​\t是特别为 Zookeeper 设计的支持崩溃恢复的原子广播协议;基于该协议，Zookeeper 设计为只有一台客户端（Leader）负责处理外部的写事务请求，然后Leader 客户端将数据同步到其他 Follower 节点。即 Zookeeper 只有一个 Leader 可以发起提案</p>\n<h4 id=\"两种模式\">两种模式</h4>\n<h5 id=\"1、消息广播\">1、消息广播</h5>\n<p>​\t1）客户端发起一个写操作请求。</p>\n<p>​\t2）Leader服务器将客户端的请求转化为事务Proposal 提案，同时为每个Proposal 分配一个全局的ID，即 事务ID。</p>\n<p>​\t3）Leader服务器为每个Follower服务器分配一个单独的队列(FIFO)，然后将需要广播的 Proposal依次放到队列中去，并且根据FIFO策略进行消息发送。</p>\n<p>​\t4）Follower接收到Proposal后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向Leader反馈一个Ack响应消息。</p>\n<p>​\t5）Leader接收到超过半数以上Follower的Ack响应消息后，即认为消息发送成功，可以发送commit消息。</p>\n<p>​\t6）Leader向所有Follower广播commit消息，同时自身也会完成事务提交。Follower 接收到commit消息后，会将上一条事务提交。</p>\n<p>​\t7）Zookeeper采用Zab协议的核心: 只要有一台服务器提交了Proposal，就要确保所有的服务器最终都能正确提交Proposal。</p>\n<img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305102130128.png\" alt=\"image-20230510213009084\" style=\"zoom: 55%;\">\n<p><strong>ZAB协议针对事务请求的处理过程类似于一个两阶段提交过程</strong></p>\n<p>1）广播事务阶段</p>\n<p>2）广播提交操作</p>\n<p><strong>这两阶段提交模型如下，有可能因为Leader宕机带来数据不一致，比如</strong></p>\n<p>1 ） Leader 发起一个事务Proposal1 后就宕机，Follower 都没有Proposal1</p>\n<p>2） Leader收到半数ACK宕机，没来得及向Follower发送Commit</p>\n<blockquote>\n<p>怎么解决呢？==ZAB引入了崩溃恢复模式==</p>\n</blockquote>\n<h5 id=\"2、崩溃恢复\">2、崩溃恢复</h5>\n<p>一旦Leader服务器出现崩溃或者由于网络原因导致Leader服务器失去了与过半 Follower的联系，那么就会进入<strong>崩溃恢复模式。</strong></p>\n<p>Zab协议崩溃恢复 需要满足的条件</p>\n<p>​\t1、确保已经被Leader提交的提案Proposal必须最终被所有的Follower服务器提交。 （已经产生的提案，Follower必须执行）</p>\n<p>​\t2、确保<strong>丢弃</strong>已经被Leader提出的，但是没有被提交的Proposal</p>\n<h6 id=\"崩溃恢复的分类\">崩溃恢复的分类</h6>\n<p>**1、崩溃恢复——异常假设： 假设两种服务器异常情况： **</p>\n<ul>\n<li>一个事务在Leader提出之后，Leader挂了（proposal 的 时候挂了）</li>\n<li>一个事务在Leader上提交了，并且过半的Follower都响应Ack了，但是Leader在Commit消息发出之前挂了 （commit 的 时候挂了）</li>\n</ul>\n<p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305102136883.png\" alt></p>\n<p><strong>2、崩溃恢复——Leader选举</strong></p>\n<p>崩溃恢复主要包括两部分：Leader选举  和  数据恢复</p>\n<p><strong>2.1、Leader选举</strong></p>\n<p>根据上述要求，Zab协议需要保证选举出来的Leader需要满足以下条件：</p>\n<ul>\n<li>新选举出来的Leader不能包含未提交的Proposal。即新Leader必须都是已经提交了Proposal的Follower服务器节点</li>\n<li>新选举的Leader节点中含有最大的zxid。这样做的好处是可以避免Leader服务器检查Proposall的提交和丢弃工作。</li>\n</ul>\n<p><strong>2.2、数据同步</strong></p>\n<ul>\n<li>\n<p>完成Leaderi选举后，在正式开始工作之前（接收事务请求，然后提出新的Proposal),Leader服务器会首先确认事务日志中的所有的Proposal是否已经被集群中过半的服务器Commit</p>\n</li>\n<li>\n<p>Leader服务器需要确保所有的Follower服务器能够接收到每一条事务的Proposal,并且能将所有己经提交的事务Proposal应用到内存数据中。</p>\n<p>等到Follower将所有尚未同步的事务Proposal都从Leader服务器上同步过，并且应用到内存数据中以后，Leader才会把该Follower加入到真正可用的Follower列表中。</p>\n</li>\n</ul>\n<p><img src=\"https://gitee.com/Ryang1118/typora/raw/master/images/202305102143394.png\" alt></p>\n<h1>分布式缓存</h1>\n<h3 id=\"多级缓存\">多级缓存</h3>\n<p>我们把缓存分为两类：</p>\n<ul>\n<li>分布式缓存，例如Redis：\n<ul>\n<li>优点：存储容量更大、可靠性更好、可以在集群间共享</li>\n<li>缺点：访问缓存有网络开销</li>\n<li>场景：缓存数据量较大、可靠性要求较高、需要在集群间共享</li>\n</ul>\n</li>\n<li>进程本地缓存，例如HashMap、GuavaCache：\n<ul>\n<li>优点：读取本地内存，没有网络开销，速度更快</li>\n<li>缺点：存储容量有限、可靠性较低、无法共享</li>\n<li>场景：性能要求较高，缓存数据量较小</li>\n</ul>\n</li>\n</ul>\n<p>可以利用Caffeine性能遥遥领先框架来实现JVM进程缓存  Cache&lt;String, String&gt; cache = Caffeine.newBuilder().build();</p>\n<h4 id=\"JVM进程缓存\">JVM进程缓存</h4>\n<p>Caffeine提供了三种缓存驱逐(清除)策略：</p>\n<p>1、<strong>基于容量</strong>：设置缓存的数量上限</p>\n<p>2、<strong>基于时间</strong>：设置缓存的有效时间</p>\n<p>3、<strong>基于引用</strong>：设置缓存为软引用或弱引用，利用GC来回收缓存数据。性能较差，不建议使用。</p>\n<p>在默认情况下，当一个缓存元素过期的时候，Caffeine不会自动立即将其清理和驱逐。而是在一次读或写操作后，或者在空闲时间完成对失效数据的驱逐。</p>\n<h4 id=\"实现\">实现</h4>\n<p>Nginx +  OpenResty</p>\n<h3 id=\"解决缓存一致性问题（Canal）\">解决缓存一致性问题（Canal）</h3>\n<p><strong>设置有效期</strong>：给缓存设置有效期，到期后自动删除。再次查询时更新</p>\n<ul>\n<li>优势：简单、方便</li>\n<li>缺点：时效性差，缓存过期之前可能不一致</li>\n<li>场景：更新频率较低，时效性要求低的业务</li>\n</ul>\n<p><strong>同步双写</strong>：在修改数据库的同时，直接修改缓存</p>\n<ul>\n<li>优势：时效性强，缓存与数据库强一致</li>\n<li>缺点：有代码侵入，耦合度高；</li>\n<li>场景：对一致性、时效性要求较高的缓存数据</li>\n</ul>\n<p>**异步通知：**修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据</p>\n<ul>\n<li>优势：低耦合，可以同时通知多个缓存服务</li>\n<li>缺点：时效性一般，可能存在中间不一致状态</li>\n<li>场景：时效性要求一般，有多个服务需要同步</li>\n</ul>\n<h1>分布式搜索引擎（es）</h1>\n<h1>分布式日志服务</h1>\n","_path":"post/fe3c2cd2.html","_link":"http://rycan.top/post/fe3c2cd2.html","_id":"clnwo8rqg004v8d0pdb9v946e"}}